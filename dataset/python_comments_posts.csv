QuestionId,Tags,answer_Id,OwnerUserId,LastEditor_Userid,commentId,Score,Text,CreationDate,AdrressedbyCommentId,need_update,addressed_in
38072,<python><r><data-analysis><excel>,38073,58890.0,,44860,8,"My comment, for what it's worth: Excel is OK for relatively small datasets of known size. It's terrible for large datasets, and for datasets of varying length.",2018-09-13 15:59:41,32440.0,no update,
38072,<python><r><data-analysis><excel>,38073,58890.0,,44894,8,Possible duplicate of [Do data scientists use Excel?](https://datascience.stackexchange.com/questions/5443/do-data-scientists-use-excel),2018-09-14 4:34:43,24000.0,update,comment
42156,<python><pandas>,42163,59425.0,8501.0,68393,0,Why was your code not working? How specifically was the output wrong?,2019-11-16 2:13:26,8501.0,update,comment
80398,<machine-learning><python><scikit-learn><one-hot-encoding>,80437,99648.0,,84249,1,"It may be because of the numeric portion.  Try just the encoder (or just dropping the numeric pipeline from the `ColumnTransformer`), and try changing the `sparse_threshold` parameter of the `ColumnTransformer`.",2020-08-17 14:12:40,55122.0,no update,
80398,<machine-learning><python><scikit-learn><one-hot-encoding>,80437,99648.0,,84250,1,"I don't think `Pandas` `SparseArray` format is compatible with `OneHotEncoder` sparse matrix format (it's `scipy.sparse` format I think). And I think you should not use `Pandas` at the end of the process, you should use it to read and wrangle `csv` files, but don't use it after scaling and transforming data, it will eat up a lot of memory.",2020-08-17 14:43:00,43374.0,update,both
80398,<machine-learning><python><scikit-learn><one-hot-encoding>,80437,99648.0,,84288,1,"Thanks @BenReiniger, I removed the numeric portion and the memory usage went down to 0.106 MB. I tied changing the sparse_threshold=0.5 instead of the default 0.3 but it did not change the outcome.",2020-08-18 11:42:24,99648.0,no update,
77803,<python><neural-network><keras><numpy>,77809,100857.0,100857.0,82366,1,please post your `model.fit` function in order to understand what it requires,2020-07-16 12:02:14,100269.0,update,comment
77803,<python><neural-network><keras><numpy>,77809,100857.0,100857.0,82368,1,"I have already: self.model.fit(np.array([1, 1, 1, 1]),
self.rightAnswer, 
epochs = 1,
batch_size = 1)",2020-07-16 12:05:41,100857.0,update,comment
77803,<python><neural-network><keras><numpy>,77809,100857.0,100857.0,82369,1,"This is the **call** to the function, I meant the function itself, what does it do, how is it implemented",2020-07-16 12:06:36,100269.0,no update,
77803,<python><neural-network><keras><numpy>,77809,100857.0,100857.0,82370,1,"Ohh, I haven't write that I use Keras. It is the function of Keras Sequential model.",2020-07-16 12:08:16,100857.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59297,8,Are you sure those are CDF plots of the data? Empirical CDF plots of that data are [these](http://i.stack.imgur.com/NWco6.png).  It appears that you are plotting the values on y-axis over some standard x-axis.  Are the values in your variable observations or points on the CDF?,2019-06-09 11:22:50,10814.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59306,8,@Fatemehhh Hello I am not plotting these two plots in a single one as there are hundreds of other datasets I need to compare with the real dataset and find out which one is the closest to the real dataset. Basically I am trying to approximate a parameter which is generating different simulated datasets at different values and when the simulated dataset is closest to the real dataset then there is the right parameter!,2019-06-09 16:46:22,71464.0,update,post
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59307,8,"@Edmund I think you are correct, those values are the variable observation and I am plotting the values on y-axis over some standard x-axis. `x_points=np.asarray(list(range(0,len(data_a))))
>>> x_points=x_points/len(data_a)
>>> plt.plot(x_points,data_a)
>>> x_points=np.asarray(list(range(0,len(data_b))))
>>> x_points=np.asarray(list(range(0,len(data_c))))
>>> x_points=x_points/len(data_c)
>>> plt.plot(x_points,data_c)`                                                                             This is the code. But my question is how one can find the closeness between the two datasets",2019-06-09 16:48:47,71464.0,update,both
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59349,8,"Wouldn't it be better simply to fit a distribution to the observed data?  It seems a bit  convoluted and computationally expensive to go about it by guessing parameters, simulating results, and testing similarity.",2019-06-11 0:30:24,10814.0,update,comment 
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59351,8,@edmund ok can you guide me through that ? Like how would you encounter that? And yes it is very computationally expensive to do this. But I am still not sure what distribution we would be fitting here!,2019-06-11 4:01:13,71464.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59370,8,This feels a lot like an [XY Problem](http://xyproblem.info/). Please share the problem you are attempting to solve instead of this part of the solution to the problem.,2019-06-11 11:16:01,10814.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59448,8,"@edmund Hello, the problem is simple. I have a real data set and simulated data sets which I am obtaining from a simulation. My goal is to find which of the simulated data set is the closest to the real data?",2019-06-12 14:48:09,71464.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59450,8,What is your model using to generate the simulated values? A PDF function? A CDF function? Something else?,2019-06-12 15:14:29,10814.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59469,8,It is using the Schelling's model of segregation.,2019-06-12 20:56:32,71464.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59552,8,Try Cosine Similarity?,2019-06-14 2:36:00,39338.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59588,8,So those are points on the  histogram of neighborhood utility values of each agent in your city grid? I would recommend fitting a distribution over the target agent values across the city grid. Then performing a goodness of  fit test to that distribution and the simulated cities. Pick the best fit.,2019-06-14 11:27:56,10814.0,update,both
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59633,8,@edmund Thank You for this! Also what do you think of cosine similarity?,2019-06-14 18:37:14,71464.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59635,8,@MohitMotwani My datasets are not of equal lengths so how would I use cosine similarity?,2019-06-14 19:07:01,71464.0,update,both
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59639,8,"It will still work, given the length of each list is the same. You should really give it a try.",2019-06-15 1:57:26,39338.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59668,8,@MohitMotwani But the two lists I am trying to compare doesnt have the same length. Would it still work?,2019-06-15 15:44:19,71464.0,update,comment
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59687,8,@MohitMotwani I want the similarity between data_a and data_b and then data_a and data_c,2019-06-15 20:27:48,71464.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,59913,8,"I would also try simple, as mentioned Cosine similarity or any other distance. See this https://stackoverflow.com/questions/3121217/cosine-similarity-of-vectors-of-different-lengths for your concern over unequal lists. The other thing you should ask, and you only know, in what sense you want to identify similar dataset? Trend? Shape? Depending on what you are looking for, maybe you can generate more features like numerical derivation of datasets to compare, or features like min, max etc. in time series, see https://github.com/blue-yonder/tsfresh. But do not forget to start simple and test!",2019-06-20 5:30:02,44456.0,no update,
53478,<python><statistics><visualization><simulation>,54256,71464.0,91700.0,60029,8,"I don't understand why you try to simulate the data. A simple linear regression would give you immediately a much closer curve, assuming that's the goal.",2019-06-22 0:27:12,64377.0,no update,
53325,<machine-learning><python><scikit-learn><logistic-regression><kaggle>,53338,73912.0,73912.0,59141,2,As far as I can see you make changes to the xtrain DF but not to xtest. xtest must have the same logic and shape as xtrain. Otherwise the model will not know what it is supposed to predict. Do the same recoding to xtest like you did to xtrain.,2019-06-06 12:32:34,71442.0,update,comment
53325,<machine-learning><python><scikit-learn><logistic-regression><kaggle>,53338,73912.0,73912.0,59148,2,How am I supposed to do that?,2019-06-06 14:01:24,73912.0,update,comment
53325,<machine-learning><python><scikit-learn><logistic-regression><kaggle>,53338,73912.0,73912.0,59150,2,You only have 197 examples in your x_test set.,2019-06-06 12:44:15,75547.0,no update,
53325,<machine-learning><python><scikit-learn><logistic-regression><kaggle>,53338,73912.0,73912.0,59151,2,"You can write a function and pass dataset to it and in that function, perform all the operation that you want to perform. Then first pass the train dataset and then pass the test dataset. In this way same operations are performed on both dataset and you should not get the error which you are getting now.",2019-06-06 14:38:33,73998.0,update,comment
106703,<python>,106814,130965.0,,108850,1,"Yes, you can do that. Look into some kaggle comps.",2022-01-04 18:20:19,35644.0,no update,
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74158,0,"**First:** The answer in iuse is encoded (by you) as a number, but it is a so called categorical value. That means that you cannot apply just any operator on it: Not equal works, but a mean definitely does not.  *For the connoisseur: It seems ordinal, so you could rank it.* **Second:** Could you include a sample of your dataframe?",2020-03-03 13:00:27,27432.0,update,both
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74163,0,"OP: it **would** be clearer if you provided a sample frame.
@SvanBalen, it sounds like the 'iuse' column has been generated.  Each row is a user/response, with columns for each answer, like 'q1_1', and _those_ are encoded 1/2/3; but 'iuse' then is just a row-wise count of qi_j==3.",2020-03-03 14:42:18,55122.0,no update,
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74169,0,"Thanks guys, I'll make sure to include a better example next time, but for now @BenReiniger has given a good explanation",2020-03-03 16:50:11,91111.0,no update,
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74200,0,"@BenReiniger Yes it could be a derived feature. Nevertheless it remains a categorial value, and taking the mean is... *wait for it* ... *mean*ingless",2020-03-04 13:18:19,27432.0,no update,
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74203,0,"@SvanBalen  'iuse' is a count variable; discrete, but numerical, not categorical.   The mean of that is perfectly sensible. I'd agree about the mean of any of the qi_j's...",2020-03-04 13:26:47,55122.0,no update,
69061,<python><statistics><descriptive-statistics>,69072,91111.0,,74204,0,"Ahhh, I think you are right! In that case my previous statements are retracted.",2020-03-04 13:34:58,27432.0,no update,
49641,<python><pandas>,49642,31599.0,45264.0,56727,1,I can not just extract the first word because there will be complications like I will get 4-Grain instead of 4-Grain Flakes for the first item in food list,2019-04-20 16:00:33,31599.0,no update,
49641,<python><pandas>,49642,31599.0,45264.0,56728,1,"Are you able to share the data? And why doesn't splitting on the first comma `,` give the result you expect? It looks like it would work, according to you example data. Perhaps, like in your other question, you could create a multi-index. `Yogurt` would be the first level, then `Plain` and e.g. `Flavoured` would be the second level.",2019-04-20 16:03:48,45264.0,update,no
49641,<python><pandas>,49642,31599.0,45264.0,56730,1,@n1k31t4 but 4-Grain would be first level and Grain would be second level. Yes I can share the data,2019-04-20 16:16:11,31599.0,update,comment
11356,<python><apache-spark><cross-validation><pyspark>,11361,17116.0,21.0,60279,35,"This does not directly answer the question, but here I give a suggestion to improve the naming method so that in the end, we don't have to type, for example:
[td1, td2, td3, td4, td5, td6, td7, td8, td9, td10]. Imagine doing this for a 100-fold CV. Here's what I'll do: portions = [0.1]*10 cv = df7.randomSplit(portions) folds = list(range(10)) for i in range(10): test_data = cv[i] fold_no_i = folds[:i] + folds[i+1:] train_data = cv[fold_no_i[0]] for j in fold_no_i[1:]: train_data = train_data.union(cv[j])",2019-06-26 20:03:51,76724.0,update,post
66034,<python><scikit-learn><pandas><preprocessing>,66041,87963.0,86339.0,71377,8,I confirm that in my computer it also works extremely slow,2020-01-07 13:01:52,86339.0,no update,
66034,<python><scikit-learn><pandas><preprocessing>,66041,87963.0,86339.0,105290,8,This should be substantially better in more recent versions; see the PR https://github.com/scikit-learn/scikit-learn/pull/18987 and it's related Issues.,2021-09-12 21:40:28,55122.0,no update,
46943,<python><deep-learning><keras><scikit-learn><tensorflow>,46970,67537.0,,53830,2,"By default it will pick it, you training is slow because of your hardware.",2019-03-10 2:55:33,35644.0,no update,
5097,<data-mining><python>,5098,8195.0,8195.0,5404,0,"I think most common ones are `numpy`, `scipy`, `scikit-learn` and `pandas`.",2015-02-12 4:47:23,3070.0,no update,
45524,<python><autoencoder><accuracy>,45525,67633.0,,52440,1,"@MatthieuBrucher Yes, I was trying to reduce any possible color... Thank you very much!",2019-02-13 17:47:05,67633.0,no update,
17773,<machine-learning><python><bigdata><random-forest>,17788,30254.0,,44398,1,"some models are retrainable, RF is not (but you can add fresh trees). Ask your advisor for papers to read on basic machine learning models so you can better understand which ones are retrainable.",2018-09-05 19:45:55,21232.0,no update,
85488,<python><scikit-learn><random-forest><encoding>,85495,107529.0,43000.0,89598,1,Does this post answer your question ? [How can I fit categorical data types for random forest classification?](https://datascience.stackexchange.com/questions/26283/how-can-i-fit-categorical-data-types-for-random-forest-classification),2020-11-16 11:39:18,69478.0,update,post
15921,<machine-learning><python><neural-network><rnn>,26796,27135.0,26562.0,31977,10,"""the same number of outputs as inputs"". Wait, what? Be careful with this - it's not often that you come across a NN where your statement would be true. These numbers can vary wildly and I can't think of the last time I worked on a NN where my inputs and outputs matched exactly. Your inputs are whatever you want them to be based on available features, your outputs depends on the answer you are trying to get (binary, classification, etc). Very rare to have those two match up.",2018-02-05 16:04:15,23240.0,update,post
97783,<python><dataset><pandas><data-cleaning><python-3.x>,97793,109295.0,,102094,0,Do you want to keep country codes or not?,2021-07-13 2:00:58,120641.0,update,comment
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95145,3,"Given the edit, and neural nets being fickle, could this just be random effects of the training?",2021-03-21 23:14:02,55122.0,update,no
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95149,3,"As far as my understanding of it goes, I don't believe that's the case here. I performed a gridsearch three times which gave comparable results around the values given in the edit. Further, setting weights to default (0:1,1:1) consistently returned poor validation results.",2021-03-22 0:29:37,113619.0,update,comment
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95151,3,Interesting! I don't suppose you could share the data and code?,2021-03-22 0:35:02,55122.0,update,both
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95201,3,"Sorry for the late reply, as this work is part of my dissertation I wanted to double check with my supervisor what I could and could not share at this point. I've added a link to a github containing the code (It is quite long and convoluted, likely not very optimised..) The output data from my gridsearches are contained in Support_Files under base_gridsearch_class_weights#X.json",2021-03-22 20:22:59,113619.0,no update,
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95204,3,"Thanks!
I still suspect NN convergence issues: in the results #3 file e.g., in each case with 0.5 accuracy, the confusion matrix has predicted everything to be the same class, and which one depends on the weights.  In the first few grid points, you increase the weights towards the negative class, but after the first two suddenly the network _starts_ predicting the positive class.",2021-03-22 20:38:55,55122.0,no update,
90936,<python><keras><tensorflow><dataset><convolutional-neural-network>,90941,113619.0,113619.0,95214,3,"Ah, I think I misconstrued your point the first time around. Yes, I agree, it does seem to be a convergence issue, only it's very consistent hence my reluctance to label it as random, per se.. Perhaps I still have some misunderstanding.
Also, it's worth noting that the results seem be erroneous in #3. the accuracy for the 17th and 18th entry do not match up with the confusion matrix. I'm not sure what happened there, but I figured I'd point it out, just in case, anyway.",2021-03-23 0:15:38,113619.0,no update,
63699,<python><classification><cnn><svm>,63700,52755.0,,68865,3,"Your second curve looks unusually regular, it's as if there wasn't enough points to calculate it with precision.",2019-11-24 23:59:39,64377.0,no update,
63699,<python><classification><cnn><svm>,63700,52755.0,,68944,3,You mean by enough points the test data ?? Actually in the test data the the number of samples for each class are not equal.,2019-11-25 19:16:41,52755.0,update,comment
63699,<python><classification><cnn><svm>,63700,52755.0,,68947,3,"it's not about the points in the data strictly speaking, but it could be related. the ROC curve is made of the points which correspond to the different possible thresholds in the trade-off between precision and recall. for some reason there seems to be very few of these points in the 2nd case (maybe even only one actually). maybe the method doesn't output a numeric value to be used as threshold?",2019-11-25 20:21:09,64377.0,update,both
28727,<python><pandas><sql>,28728,47352.0,,33638,1,"If your dataset isn't toooooo large,then after creating the DF just save it into feather format, provided that you are not low on space for faster retrieval",2018-03-06 21:00:58,35644.0,no update,
28727,<python><pandas><sql>,28728,47352.0,,33640,1,Also you can just make the changes in place using `lambda or map`,2018-03-06 21:10:02,35644.0,no update,
28727,<python><pandas><sql>,28728,47352.0,,33721,1,Is there an advantage to making the changes in place vs adding a column to the DF that contains that info?,2018-03-07 22:49:27,47352.0,update,comment
13580,<machine-learning><python><scikit-learn><dataset><cross-validation>,13582,15412.0,85045.0,15015,0,Just split it twice.,2016-08-20 19:32:34,381.0,no update,
13580,<machine-learning><python><scikit-learn><dataset><cross-validation>,13582,15412.0,85045.0,15016,0,Wouldn't that be somewhat ugly? I mean there must be a way in scikit-learn for that.,2016-08-20 19:33:36,15412.0,no update,
18154,<python><pandas>,24501,22054.0,,20561,6,"Without transforming it into a Series, just try this:
`df['month'].value_counts()`, where df is your pandas dataframe",2017-04-06 9:41:32,15412.0,update,comment
18154,<python><pandas>,24501,22054.0,,20562,6,"@Nain thanks, but I need to group by 'sally' and there are missing months like the example above.",2017-04-06 9:44:42,22054.0,no update,
18154,<python><pandas>,24501,22054.0,,20580,6,@Kyle. Could you explain more? Months are not complete. How can I insert month with zero count?,2017-04-06 22:21:56,22054.0,update,both
18154,<python><pandas>,24501,22054.0,,21890,6,`df.month.value_counts(dropna=False)`,2017-05-15 9:03:55,32252.0,update,both
20442,<python><neural-network><time-series><keras><rnn>,20490,23138.0,23138.0,24823,4,input shape is suspicious. what value as look_back?  did you import Adam optimizer as such keras.optimizers.Adam as adam? learning rate of 0.1 is quite high. batch_size of 1 looks small.,2017-07-14 20:04:54,2968.0,update,no
20442,<python><neural-network><time-series><keras><rnn>,20490,23138.0,23138.0,24824,4,"@edouard
look_back is 1.
yes, adam optimizer is from keras.optimizers.adam
I've tried learning rate values - 0.01, 0.001, 0.03, 0.3 - none worked.",2017-07-14 20:12:53,23138.0,no update,
20442,<python><neural-network><time-series><keras><rnn>,20490,23138.0,23138.0,24826,4,"@edouard 
I've edited the question with some changes that you mentioned. Decreasing learning rate to 0.01 didn't help.",2017-07-14 20:27:25,23138.0,no update,
94623,<machine-learning><python><classification><training>,94625,83862.0,,98825,2,How do you believe other train/test/validation data sets are created?,2021-05-18 19:24:00,13296.0,update,comment
94623,<machine-learning><python><classification><training>,94625,83862.0,,98826,2,"Maybe I am getting confused here, the train/test/validation datasets are all created from the original dataset itslef? Am I right here? Maybe something like a split of 60%/20%/20% ? and then I can use my training data to build a model and then test it on the valdiation data?",2021-05-18 19:25:45,83862.0,update,comment
11928,<python><scikit-learn><pandas><random-forest><python-3.x>,11933,17310.0,118595.0,72667,94,You can use numpys 'np.isfinite()' to create a boolean mask and after indexing by this mask your data,2020-02-01 10:37:51,89254.0,no update,
11928,<python><scikit-learn><pandas><random-forest><python-3.x>,11933,17310.0,118595.0,74191,94,Check if you have taken log() of any feature with zero value in it?,2020-03-04 10:17:45,74313.0,update,post
43439,<machine-learning><python><pandas><dataframe>,43441,65273.0,,50549,0,Something like a groupby?,2019-01-03 16:59:12,61418.0,update,no
16326,<machine-learning><python><predictive-modeling><probability>,16327,27962.0,27962.0,18299,2,"Hi. Welcome to the DataScience SE.  Can you edit the title to be more relevant to the question. Currently, it's a bit confusing and broad :)",2017-01-15 14:24:52,11097.0,update,comment
37027,<machine-learning><python><deep-learning><reinforcement-learning><dqn>,37071,42882.0,42882.0,43397,2,"The site does not give out generic guidance and help. If you could focus the question on a single part of your problem that you immediately face, then it can be answered in one go. If that doesn't take you from start to end in one go, then you can ask a *different* question when you next get stuck. The next step for you should be to remove the last paragraph (where you ask ""Can some help me/ guide me to write the environment"") and replace with something you really need to know when *you* are doing the work (e.g. ""What methods do I need to represent this environment to an agent?"")",2018-08-16 20:12:08,836.0,update,both
74588,<python><training><q-learning>,74699,97597.0,,79366,1,"You can just save the model's parameters, right? https://pytorch.org/tutorials/beginner/saving_loading_models.html You can also have an evaluation function that evaluates how good your model is after a certain number of iterations, and save the one which performs the best.",2020-05-21 19:37:35,97626.0,update,comment
74588,<python><training><q-learning>,74699,97597.0,,79398,1,"Thanks a lor for your reply, that link really helps me! You can put it into an answer and then I’ll mark it as the answer that solved my problem. :) thanks a lot!",2020-05-22 19:51:04,97597.0,no update,
25184,<python><pandas>,25200,9988.0,9988.0,29509,0,can you provide a small reproducible input data sets and your desired data set?,2017-11-28 20:54:03,20492.0,update,comment
25184,<python><pandas>,25200,9988.0,9988.0,29517,0,"I have 2 datasets in this situation. One is df, which contains [address_locality, address_region] as columns. The other is called indicadores (sorry for the portuguese language, means 'indicators'), which are some governmental indices by each city in Brazil.
So, in this case, it would look like this:
INDICATORS 
columns: state, city, population
example: sp, sao_paulo, 7M",2017-11-28 23:22:49,9988.0,no update,
25184,<python><pandas>,25200,9988.0,9988.0,29518,0,Please post small (3-7 rows) sample data sets and your desired data set [into your question](https://datascience.stackexchange.com/posts/25184/edit) - this will help us to: (1) better understand what exactly are you trying to achieve; (2) develop and test code. Please read [how to make good reproducible pandas examples](http://stackoverflow.com/questions/20109391/how-to-make-good-reproducible-pandas-examples) and edit your post correspondingly.,2017-11-28 23:27:17,20492.0,update,both
25184,<python><pandas>,25200,9988.0,9988.0,29520,0,"Done, sorry, I'm starting here in stack exchange.",2017-11-29 0:15:12,9988.0,no update,
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100825,0,"It sounds like you have three variables: species, location, and abundance. Is this correct? Such data are not amenable to a box plot or violin plot, as those would be for just the categorical species variable and one of the numerical variables. My first visualization would be a scatter plot of the location and abundance with different colors and/or shapes for the species: lions in red, tigers in blue, and bears in black, for instance.",2021-06-15 9:53:27,73930.0,update,no
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100858,0,"@Dave My data counts the number of species at a given altitude: *x* species A at altitude *y*. To me (at least), it doesn't seem too different than the *life_exp* vs *continent* plot [this this tutorial](https://stackabuse.com/seaborn-violin-plot-tutorial-and-examples).",2021-06-15 16:10:23,107613.0,update,no
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100860,0,"The x-axis in their chart is the continent; the y-axis is the `life_exp`. You have a third dimension. I still recommend a bivariate scatter plot with your categorical variable denoted by colors, shapes, or both. That captures all three of your variables.",2021-06-15 16:12:43,73930.0,no update,
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100861,0,"@Dave My data could be reshaped to two variables: species and location. Using my example data: there would be 2 entries for species A at altitude 7000, 1 entry at altitude 6000, etc. (The 0.5 entries can either be omitted or rounded up/down.)",2021-06-15 16:13:54,107613.0,no update,
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100862,0,"Then make a violin chart with categorical species on the x-axis and numerical altitude on the y-axis. That is analogous to the plot in the link, but it does omit the abundance variable.",2021-06-15 16:15:27,73930.0,update,comment
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100863,0,"Oh, do you mean that there are 50 lions, 9 tigers, and 13 bears at altitude 1; 9 lions, 11 tigers, and 61 bears at altitude 2; etc?",2021-06-15 16:16:34,73930.0,update,both
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100864,0,@Dave That's correct.,2021-06-15 16:17:19,107613.0,no update,
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100865,0,"Then you have 50 observations of (lion, 1), 9 observations of (lion, 2), 61 observations of (bear, 2), etc. Make your data frame that way, and then you have two variables for your violin plot: categorical `species` on the x-axis and numerical `altitude` on the y-axis.",2021-06-15 16:19:56,73930.0,no update,
96636,<python><visualization><data-cleaning><transformation>,96675,107613.0,107613.0,100878,0,"I'm just wondering if there's an easy way to do it? Seems like what I want is that does the opposite of pandas' `group()` method. Whatever, I found a solution.",2021-06-15 23:32:36,107613.0,update,comment
36090,<python><scikit-learn><clustering>,36097,56719.0,56719.0,42380,2,"why do you say your results are terrible? What is your goal?
Can you show some examples of what your `text.csv` looks like and what kind of information you expect to get from clustering?",2018-07-27 10:36:19,54395.0,update,comment
36090,<python><scikit-learn><clustering>,36097,56719.0,56719.0,42383,2,"I have many different themes present in various clusters. I pre-processed my data (stopwords, lowercase, I removed the punct...). But still I have 'like to cancel order' in one cluster and 'love cancel order' in another. When, in fact, the ideal is to join all 'cancel order' in a single cluster.",2018-07-27 10:49:23,56719.0,no update,
36090,<python><scikit-learn><clustering>,36097,56719.0,56719.0,42385,2,"Ok, why did you choose `6` clusters?
if you choose 6 but there in truth are 2, you will see your results split.
Do you have a predefined number of clusters that you need to find the items that belong to it?",2018-07-27 11:35:45,54395.0,update,comment
36090,<python><scikit-learn><clustering>,36097,56719.0,56719.0,42388,2,"I would like to be at least 30. But I do not know if it is possible. I think my program is really bad because even with 60 (yes, 60!) clusters, I have errors.",2018-07-27 12:04:40,56719.0,no update,
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,39987,3,Why do you want them all to be the same?,2018-07-05 13:45:35,33202.0,update,comment
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,39990,3,it's [right there in the accepted answer to the linked question](https://datascience.stackexchange.com/a/16804/23305) if you read the entire answer,2018-07-05 15:27:08,23305.0,no update,
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,40024,3,"@kbrose - as an example step in process of creating decision tree, looking for its hyperparameters, creating homogenous forest (with those hyperparameters), adding bootstrap, adding fraction of features and increasing n_estimators. So such forest would be something different than single tree, but with exactly the same predictive capability.",2018-07-06 5:48:21,82.0,no update,
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,40025,3,"@oW_ Yes, I read the entire answer, especially **You can fix this by passing the same seed to each DecisionTreeClassifier which you can do using random_state=something. RandomForestClassifier also has a random_state parameter which it passes along each DecisionTreeClassifier.** - I did that, and it did produced trees with fixed seed but different for each one. So it seems that there is no such possibility?",2018-07-06 5:51:44,82.0,update,no
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,40068,3,this homogenous forest would *not* be different from a single decision tree. If they all share the same random seed then the randomness introduced by bootstrap and max_features would disappear ... that's exactly the point... that's also the reason why you can't do it with RandomForestClassifier..unless I'm missing something here,2018-07-06 15:22:56,23305.0,no update,
34038,<python><classification><scikit-learn><random-forest><decision-trees>,34124,82.0,29169.0,40437,3,"Agreed oW_. If all trees are exactly the same you don’t have a forest, you have a tree. If the trees are not exactly the same then your forest has more predictive capabilities than a single tree. Maybe I’m not understanding something crucial here.",2018-07-12 13:56:10,33202.0,no update,
80462,<python><mathematics>,80463,100262.0,103697.0,84304,0,"Please note, that purely coding related questions are actually better suited for [Stack Overflow](https://stackoverflow.com)",2020-08-18 17:49:23,84891.0,no update,
80462,<python><mathematics>,80463,100262.0,103697.0,84306,0,"Ok, thanks for letting me know.",2020-08-18 19:26:24,100262.0,no update,
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34709,0,"Ok to be clear, the inputs can either be ""Ca"" or ""Do"" right? Nothing else? If so then why don't you vectorize that as [1,0] and [0,1] respectively. Then the output is either dog or cat. So that should vectorized the same way. Why do you have 3 output nodes?",2018-03-27 9:09:02,29587.0,update,comment
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34710,0,"The model I'm working on has 10 classes, and 10 different kinds of input (every input belongs to one category). The actual case would be [1,0,0,0,0,0,0,0,0,0]. I edit the post to avoid confusion, the example for this post would be three inputs.",2018-03-27 9:17:50,49517.0,no update,
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34711,0,Ok so ten possible inputs and 10 possible outputs right? Based on the little words you have at the input you want to predict an associated label?,2018-03-27 9:19:35,29587.0,update,comment
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34713,0,"Exactly. Any traditional Ml classifier would predict with a 100% accuracy, because ""Do"" is always ""Dog"". Deep Learning work differently, and choosing the right number of neurons/hidden layers/epochs is not straightforward.",2018-03-27 9:21:54,49517.0,no update,
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34714,0,Why do you need to use a neural network? I think it is important to pick the right tool for the job at hand.,2018-03-27 9:22:29,29587.0,update,comment
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34716,0,"I want to include this text classifier to an image recognition algorithm, to improve the image classifier joining the two information. I will not have a single input per category, so I'll probably need a Tokenizer, but now I'm just learning how to implement simple classifiers and merge layers.",2018-03-27 9:32:12,49517.0,no update,
29570,<machine-learning><python><neural-network><keras><tensorflow>,29573,49517.0,24000.0,34717,0,"I hope this helps, let me know if you have other questions.",2018-03-27 9:39:59,29587.0,no update,
8657,<python><logistic-regression><gradient-descent>,8663,13775.0,13775.0,8336,7,Good for you for trying to write your own implementation! Try increasing your step size (eta) and checking your error converges. Adaptively reduce the step size if necessary.,2015-10-30 7:43:54,381.0,no update,
8657,<python><logistic-regression><gradient-descent>,8663,13775.0,13775.0,8344,7,"Thanks @Emre for the suggestion.  I tried adaptively updating the step size, but I still don't see the loss decreasing after successive iterations.  Any more thoughts?",2015-10-30 17:30:30,13775.0,no update,
8657,<python><logistic-regression><gradient-descent>,8663,13775.0,13775.0,8351,7,It would be better if you can be more precise in your questions. What do you mean that the loss doesn't change much? What is the expected outcome and what are you seeing?  It isn't expected to move much once it's in a vicinity of the solution...,2015-10-31 7:53:12,13786.0,update,both
32888,<machine-learning><python><scipy>,32922,53418.0,,38604,4,Is this sample data from your reading or fake/ dummy data ?,2018-06-10 2:03:05,51659.0,update,comment
32888,<machine-learning><python><scipy>,32922,53418.0,,38605,4,"it is data from the device, but is a little hardcoded, because I allowed the prototype to post even if the temperature was not correct. In the future I am planning to use real and correct data",2018-06-10 6:52:23,53418.0,no update,
32888,<machine-learning><python><scipy>,32922,53418.0,,38625,4,"Why do you think that the environment temperature will be useful in predicting the user's choice of temperature? What relationship do you expect them to have?  Have you tried visualizing your data to see if there seems to be a relationship between environment_temp and user_set_temp?  between day of the week and user_set_temp?  between time of day and user_set_temp?  The first step is always to ""look at the data"".",2018-06-10 20:01:34,8560.0,update,comment
32888,<machine-learning><python><scipy>,32922,53418.0,,38634,4,@EmanuelGiurgiu I would not recommend to go ahead with current data. but if you can get hold of local weather data for the timestamps in your data then below recommended procedure would be beneficial.,2018-06-11 3:15:55,51659.0,no update,
26496,<machine-learning><python><scikit-learn>,26512,44396.0,40853.0,30894,7,Can't you use web mining approaches?,2018-01-10 19:38:02,28175.0,update,comment
26496,<machine-learning><python><scikit-learn>,26512,44396.0,40853.0,30900,7,What do you mean by that?,2018-01-10 22:26:53,44396.0,update,comment
26496,<machine-learning><python><scikit-learn>,26512,44396.0,40853.0,30908,7,"It's a complex task to get the desired outcome as the ad titles don't follow any standard format. for example number of pieces can be placed at the start or at the end or in the middle of the title. you need to handle it carefully. Now, in the example I stated to get the first sentence as number of pieces you need to maintain a dictionary like PIC,PICS,PCS,PIECES....etc. Then you need to extract the value which is present before it(assuming that number of pieces are written before PCS as given in your sample). I gave explanation only for 1 feature. Do let me know if you need more explanation.",2018-01-11 8:05:42,40853.0,no update,
26496,<machine-learning><python><scikit-learn>,26512,44396.0,40853.0,30935,7,Are the four examples you give the only types of format there are in your dataset.  Are there more formats you have to account for?,2018-01-11 20:33:21,24409.0,update,no
26496,<machine-learning><python><scikit-learn>,26512,44396.0,40853.0,30941,7,"@grldsndrs There are countless # of formats, unfortunately. And even then, those formats will always be changing as new data comes in. Which is what makes it harder to hard code in. But Toros91 explanation helps a lot. ty",2018-01-11 21:32:56,44396.0,no update,
81961,<machine-learning><python><predictive-modeling><hyperparameter-tuning>,81990,101318.0,,85941,0,Are you training with a GPU? If no then it definitely will be slow. Yes GridSearchCV is very slow when it comes to hyperparameter optimization even when training with a GPU. You could use RandomSearchCV which is faster but the best option would be to use a Bayesian Optimizer. A library I would recommend for this is Hyperopt.,2020-09-19 14:10:11,104392.0,update,comment
81961,<machine-learning><python><predictive-modeling><hyperparameter-tuning>,81990,101318.0,,85972,0,"Okay, My computer doesn't have any GPU, so is there any way to run it on Google Colab or Kaggle Kernal? Or it provides GPU facility.",2020-09-20 8:41:23,101318.0,update,comment
81961,<machine-learning><python><predictive-modeling><hyperparameter-tuning>,81990,101318.0,,85973,0,I will add an answer,2020-09-20 8:46:19,104392.0,no update,
114294,<python><clustering><k-means>,114301,140272.0,29169.0,115465,0,"The `KMeans` class stores the cluster centers in the `cluster_centers_` attribute, which returns an array of shape `(n_clusters, n_features)` (see also [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)).",2022-09-10 15:42:12,75157.0,no update,
90772,<python><keras>,90776,112754.0,,94974,0,Have a look at this answer: https://stackoverflow.com/a/53430549/12268505,2021-03-17 15:21:54,84891.0,no update,
9670,<python><nlp><nltk>,9671,15233.0,11097.0,9472,3,"Cross posting is generally discouraged in SE. So, please delete one of the two posts.  I'm sure you'd get a nice answer here, and welcome to the site! :)",2016-01-07 10:07:27,11097.0,no update,
9670,<python><nlp><nltk>,9671,15233.0,11097.0,9473,3,"I'll go take the other one down if it's against the rules, interested to hear your guys take on this! :D",2016-01-07 10:13:56,15233.0,no update,
44973,<machine-learning><python><predictive-modeling>,44977,65902.0,65902.0,51938,1,Could you give some more detail about your dataset?,2019-02-02 19:54:08,53357.0,update,both
44973,<machine-learning><python><predictive-modeling>,44977,65902.0,65902.0,51939,1,"The data contains user names and surnames. However, due to a problem, some letters are missing. I wrote the correct values manually. After that, I would like to fix the future problems with a model. I cannot share data because user information is confidential.",2019-02-02 20:21:23,65902.0,no update,
44973,<machine-learning><python><predictive-modeling>,44977,65902.0,65902.0,51940,1,Are the names from a fixed set or the can vary in the future?,2019-02-02 20:31:53,53357.0,update,comment
44973,<machine-learning><python><predictive-modeling>,44977,65902.0,65902.0,51942,1,Names can vary in the future.,2019-02-02 20:34:41,65902.0,no update,
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57681,4,"Some details that would help (us or you) to diagnose the problem are:
1. The size of `df1` before you run this operation- both in terms of rows/columns and in terms of memory.
2. The cardinality (number of unique values) of `date`, `unit`, `company`, `city`
3. How much memory is available to the process.",2019-05-07 14:09:02,2932.0,no update,
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57684,4,"Thanks, I updated my question",2019-05-07 14:16:05,31608.0,no update,
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57687,4,Can you look at system monitor while it is running to see if you run out of memory? Just as a sanity check.,2019-05-07 15:32:28,45264.0,update,comment
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57688,4,"Yes, it says that Python3.7 takes 122GB of memory and all of my memory is being used. Usually I am using only 5GB out of 16GB of memory. I noticed that when I only have two columns in groupby date and unit that I get many NaN value rows and then I need to drop them to get the needed not-NaN value rows. That is strange. Perhaps, in this case, it is also creating NaN values, but not sure why is it happening.",2019-05-07 15:37:09,31608.0,no update,
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57689,4,"So you are basically running out of memory? You should be able to see in terminal, using a tool like [`htop`](https://delightlylinux.wordpress.com/2014/03/24/htop-a-better-process-viewer-then-top/), how much of the actual 16Gb is used. The 122 Gb measurement must include virtual memory.",2019-05-07 15:53:17,45264.0,update,both
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57690,4,"Yes, when I run that cell it says that 16/16GB is used. Which means all of it. But after a while, say 10 mins of running, the memory comes back to the normal 5GB. But it continues running after that.",2019-05-07 15:54:22,31608.0,no update,
51554,<python><pandas><jupyter><ipython>,51561,31608.0,45264.0,57691,4,Please see my answer for some suggestions :),2019-05-07 16:04:11,45264.0,no update,
85053,<machine-learning><python><deep-learning>,85054,61261.0,,89193,0,Did you try elastic search where you can reverse index and retrieve. Its the simplest and most effective. If you want to treat for spelling mistakes then you might want to run a batch process of spell mapping and then submit them to elastic search,2020-11-07 7:52:51,61634.0,no update,
85053,<machine-learning><python><deep-learning>,85054,61261.0,,89195,0,"@maheshghanta I am not familiar with elastic search. The main idea is not for spelling correction. Actually, I am trying to provide user with suggestions for the query they are typing in? Can you explain a bit on what you meant by spell mapping?",2020-11-07 8:03:33,61261.0,update,comment
78551,<python><statistics><data-science-model><data-analysis>,79796,101586.0,101586.0,83388,0,What exactly are you predicting and at what granularity?,2020-07-30 16:21:04,84891.0,update,comment
78551,<python><statistics><data-science-model><data-analysis>,79796,101586.0,101586.0,83389,0,I have updated the question.,2020-07-30 17:24:37,101586.0,no update,
78551,<python><statistics><data-science-model><data-analysis>,79796,101586.0,101586.0,83390,0,"Do you have labels for the variable you are trying to predict?  That is, do you have historical measurements of corrosion?",2020-07-30 17:48:03,75152.0,update,comment
78551,<python><statistics><data-science-model><data-analysis>,79796,101586.0,101586.0,83391,0,"Yes, I have other variables and historical measurements of corrosion, but this flow rate was stored as separate data.
Not able to figure out how to use this data(in pic), whether I should take avg or sum or .. for each year and make a new feature in the dataset",2020-07-30 18:03:33,101586.0,no update,
37168,<machine-learning><python><scikit-learn><dataset><regression>,37172,57889.0,,43519,5,What is the range of output in your train data?,2018-08-20 5:30:22,44139.0,update,no
37168,<machine-learning><python><scikit-learn><dataset><regression>,37172,57889.0,,43521,5,I am asking RANGE of output variable in your model !,2018-08-20 5:41:22,44139.0,no update,
37168,<machine-learning><python><scikit-learn><dataset><regression>,37172,57889.0,,43522,5,"Output range is (99.12,5628)",2018-08-20 5:56:52,57889.0,no update,
37168,<machine-learning><python><scikit-learn><dataset><regression>,37172,57889.0,,43523,5,"Well, your RMSE and R2 give the same interpretation, model A is better than B. As for MAPE, I can't say the actual problem, but it might be because of the errors cancelling each other out. Take a look at this- http://qr.ae/TUNpEP and https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d",2018-08-20 6:13:28,44139.0,no update,
14774,<python><visualization><geospatial>,14785,25611.0,29169.0,16417,14,Folium has a [plugin for heat maps](https://github.com/python-visualization/folium/blob/master/folium/plugins/heat_map.py). [Here is an example](http://qingkaikong.blogspot.com/2016/06/using-folium-3-heatmap.html).,2016-10-27 5:41:17,381.0,no update,
14774,<python><visualization><geospatial>,14785,25611.0,29169.0,16430,14,"Thanks @Emre that's exactly what I needed.  Sadly Mode doesn't support folium yet, but I'll bug their customer service and see if they'll load it.",2016-10-27 16:32:02,25611.0,no update,
14774,<python><visualization><geospatial>,14785,25611.0,29169.0,36314,14,Another good library for this is `gmplot` which also let's you export the map as a standalone html file. [Here](https://eatsleepdata.com/data-viz/how-to-generate-a-geographical-heatmap-with-python.html) is a tutorial on using it.,2018-04-27 18:25:40,27015.0,no update,
46782,<python><pandas><matplotlib>,46789,65776.0,,53588,0,Without sample data it is hard to understand where the names are stored.,2019-03-06 12:26:18,10372.0,no update,
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,77891,1,so u expect the coef of X1 to be negative because it is inversely proportional to Y?,2020-04-27 22:26:18,91700.0,update,comment
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,77892,1,"Yes, that's it.",2020-04-27 22:43:40,95905.0,no update,
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,77897,1,How can I achieve that?,2020-04-28 1:25:09,95905.0,update,post
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,77898,1,"What exactly do you mean by ""$X_1$ is known to be inversely proportional to $Y$""?  Univariate analysis, domain knowledge, theoretical fact, ...?",2020-04-28 2:15:36,55122.0,update,post
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,77899,1,"It's only domain knowledge. I'm working on a model to make some prediction scenarios. One of these scenarios consists of raising the values of $X_{1}$. I expect $Y$ to go down, but $Y$ raises too, which in the domain of the features is wrong behaviour.",2020-04-28 2:31:41,95905.0,no update,
73114,<python><regression><multivariate-distribution>,73128,95905.0,95905.0,78066,1,See also https://stats.stackexchange.com/a/73666/232706,2020-05-01 2:15:08,55122.0,no update,
20289,<python><nlp>,20312,35399.0,,24680,1,"If your non-article scraps are not totally random, one fast way is to manually select some strings as flags (e.g. ""#signInForm"", ...) that probably denote a non-article and use them to discard the documents. This can be easily implemented and fast checked and will save you the trouble of doing something more sophisticated, such as outliers detection or document clustering.. So if possible, I would try the simpler approach first.",2017-07-10 9:17:36,8370.0,no update,
20289,<python><nlp>,20312,35399.0,,24686,1,"@Bogas Thanks for the suggestion! However, scraps are usually random to the point where it's very hard to guess... That being said, there might be a select few I can detect by doing this method.",2017-07-10 14:40:54,35399.0,no update,
82012,<machine-learning><python><xgboost><overfitting><classifier>,88471,82764.0,86339.0,86005,6,"Hi! As a sanity check, can you take some of your positive training examples and put them through your inference pipeline and confirm they are still coming out positive? Can you find any examples in your 43k which you know should be positive given the training data? Does the new data have any missing values? Is it featurized in the same way as the training set? Without know more I wouldn't worry about the degree of imbalance you have but would focus more on sanity checks to make sure nothing has changed in terms of the model/feature pipelines between training and scoring.",2020-09-21 3:10:01,104498.0,update,no
82012,<machine-learning><python><xgboost><overfitting><classifier>,88471,82764.0,86339.0,86020,6,"@BrandonDonehoo Hi to you as well! Thanks for your reply, this is great stuff. If you don't mind, I do have a few questions. In order of the way you posed your recommendations:
1.  To clarify, you suggest training with ONLY positive cases, then make predictions on my production set and just see if it will make positive predictions?
2. The data is too sparse for this kind of check to be done manually :(.
3. This is something I should have checked for. I will just drop any rows if a NaN exists and see how it behaves!
4. Yes, predset is being hash encoded. (Considering Target)
Thanks again!",2020-09-21 12:27:24,82764.0,no update,
82012,<machine-learning><python><xgboost><overfitting><classifier>,88471,82764.0,86339.0,91448,6,Any results from the testing you alluded to in your last comment?  You allude in the post to the model being very confident that all the test samples are negative; is it similarly confident about negative and/or positive samples in the training set?  Could you provide the feature preprocessing code?,2020-12-31 15:48:44,55122.0,update,comment
114959,<machine-learning><python><nlp><scikit-learn><text-classification>,114989,109113.0,109113.0,116078,3,What model type did you use and what's the domain of the task?,2022-10-06 13:31:29,84891.0,update,both
114959,<machine-learning><python><nlp><scikit-learn><text-classification>,114989,109113.0,109113.0,116079,3,@Jonathan Not sure what you're asking. The domain of the task is multi class text classification.,2022-10-06 13:37:51,109113.0,no update,
81328,<machine-learning><python><supervised-learning><naive-bayes-classifier><naive-bayes-algorithim>,81331,96815.0,96815.0,85188,2,Please give more detail about your process: how many instances? Size of documents? What are the 39 features? How many words features? It might also be useful to look at precision/recall instead of only accuracy.,2020-09-07 11:10:51,64377.0,update,both
81328,<machine-learning><python><supervised-learning><naive-bayes-classifier><naive-bayes-algorithim>,81331,96815.0,96815.0,85194,2,"Hi Erwan, I updated the question. I think I answered to all your questions. Please let me know if you need more info. Thank you so much",2020-09-07 11:31:17,96815.0,no update,
106532,<machine-learning><python><linear-regression><online-learning>,106563,111822.0,,108638,1,try first 500 or even 1000 points,2021-12-29 16:30:43,100269.0,no update,
106532,<machine-learning><python><linear-regression><online-learning>,106563,111822.0,,108647,1,"I tried for various combinations, yet SGDRegression doesn't seem to get the correct pattern at all. Even raising the initial learning to 1000, I get a slope of 17.78 and intercept of 24.30, despite my lowest value in the entire dataset being 100.0. That's some serious underfitting that I don't understand.",2021-12-29 16:53:42,111822.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71742,2,Could you give an example data of X and Y in order to make the code reproducible?,2020-01-13 8:35:09,86339.0,update,comment
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71744,2,"sure, sorry. For example, X = ""How to join amazon company "". Y = [""Career Advice"", ""Fresher ""]. @CarlosMougan",2020-01-13 9:02:17,88280.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71748,2,"Try to make the question reproducible, so if we copy paste we can debug the same code that you have. Per example there are somethings that you dont´t have defined such as mind_df, ngram_range, clf and the class imports",2020-01-13 9:39:32,86339.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71749,2,"[link](https://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/), This code has assisted me a lot, my model is working fine, The only problem I am facing is that after I pickle the model and try to use it again, I get the error I have mentioned in the question above. I only want to know, how """" multilabel_binarizer.inverse_transform() """" function will work after pickling the model. May be I am not able to pickle this funtion. Apart from this there is no other problem. @CarlosMougan .",2020-01-13 9:46:51,88280.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71751,2,why dont you perform the Multilabel inside the pipeline? It has a transformer implementation.,2020-01-13 10:05:29,86339.0,update,both
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71752,2,"In the pipeline, I have vectorizer and my model (clf), and when I fit my pipeline, I am using xtrain and ytrain both for fitting. But in case of multilabel binarizer, it takes labels(ytrain) ONLY for fit and transform. So there it throws the error. @CarlosMougan",2020-01-13 10:12:18,88280.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71754,2,Have you tried One Hot Encoder instead of multilabel binarizer?,2020-01-13 10:28:51,86339.0,update,comment
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71755,2,And I dont know which classifier are you using but some of them support multiclassification so you would avoid this part.  Logistic Regression of sklearn supports multiclass,2020-01-13 10:32:07,86339.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71756,2,"No, can you help me by sharing some good content which can be useful in my case? @CarlosMougan",2020-01-13 10:32:54,88280.0,update,post
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71757,2,"What happens if you take out this lines: ""mlb = MultiLabelBinarizer()
multilabel_y = mlb.fit_transform(y)"" and call p.fit(X, y)??",2020-01-13 10:34:23,86339.0,update,post
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71758,2,I have used onevsrest model using logistic regression. @CarlosMougan,2020-01-13 10:34:32,88280.0,no update,
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71759,2,If you take out those lines? Does it give you an error? The sklearn.LogisticRegression will do multiclassication for you.,2020-01-13 10:36:17,86339.0,update,comment
66380,<machine-learning><python><multiclass-classification><data-science-model><multilabel-classification>,66397,88280.0,86339.0,71762,2,"NO, there are no errors, thank you so much Sir, I was stuck in this part for more than 3 days. But now it is all good. And all thanks to you. Logistic regression did multiclassification for me. I cannot stop thanking you. @CarlosMougan",2020-01-13 10:42:51,88280.0,no update,
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,10121,4,"Why should it be any different from, say, English? Python can transparently process UTF8. Do you foresee any specific problem with grammar or anything?",2016-02-13 14:55:51,15361.0,update,no
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,10123,4,"@Diego, as I understand, I need a dictionary of words of the Russian language with the weight of each word (something like [this](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) for English). Which library could you advise my?",2016-02-13 19:51:37,9992.0,update,no
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,10126,4,"May be you should replace the word library with the word dictionary in your question. Python library is usually referring to a bunch of functions and classes, not just a collection of words.",2016-02-14 1:25:15,15361.0,no update,
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,10145,4,"@Diego, thank you for recommendation, but I think my question correctly shows that I'm 
interested in dictionary or library. Both for me will be ok.",2016-02-15 16:22:04,9992.0,no update,
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,21737,4,This may be a duplicate of https://datascience.stackexchange.com/questions/6691/sentiment-analysis-model-for-spanish,2017-05-10 15:55:34,31513.0,no update,
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,21739,4,Possible duplicate of [Sentiment Analysis model for Spanish](https://datascience.stackexchange.com/questions/6691/sentiment-analysis-model-for-spanish),2017-05-10 15:56:42,31513.0,no update,
10211,<python><nlp><sentiment-analysis>,10795,9992.0,31513.0,50896,4,"Have a question for polyglot solution. Because Russian language was not set explicitly, how do you know that the word polarity is correct for Russian and not for English which I assume is the default language used by polyglot sentiment module?",2019-01-10 18:52:32,65662.0,no update,
28158,<machine-learning><python><scikit-learn><cross-validation>,28159,46433.0,53476.0,32874,29,See also: [Choice of k in k-fold cross-validation](https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation).,2018-02-22 12:56:35,46697.0,no update,
63174,<python><scikit-learn><decision-trees>,63177,58438.0,58438.0,68358,1,"The default settings include Gini impurity as the criterion, not entropy. Are you sure you set all other values to default / can you share the code sample?",2019-11-15 6:12:36,60503.0,update,comment
63174,<python><scikit-learn><decision-trees>,63177,58438.0,58438.0,68359,1,One reason for the tree not splitting some nodes could be that observations sharing the exact same features have different classes. Do you have such data?,2019-11-15 6:14:39,60503.0,update,comment
694,<machine-learning><python><neural-network>,695,989.0,8432.0,1733,150,See also http://stackoverflow.com/q/2276933/2359271,2014-07-08 20:14:34,322.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,2291,150,"If you want to only use Restricted Boltzmann Machine, you can stick with scikit-learn as well.",2014-09-17 11:42:43,3342.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6758,150,And now there's a new contender - [Scikit Neuralnetwork](http://scikit-neuralnetwork.readthedocs.org/en/latest/): Has anyone had experience with this yet? How does it compare with Pylearn2 or Theano?,2015-07-07 12:20:58,10519.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6781,150,"There is also Keras - https://github.com/fchollet/keras - which is relatively recent. The problems with tracking ""best"" by any measure, and keeping the Q&A valid over time is why this sort of question is usually off topic in other Stack Exchange networks.",2015-07-09 8:37:26,836.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6791,150,"Does any of this packages scale like h2o deep learning?
As far as I know lasagne doesn't.Theano does support GPU so as any library basing on it,but does any of them support mapreduce or spark.",2015-07-09 21:20:08,10327.0,update,no
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6807,150,h2o doesn't even use the GPU yet so it's hardly scalable.,2015-07-11 6:51:23,381.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6826,150,"@Emre: Scalable is different to high performance. It typically means you can solve larger problems by adding more resources of the same type you have already. Scalability still wins out, when you have 100 machines available, even if your software is 20 times slower on each of them . . . (although I'd rather pay the price for 5 machines and have benefits of both GPU and multi-machine scale).",2015-07-13 15:25:45,836.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,6829,150,"So use multiple GPUs...nobody uses CPUs for serious work in neural networks. If you can get Google-level performance out of a good GPU or two, just what are you going to do with a thousand CPUs?",2015-07-13 16:11:09,381.0,update,comment
694,<machine-learning><python><neural-network>,695,989.0,8432.0,8833,150,"I'm voting to close this question as off-topic because is become a poster example of why recommendations and ""best"" questions don't work in the format. The accepted answer is factually inaccurate after 12 months (PyLearn2 has in that time gone from ""active development"" to ""accepting patches"")",2015-11-26 17:33:08,836.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,9032,150,"I can't vouch for it, but [Brainstorm](https://github.com/IDSIA/brainstorm) seems to be the spiritual successor to PyBrain and looks promising.",2015-12-08 17:59:14,14617.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,24898,150,Cross-site duplicate: https://scicomp.stackexchange.com/q/5110/3212,2017-07-17 23:33:50,6.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,27621,150,"Please define your criteria for ""best""",2017-10-13 11:06:25,40422.0,no update,
694,<machine-learning><python><neural-network>,695,989.0,8432.0,48495,150,"I'd add CNTK , presented in jun 2017 by Microsoft , Python API stable , with performance competiting with tensorflow https://minimaxir.com/2017/06/keras-cntk/",2018-11-19 11:51:19,62867.0,no update,
11314,<python><statistics><anomaly-detection>,11376,18019.0,18019.0,11436,5,"@mods, can i move/duplicate this question at stats.stackexchange.com? i believe i can get some help there as well!",2016-04-21 14:13:31,18019.0,update,comment
65980,<python><classification><categorical-data><supervised-learning>,65985,82276.0,82276.0,71328,2,Hi and welcome to the site! Can you share a couple of examples what your data exactly looks like? And what exactly are you trying to classify (supervised)? Or is it rather about clustering (unsupervised)?,2020-01-06 19:55:50,84891.0,update,comment
65980,<python><classification><categorical-data><supervised-learning>,65985,82276.0,82276.0,71331,2,Edited and add some examples,2020-01-06 20:06:39,82276.0,no update,
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14034,0,Do you have details about the number of samples written by a single person? Is it 200 together or by each?,2016-07-13 8:51:45,21024.0,update,post
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14052,0,"It is 200 together (i.e., two samples per person).",2016-07-13 12:16:19,21396.0,no update,
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14056,0,Do you just want a person to handwriting match? Or a ranking giving the highest priority to the ones with the maximum match?,2016-07-13 12:51:59,21024.0,update,both
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14062,0,"Just a match. E.g, if I have person_1_writing_sample_1, person_1_writing_sample_2, person_2_writing_sample_1, and person_2_writing_sample_2, I want to match the two former and the two latter.",2016-07-13 13:04:22,21396.0,no update,
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14073,0,Try k-means with 100 clusters. You should be able to find a library for it in every language.,2016-07-13 18:50:47,381.0,no update,
12755,<machine-learning><python><r>,12784,21396.0,21024.0,14086,0,"I hasten to add that you can't run a clustering algorithm (like k-means) with free-form text; you need to featurize it first, and you want [stylometric features](https://en.wikipedia.org/wiki/Stylometry) in particular. See e.g. [Using Machine Learning Techniques for Stylometry](http://www2.tcs.ifi.lmu.de/~ramyaa/publications/stylometry.pdf), [Conversationally-inspired Stylometric Features for Authorship Attribution in Instant Messaging](http://www.dcs.gla.ac.uk/~vincia/papers/stylometry-MM-2012.pdf) or [Adversarial Stylometry](https://www.cs.drexel.edu/~sa499/papers/adversarial_stylometry.pdf)",2016-07-14 0:55:34,381.0,no update,
21994,<python><pandas><csv><tableau>,21999,13235.0,,25550,2,I'm voting to close this question as off-topic because it was cross posted,2017-08-06 10:54:39,21.0,no update,
21994,<python><pandas><csv><tableau>,21999,13235.0,,25551,2,"@SeanOwen I've deleted the other post. The question fits better into data science and the answer below is correct, so please reopen it here.",2017-08-06 11:06:27,13235.0,no update,
103903,<python><anaconda><spyder>,103946,119921.0,119921.0,107107,1,Just for the joke. Do not use Anaconda :D. It is bloated. You can do almost anything with a standard python installation.,2021-11-07 20:02:10,69670.0,no update,
103903,<python><anaconda><spyder>,103946,119921.0,119921.0,107128,1,I know it now but it's too late:P,2021-11-08 11:47:38,119921.0,no update,
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23483,7,"Logistic regression should have a single stable minimum (like linear regression), so it is likely that something is causing this that you haven't noticed",2017-07-04 15:57:02,836.0,no update,
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23494,7,So there must be guaranteed convergence to the minimum cost? Would you be able to do a code review for me please?,2017-07-04 23:28:25,33716.0,update,comment
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23498,7,"If there's a lot of code you need reviewing, maybe post it on codereview.stackexchange.com - if it is only a small amount required to replicate the problem, you could add it to your question here (edit it in as a code block, please include enough to fully replicate the problem).",2017-07-05 6:52:00,836.0,no update,
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23538,7,"while it is true that ensuring a global minimum should give you same result regardless of the optimization algorithm, there can be subtleties in the implementation of the algorithm (i.e. the methods to handle numerical stability etc) that may lead to slightly different solutions. These small difference in solutions may lead to larger performance difference when evaluated on small test set. May be that is causing such a large performance difference in your case. And yes, in general, optimization algorithms can largely influence the learning outcome. Btw, I got the desired result in MATLAB.",2017-07-06 6:30:50,26127.0,no update,
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23552,7,"@NeilSlater: ok, I have just added the code directly into the question as an edit. Does it look ok?",2017-07-06 15:07:22,33716.0,update,comment
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23553,7,@Sal: I see. May I ask if you had taken a look at my code to see if it is correct?,2017-07-06 15:07:56,33716.0,update,comment
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23555,7,"@AKKA: I cannot see any obvious problem with the code after 10 mins scanning through it. Best I can think is that maybe the default value of `gtol` is too high for your data, which could occur if it is not normalised. I might take a deeper look later on, if no-one else has answered in the meantime.",2017-07-06 15:44:54,836.0,no update,
20170,<machine-learning><python><logistic-regression><accuracy>,20259,33716.0,33716.0,23644,7,"@NeilSlater: I had tried reducing the value of `gtol` already, based on a suggestion from someone in a replicate post of my question on stackoverflow: https://stackoverflow.com/questions/44915145/coursera-ml-does-the-choice-of-optimization-algorithm-affect-the-accuracy-of-m?noredirect=1#comment76806558_44915145 . Reducing `gtol` does not solve the problem :( no one has answered since you. Could you please help me take a deeper look? Thank you",2017-07-08 7:53:34,33716.0,no update,
55811,<python><pandas><csv><data-formats>,55939,38793.0,,61499,2,Pandas wide-to-long command could be helpful https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html,2019-07-17 10:06:00,71442.0,no update,
55811,<python><pandas><csv><data-formats>,55939,38793.0,,61515,2,@Peter can you please post your code in the answer section,2019-07-17 13:17:10,38793.0,no update,
55811,<python><pandas><csv><data-formats>,55939,38793.0,,83686,2,You may be interested in [How to deal with multi-level column names downloaded with yfinance?](https://stackoverflow.com/questions/63107594),2020-08-05 0:11:36,87159.0,no update,
28472,<python><neural-network><classification><computer-vision>,28515,47074.0,35644.0,33260,4,"Isn't it a challenge of a feature selection? If you calculate for each pixel the number of adjacent pixels with the same color and takes the maximum value as a feature of the image, you'll get a small number for teh group 1 and higher number for the group 2.",2018-03-01 17:30:39,10620.0,no update,
28472,<python><neural-network><classification><computer-vision>,28515,47074.0,35644.0,33279,4,"Do you want the answer to specifically uses neural networks? Or are other approaches acceptable (such as the rule based approaches mentioned)? If you are open to any technique, that could change people's answers, rather than if you're only interested in neural network techniques. You also mention ""any color for the small point in group 2, this includes all possible gray values, is that correct?",2018-03-01 22:54:38,37095.0,update,comment
28472,<python><neural-network><classification><computer-vision>,28515,47074.0,35644.0,33316,4,The final goal of this work is to be used with photos of metalic surfaces taken with a color camera. So the ideal is to be used with CNN.,2018-03-02 10:34:16,47074.0,no update,
38548,<python><time-series><lstm><forecasting>,38553,59377.0,-1.0,48587,1,"I had answered a similar question before, here are some tricks you can use to avoid or suppress the replication problem. https://stackoverflow.com/questions/52252442/how-to-handle-shift-in-forecasted-value/53141558#53141558",2018-11-21 13:25:16,60954.0,no update,
33910,<python><scikit-learn><feature-selection><training>,33925,49884.0,,51566,6,"Just a quick question, I tried the implementation in the answer
I tried converting avgw2v and tfidfw2v vectors into array as I was getting the same problem mentioned in the question. The fact is model's performance had dropped drastically with this implementation. My roc score is just 60 and its was 85 when I didnot convert the vectors into array. Any reason for this massive change in performance?",2019-01-25 16:20:05,65323.0,no update,
103603,<machine-learning><python><nlp><scikit-learn><prediction>,103623,117548.0,,106842,1,"a few hints: 1) many algorithms can perform very well (eg CARTs, SVMs, ANNs, ..) 2) You can encode the keywords (eg one-hot-encoding, label encoding, word vector) and use them as inputs into chosen algorithm 3) under NFL theorem, no algorithm is dramatically better than others; keep this in mind, there is no magic bullet algorithm, only trial and error",2021-10-29 15:19:55,100269.0,no update,
103603,<machine-learning><python><nlp><scikit-learn><prediction>,103623,117548.0,,106877,1,"I tried, first converting into the array through count vectorizer, I got the predictions using DecisionTreeClassifier, but I just got a single prediction after inputting the symptoms. Is there any way to receive the best five predictions also with their percentage matches?",2021-10-30 7:35:04,117548.0,no update,
46541,<python><pandas><dataframe>,46543,53648.0,45264.0,53421,1,I assume it is a typo - but there isn't actually a duplicate in row z anyway because one `b` also has a space: `'b '`.,2019-03-02 20:04:21,45264.0,no update,
46541,<python><pandas><dataframe>,46543,53648.0,45264.0,53422,1,right. I've made a correction. thx.,2019-03-02 20:41:06,53648.0,no update,
38792,<python><regression><xgboost><kaggle><boosting>,95124,59652.0,,48876,1,So on how many different datasets / tasks did you do your comparison experiment? 1?,2018-11-29 18:59:49,21257.0,update,comment
61428,<python><classification><time-series><pandas><churn>,61437,32605.0,32605.0,66833,1,"If anyone could point me in the right direction, that would be great. If not,  tell me if I should add more information or post this in some other forum.",2019-10-08 13:54:43,32605.0,no update,
32405,<python><numpy>,32406,52713.0,,38067,2,What is `U` parameters for? Is this supposed to be a neural network?,2018-05-30 16:01:05,29587.0,update,both
32405,<python><numpy>,32406,52713.0,,38068,2,"Yes, it's a simple naive implementation of RNN found [here](https://www.manning.com/books/deep-learning-with-python)",2018-05-30 16:16:08,52713.0,no update,
66973,<python><random-forest><cross-validation><python-3.x><gridsearchcv>,66975,88088.0,85045.0,72349,2,"What do you mean by ""breaks up""? An error message (provide it please), python crashes, your computer crashes, ...?",2020-01-24 13:46:39,55122.0,update,comment
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66938,1,Can you try `model.predict(prepare('PyCharmProject\dog.jpg'))`?,2019-10-10 13:31:59,49590.0,update,both
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66939,1,"Yes, I've tried before and with some hope I just tried again but I just returns the same error.",2019-10-10 13:45:33,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66940,1,Could you reshape your input to `X.shape[1:]` in your model?,2019-10-10 14:09:06,49590.0,update,comment
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66941,1,"But that is the way it is, you can see the way I've build my model in the lowest code box.
My first layer is this: model.add(Conv2D(layer_size, (3,3), input_shape = X.shape[1:]))",2019-10-10 14:26:32,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66942,1,I meant the output shape of `prepare('PyCharmProject\dog.jpg')` should match the shape of `X.shape[1:]` in your model. Does that make sense?,2019-10-10 14:33:39,49590.0,update,comment
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66943,1,"Yes, I understand what you mean, but I am not sure how to do it code as the error says the output shape should be ""shape=(None, None, None, 1)"" while in the prepare function I'm setting the shape to be ""reshape(-1, IMG_SIZE, IMG_SIZE, 1)"", while IMG_SIZE = 50 as that was the resolution of the images I used to train the model.",2019-10-10 14:43:43,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66945,1,Can you add in `print(X.shape[1:])` in your model?,2019-10-10 14:55:43,49590.0,update,comment
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66947,1,"After running it the output was: (50, 50, 1)",2019-10-10 15:09:36,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66948,1,"In your `prepare` function, just return `new_array` instead of reshaping it. Also do `print(new_array.shape)` before the return statement just to make sure the shape is consistent with `(50, 50, 1)`",2019-10-10 15:14:55,49590.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66949,1,"After doing it printed out ""(50, 50)"" and I received an error stating: ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (50, 50)",2019-10-10 15:22:24,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66951,1,"just do `return new_array.reshape(IMG_SIZE, IMG_SIZE, 1)`. Hopefully this should work?",2019-10-10 15:28:25,49590.0,update,comment
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66956,1,"I've attempted that before, it returns an error that it needs a 4 dimensional array. Actually I'll just put the error here: ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (50, 50, 1)",2019-10-10 15:50:55,82016.0,no update,
61557,<python><deep-learning><tensorflow><data-science-model>,61582,82016.0,12359.0,66973,1,"I found the solution, I just had to normalize the data just like I did when building the model. Basically I just added this ""img_array = img_array/255.0"" inside the the prepare function and now it works! Thank you for trying to help me Danny, I really appreciate it!",2019-10-10 23:39:46,82016.0,no update,
16835,<python><deep-learning><tensorflow><pytorch>,18073,14675.0,14675.0,19117,27,You can also add [this ongoing (as of writing this comment) discussion](https://discuss.pytorch.org/t/about-the-variable-length-input-in-rnn-scenario/345) by pytorch's contributors to your list of references.,2017-02-15 3:41:05,25984.0,no update,
16835,<python><deep-learning><tensorflow><pytorch>,18073,14675.0,14675.0,20291,27,"I found this link very interesting and it is comparing as you asked for(about dynamic graph and using DyNet and Chainer)
https://hackernoon.com/how-is-pytorch-different-from-tensorflow-2c90f44747d6",2017-03-28 16:48:43,30489.0,no update,
71636,<python><dataset><data><data-cleaning><pca>,71648,93899.0,,76609,0,"Although the suggested answer does what you described, but I wonder what this kind of standardization mean? Maybe other features are directly correlation to velocity, what about their distribution, normally distributed? I am not sure I understand what goes on here. You may want to ask this question at https://stats.stackexchange.com/",2020-04-03 7:14:32,44456.0,update,comment
71636,<python><dataset><data><data-cleaning><pca>,71648,93899.0,,91901,0,"1) it’s not clear why it was suggested to standardize by  velocity, I guess it was some misunderstanding. Maybe you could clarify with the person who suggested it? 2) what does it mean „improve PCA“? 3) in general, it is advisable to standardize the data before PCA if the columns have significantly different scale. However, one takes the mean and standard deviation of particular column. See the following question and the questions/ answers linked therein. https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca",2021-01-10 21:21:12,29781.0,update,no
82742,<machine-learning><python><classification><predictive-modeling><text-classification>,82751,96922.0,96922.0,86807,3,"recently, I answered a similar Q. See it [here](https://datascience.stackexchange.com/questions/75364/better-approach-to-assign-values-to-determine-potential-fake-sentences/82685?noredirect=1#comment86753_82685)",2020-10-08 14:06:31,11061.0,no update,
82742,<machine-learning><python><classification><predictive-modeling><text-classification>,82751,96922.0,96922.0,86809,3,"thanks mnm. Based of your previous answer, I updated my question to be more specific on my difficulties. Also can it be considered a machine learning algorithm what I am proposing?",2020-10-08 14:17:47,96922.0,update,post
27949,<machine-learning><python><scikit-learn><decision-trees><performance>,27950,46431.0,,32616,1,are your features numerical?,2018-02-18 9:24:24,28175.0,update,both
27949,<machine-learning><python><scikit-learn><decision-trees><performance>,27950,46431.0,,32617,1,There are a few numerical and a few categorical features,2018-02-18 9:25:38,46431.0,no update,
27949,<machine-learning><python><scikit-learn><decision-trees><performance>,27950,46431.0,,32618,1,Actually I've never seen scaling categorical features.,2018-02-18 9:26:56,28175.0,no update,
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56604,1,Have you tried the DataFrame's `to_dict` method?,2019-04-18 11:10:02,45374.0,update,no
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56605,1,"yes, but I couldnot achieve the multi-key dictionary I require as mentioned above. `df = df.set_index([""item1"", ""item2""]) # Columns  for dict keys
df_dict = df.to_dict(""index"") # Turn into dict `  It gives error",2019-04-18 11:12:14,31599.0,no update,
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56608,1,@bradS can you create the dictionary?,2019-04-18 12:10:45,31599.0,update,both
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56619,1,Does this help: https://stackoverflow.com/questions/52192177/convert-pandas-dataframe-to-dictionary-with-multiple-keys ?,2019-04-18 13:48:19,45374.0,update,no
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56639,1,"Why do you want to do this? There might be a nicer data structure to use. The original dataframe is already very useful as it is, in my opinion. If you want to check combination of `name` and other variables, you could e.g. use the [`DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) method to then do something for each value of `name`.",2019-04-19 0:12:07,45264.0,update,post
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56640,1,@n1k31t4 I want to use gurobi optimization as in https://www.gurobi.com/documentation/8.1/examples/diet_py.html  I want to convert into dcitionary format,2019-04-19 1:22:48,31599.0,update,post
49529,<python><pandas><indexing>,49590,31599.0,45264.0,56671,1,"@KHANirfan - ok, I understand. Please see my answer for a way to do it  :)",2019-04-19 14:50:26,45264.0,no update,
49529,<python><pandas><indexing>,49590,31599.0,45264.0,69613,1,"In general this is unnecessary, probably a bad idea, and Gurobi doesn't strictly need it anyway: `nutritionValues = { ('hamburger', 'calories'): 410, ...`. Why not simply use a nested defaultdict-of-dict, that's what people generally do? But yeah you could use pandas multi-index. Whatever data structure you create may be a pain to use/export/import/pickle outside pandas.",2019-12-09 1:26:40,8501.0,update,post
82028,<python><scikit-learn><xgboost><optimization>,82350,98535.0,,86033,3,"When you try your classifier the hyper-parameters do not match the range of the param-grid. For instance, the learning rate is 0.7 whereas in your grid search you have a list with values [0.1,0.01,0.05], why is that?",2020-09-21 17:43:20,14560.0,update,no
82028,<python><scikit-learn><xgboost><optimization>,82350,98535.0,,86055,3,@Grzegors I did 0.7 because when I run it first round I have gotten 0.1 and then I tried bigger numbers and in the end it was 0.7,2020-09-22 6:16:32,98535.0,no update,
82028,<python><scikit-learn><xgboost><optimization>,82350,98535.0,,86231,3,@Reut Could you describe what your process was for optimization? And more about your data.,2020-09-24 15:41:42,45799.0,update,both
77399,<python><r><statistics><correlation>,77527,100460.0,100460.0,82018,2,"As a heads up, there is a bioinformatics Stack: https://bioinformatics.stackexchange.com/.",2020-07-10 19:01:00,73930.0,no update,
77399,<python><r><statistics><correlation>,77527,100460.0,100460.0,82172,2,I didn't know! Thank you!,2020-07-13 16:45:07,100460.0,no update,
47735,<python><neural-network><keras><cnn>,47739,69972.0,69972.0,54775,3,Are you computing validation metrics on only batchSize examples? Maybe you should use more examples to get a more robust estimate of those metrics?,2019-03-21 13:20:28,33202.0,update,post
47735,<python><neural-network><keras><cnn>,47739,69972.0,69972.0,54778,3,"As far I understand,         ""validation_steps=9"" in model.fit_generator should make it so that the entire validation set is iterated over. But I might be wrong there, i will look into it",2019-03-21 13:40:51,69972.0,no update,
47735,<python><neural-network><keras><cnn>,47739,69972.0,69972.0,54788,3,Could be. I don’t use generators for validation data so I’m not sure if the specifics,2019-03-21 16:19:47,33202.0,no update,
28825,<machine-learning><python><clustering><jupyter>,29267,43455.0,43455.0,33844,1,Is there a problem left?,2018-03-09 17:15:42,381.0,update,comment
28825,<machine-learning><python><clustering><jupyter>,29267,43455.0,43455.0,34299,1,No. I already solved that problem and I also added the solution of my problem.,2018-03-18 15:35:31,43455.0,no update,
28825,<machine-learning><python><clustering><jupyter>,29267,43455.0,43455.0,34306,1,Please add the solution as an answer and accept it.,2018-03-19 0:03:18,381.0,update,no
47162,<python><data>,47254,61508.0,,54018,1,"`f` is a list (see `help(glob.glob)`). Instead of converting it to a string and then stripping the brackets, why don't you just get the element with `f[0]`? Have you tried `os.listdir(f[0])`?",2019-03-12 14:27:41,53479.0,update,both
47162,<python><data>,47254,61508.0,,54021,1,"@mapto, thank you - this does work! Appreciate your feedback.",2019-03-12 14:58:14,61508.0,no update,
47162,<python><data>,47254,61508.0,,54026,1,What do you actually want to do? Just count the number of files in a directory? Your `glob.glob(ct)` doesn't show you are doing any fancy filtering or anything. Or do you want to get the number of files in many directories?,2019-03-12 16:54:08,45264.0,update,post
47162,<python><data>,47254,61508.0,,54108,1,"@n1k31t4, yeah I just want to count files in a directory. `ct` is a string that contains wildcard characters.",2019-03-13 17:20:45,61508.0,no update,
47162,<python><data>,47254,61508.0,,54109,1,"@ChrisTennant - in that case, if `glob` is already returning the filenames that you want (i.e. that match you wildcard expression) then you can simply use the length of the returned list: `len(f)`. If it is the case that each element in the list `f` is a folder, and you want to count the number of files in each of those folders, then just loop over `f` after performing the glob. I'll add an answer with example code.",2019-03-13 17:29:07,45264.0,no update,
74724,<python><neural-network><deep-learning><keras><image-classification>,74800,97738.0,43000.0,79463,1,I suggest using [one hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) method for output? Instead of only one output?,2020-05-24 2:10:19,97745.0,update,post
103684,<python><keras>,103687,127196.0,112944.0,106935,0,"I think You should use `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)` to split the data, you can't assign `dataframe` directly to `tuples`",2021-11-01 16:00:08,83275.0,no update,
15962,<machine-learning><python><tensorflow><optimization><gradient-descent>,15976,27420.0,27420.0,17833,11,Can you please tell how have you initialized your weights?,2016-12-28 4:30:10,21608.0,update,no
15962,<machine-learning><python><tensorflow><optimization><gradient-descent>,15976,27420.0,27420.0,17997,11,"I suggest trying the Adam solver.  It seems to be better-behaved and a better default, and you might be able to use the default learning rates for it.",2017-01-05 0:26:04,8560.0,no update,
54602,<python><data-mining><pca><dimensionality-reduction><variance>,54609,57323.0,,60297,0,"Please provide more explanation like what is three PCs(does it mean three features?),PC1, possibly after that I can provide some answer.",2019-06-27 9:57:24,73441.0,update,both
103769,<python><pandas>,103780,127301.0,43000.0,106997,0,To perform this comparison it's important to know how the two dataframe relate to each other (if at all). Is there a key that can be used to connect the two or is there some other way to relate the two dataframes?,2021-11-03 12:48:12,75157.0,update,comment
103769,<python><pandas>,103780,127301.0,43000.0,107002,0,"This is a code question. You can totally post in on Stack Overflow, with ""Pandas"" tag, you might get a bit more answers",2021-11-03 14:55:48,101580.0,no update,
103769,<python><pandas>,103780,127301.0,43000.0,107003,0,@Oxbowerce ... they are build separately from different sources. The only common what they have is the TYP <-> SOFTWARE_NAME and VERSION <-> VERSION. No other keys available.,2021-11-03 15:28:18,127301.0,no update,
5199,<python>,5201,8368.0,,5504,1,"I think that it is impossible to answer this question as it's stated. You may have a better chance of getting a reasonable answer, if you will provide more details about your project(s), goals, specific packages, users (including their number) and requirements for the setup. Keep in mind that, in addition to _local machines_ and _local servers_, you have various **cloud** options, such as _Amazon Web Services (AWS)_.",2015-02-21 7:47:31,2452.0,no update,
5199,<python>,5201,8368.0,,5505,1,"This question is not really Data Science related, as it stands now.  Maybe you can elaborate and make the connection clearer?  Otherwise, you may want to move it to Programmers Stack Exchange.",2015-02-21 11:37:13,1367.0,update,post
109153,<python><dataset><pandas><correlation><wikipedia>,109155,133609.0,,110911,0,"yes, you can also use a scatter plot with the features ""links"" and ""wikipediaTranslatedPages""",2022-03-18 10:34:33,125527.0,no update,
109153,<python><dataset><pandas><correlation><wikipedia>,109155,133609.0,,110912,0,can you elaborate a bit more on that?,2022-03-18 10:35:45,133609.0,update,comment
72949,<python><pca><jupyter>,72957,95247.0,95247.0,77756,1,Is it possible for you to post the plot that was generated?,2020-04-25 6:16:00,61253.0,update,post
72949,<python><pca><jupyter>,72957,95247.0,95247.0,77771,1,"@PushkarajJoshi It was an elongated almost diagonal (but almost horizontal) conglomerate of points, covering both sides of the Y axis. There were 3 points significantly above the conglomerare (on positive X and positive Y).",2020-04-25 14:57:51,95247.0,no update,
35947,<python><r>,35987,6550.0,,42215,0,You should ask on StackOverflow since it is not related to Data Science !,2018-07-24 9:43:57,17208.0,no update,
32644,<python><scikit-learn>,32674,20903.0,,38333,6,"Depends on your dataset a lot, your processor, its cores, ssd/hdd, oob_score set to true or not, n_estimators etc.",2018-06-05 6:38:35,35644.0,no update,
32644,<python><scikit-learn>,32674,20903.0,,38337,6,"Why processor and hdd/ssd? I mean the RAM-size, sorry I expressed wrong.",2018-06-05 6:46:54,20903.0,update,no
32644,<python><scikit-learn>,32674,20903.0,,38338,6,"Computations are done by what(cpu only), you must be kidding me.. hdd/sdd because its faster to access data...",2018-06-05 6:49:18,35644.0,no update,
8084,<python><optimization>,8088,12777.0,-1.0,7608,2,"The param to be optimised, `x0` is a `ndarray` . . . so your ""one variable"" is an array - i.e. any number of scalar or concatenated array variables. In what way is this not working for multi-variable functions?",2015-09-14 6:24:16,836.0,update,both
8084,<python><optimization>,8088,12777.0,-1.0,7609,2,"I upvoted @NeilSlater's comment, but I shouldn't have. The question is about multivariate (rectangular) constraints, not about multivariate argument.",2015-09-14 6:55:56,6550.0,no update,
8084,<python><optimization>,8088,12777.0,-1.0,7610,2,"@NeilSlater when you say, ""in what way is this not working,"" what is the ""this"" you're referring to?  i.e., which scipy function are you referring to?  the different functions aren't suitable to my case for different reasons.",2015-09-14 7:24:50,12777.0,update,post
8084,<python><optimization>,8088,12777.0,-1.0,7611,2,"Sorry, I missed the constraints as being the main blocker to using `fsolve`, and thought you had in turn missed the first param being an array. Perhaps making the question more self-contained would have helped me spot the difference.",2015-09-14 7:55:27,836.0,no update,
39095,<python><neural-network><image-classification><convolution><pytorch>,53616,47730.0,26562.0,46647,6,IMHO depthwise/separable convolutions provide marginal improvement and only in specific areas. IMHO ResNet is a better start overall then google's neural networks. And in my head convolutions and cross correlation are completely different things with completely different math behind. You can calculate similarity score based on the output of the layer before the last one. And weights could be completely different for the same results. So IMHO weights are not a good approximation of similarity.,2018-10-13 22:27:54,57453.0,no update,
39095,<python><neural-network><image-classification><convolution><pytorch>,53616,47730.0,26562.0,50687,6,"Before we consider all of those details, how are you going to train your network? What's your target?",2019-01-07 1:30:57,41525.0,update,post
103440,<python><machine-learning><pytorch><statistics>,103450,126905.0,,106651,2,"in my opinion, what you ask can be answered by baseline statistics, particularly std deviation calculations. You calculate the std deviation of fuel consumption of flights and this gives you a baseline to rely upon. If training error is in that range, I dont think there is anything one can do",2021-10-24 8:55:39,100269.0,no update,
103440,<python><machine-learning><pytorch><statistics>,103450,126905.0,,106867,2,"What is your precise goal ?
Is your goal to predict normal/simulated consumption from QAR data? (so your do not need OFP data later)
If yes, you should train with variables from QAR and target from OFP.
Then you will be able to calculate the difference between predicted target and QAR 
data, and decide if this is a normal or non normal consumption.",2021-10-29 21:23:38,116702.0,update,no
21887,<python><neural-network><deep-learning><keras><q-learning>,21899,37300.0,836.0,25417,6,"When you do `model.predict(X)` the first axis in `X` is always an index in a batch. So if you want to predict on one sample, do something like `X = np.expand_dims(X, axis=0)`",2017-08-02 3:52:40,25504.0,no update,
21887,<python><neural-network><deep-learning><keras><q-learning>,21899,37300.0,836.0,25426,6,@MikhailYurasov thank you so much. My mistake was I thought keras would automatically add this extra dimension and then I got confused cause it was in the wrong order. Nevertheless I really appreciate your fast help. Since it is a comment I cant mark it as the right answers sorry for that.,2017-08-02 9:43:05,37300.0,no update,
21887,<python><neural-network><deep-learning><keras><q-learning>,21899,37300.0,836.0,25427,6,"It is generally a bad idea to put messages like ""moderators please don't close my question"" on a Stack Exchange site, so I have edited that out. If your question was bad in some way, this should make absolutely no difference to mod behaviour - no-one is making value judgements about your need to get an answer, stuff gets closed based on qualities of the question, not you personally. In your case there doesn't seem to be problem to me, the question is a practical concern with a library used for data science, and that topic is one of those central to this site.",2017-08-02 11:38:35,836.0,no update,
21887,<python><neural-network><deep-learning><keras><q-learning>,21899,37300.0,836.0,25471,6,"@NeilSlater thank you very much for your tip and help. In the future I wont write anything like ""mods please dont close this question"" in my question. Thinking about it you are totally right and it is stupid and doesnt add any value. I am sorry for my mistake, it wont happen again.",2017-08-03 11:28:03,37300.0,no update,
38547,<python><neural-network><classification><categorical-data>,40612,56985.0,8878.0,45204,1,"If we are going to down vote, can we at least explain why this is a bad question.  Examples:  This is common knowledge.  I was working on the down vote badge. This question is unclear.  I have not included the right tags.",2018-09-20 15:39:45,56985.0,no update,
38547,<python><neural-network><classification><categorical-data>,40612,56985.0,8878.0,45208,1,How can you one hot if the classes are different? Maybe they might balance off each other somehow?,2018-09-20 15:58:49,35644.0,update,post
38547,<python><neural-network><classification><categorical-data>,40612,56985.0,8878.0,47751,1,Downvoting this question is not the best but unfortunately we have a huge tendency to downvote in DSEC! I edited your question. Your main problem is about having an unseen value in categorical feature while testing on unseen data. A pretty good question actually!,2018-11-02 10:23:55,8878.0,no update,
67009,<python><jupyter>,67023,303.0,,72401,4,I hope this blog will guide/help you to get the answer. Link: https://blog.ouseful.info/2017/06/13/using-jupyter-notebooks-for-assessment-export-as-word-docx-extension/,2020-01-25 4:15:59,83275.0,no update,
67009,<python><jupyter>,67023,303.0,,109842,4,Got a medal for 10k views and feel like the currently accepted answer is not practical enough. So I've added a bounty. I don't know what is the etiquette about changing the accepted answer so I'll leave it like that untill it is clarified.,2022-02-03 9:20:00,303.0,no update,
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15439,21,"You can call the model.predict_generator(...) function with a generator that reads data from a directory containing the test set. It returns the predictions, which you can use to calculate a confusion matrix. Is that what you're looking for? See here for docs: https://keras.io/models/sequential/",2016-09-07 15:30:44,676.0,no update,
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15440,21,"Yes, I did see that. predict_generator returns a list of predictions which is a list of float values between 0 and 1. How do I interpret this? It cannot be directly used with the confusion matrix.",2016-09-07 15:34:32,14360.0,update,post
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15445,21,"I haven't tried predict_generator yet (it's rather new), but it seems to return class probabilities. Try to convert values <= 0.5 to 0 and > 0.5 to 1. Once you have a list consisting of 0s and 1s you can feed it to the function for calculating the confusion matrix.",2016-09-07 18:44:52,676.0,no update,
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15450,21,"As an aside, this will work fine for two class problems, but what if there are more than two classes?",2016-09-08 0:33:00,14360.0,update,no
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15451,21,"If there are more than two classes, your network needs more than one output. For n classes you have n outputs and you predict the class that has the highest output. Have a look at the softmax function (https://en.wikipedia.org/wiki/Softmax_function).",2016-09-08 7:46:31,676.0,no update,
13894,<machine-learning><python><deep-learning><keras><confusion-matrix>,22559,14360.0,20372.0,15467,21,Thanks for the clarification. I followed the suggested approach. The things is the accuracy obtained from predict_generator and that from the confusion matrix do not match.,2016-09-09 16:36:22,14360.0,no update,
76399,<machine-learning><python><scikit-learn><random-forest><accuracy>,76400,98535.0,98535.0,80873,2,"MAE/MSE/RMSE are metrics for regression, while Accuracy is for classification. What are you measuring with Accuracy?",2020-06-21 13:52:06,86339.0,update,comment
76399,<machine-learning><python><scikit-learn><random-forest><accuracy>,76400,98535.0,98535.0,80874,2,I would like to get some measurment of how good was the classification,2020-06-21 14:14:46,98535.0,no update,
76399,<machine-learning><python><scikit-learn><random-forest><accuracy>,76400,98535.0,98535.0,80880,2,"But you are running a regression model, the post is called random forest regressor. Accuracy is not a great metric to evaluate this. What is your train target like?",2020-06-21 15:52:55,86339.0,update,post
103428,<python><pandas>,103429,126875.0,126875.0,106640,0,"Your output image is not clear, could you update it, please?",2021-10-24 1:14:06,105181.0,update,comment
103428,<python><pandas>,103429,126875.0,126875.0,106644,0,"sorry, I have now updated the image.",2021-10-24 3:51:29,126875.0,no update,
30682,<machine-learning><python><predictive-modeling><regression><data-cleaning>,30687,50852.0,,35982,3,"Dataset looks interesting, is it open sourced? Can you add a correlation plot also? Also try adding $lmplots$, I had come across a similar kind of dataset, In that case I didn't focus much on boxplots as the dataset was related to the price of Houses(Singapore Houses was the dataset), so it makes sense that prices now are way more than before and the box plots did show that",2018-04-23 11:49:55,35644.0,update,no
30682,<machine-learning><python><predictive-modeling><regression><data-cleaning>,30687,50852.0,,35986,3,"People now don't care about outliers anymore , thanks to the Xgboost, Also your distribution are skewed, use some  log transformations",2018-04-23 12:00:47,35644.0,no update,
30682,<machine-learning><python><predictive-modeling><regression><data-cleaning>,30687,50852.0,,36005,3,"Thanks for all your inputs. Yes, XGBoost is pretty robust to outliers but I am using dnn regression as of now so was concerned as to how should I handle outliers. You have asked to add lmplots, may I know the significance? The data is not open-sourced, it's from a private project:)",2018-04-23 15:14:32,50852.0,update,both
30682,<machine-learning><python><predictive-modeling><regression><data-cleaning>,30687,50852.0,,36007,3,"to draw more conclusions, if you have a categorical variables.., then you should attempt..",2018-04-23 16:26:51,35644.0,no update,
74313,<python><clustering><pandas><k-means><matplotlib>,74320,96815.0,,79039,0,Do you only have 3 samples?,2020-05-17 1:02:14,41193.0,update,comment
74313,<python><clustering><pandas><k-means><matplotlib>,74320,96815.0,,79040,0,"I have around 2000 texts but most of them are similar to those I wrote above. Probably I should fix a threshold (or something else) for similarity to compare them to others. What I have been thinking is to groups these, than one by one checked the others and try to add them to the closest cluster. Does it make sense? If you like I could add more texts",2020-05-17 1:05:45,96815.0,no update,
74313,<python><clustering><pandas><k-means><matplotlib>,74320,96815.0,,79041,0,"If you are looking for a 2D visualization on this, you can apply tf-idf, then use a decomposition technique (e.g. PCA, NMF, or even LDA/topic modelling if you want to go further) for reducing the tf output dimensionality. Then you can scatterplot it. To group these points, you can use kmeans again, setting in the new plot the kmeans.label of each sample.",2020-05-17 1:10:42,41193.0,no update,
74313,<python><clustering><pandas><k-means><matplotlib>,74320,96815.0,,79042,0,Do you think it would be something doable to show as example the steps you mentioned using my three sentences above?,2020-05-17 1:16:18,96815.0,update,comment
35488,<python><feature-engineering><encoding>,35491,54395.0,,41649,0,"Does first_name, last_name, age come in order as you wrote or it can be in any order, like **""last_name doe age 50 first_name john""**?",2018-07-15 13:42:00,44139.0,update,post
35488,<python><feature-engineering><encoding>,35491,54395.0,,41650,0,@AnkitSeth the representation which I wrote is one of the possible views of the record since Python dictionary's key/value pairs are unordered.,2018-07-15 13:44:55,54395.0,no update,
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,70930,9,"For anybody interested, I've started working on a project to implement an algorhytm similar to the one described in @BenReiniger answer over at https://gitlab.com/nathan_vanthof/regression-tree-algorithm-with-linear-regression-models-in-each-leaf",2019-12-29 23:19:18,87545.0,no update,
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,70941,9,"Your updated question is more general: modeling a piecewise-linear function where the slopes can change, so they're really n different lines, not just the same line with discontinuities (as in the first example). Also, **please add a plot of the updated question**, currently the question is a mishmash of both askings, hence hard to understand. And presumably that should be 'x2' on the x-axis of the updated question, not 'x'.",2019-12-30 9:20:58,8501.0,no update,
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,70944,9,"@smci my question is: is there is an existing decision tree with the end leaf being a linear regression? The problem statement is just an example case (minimal replicable problem), changing the example case does not change the question.",2019-12-30 10:21:05,87545.0,update,post
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,70946,9,"@smci I've restructured my question, I hope it's clear the question did not change, only the example case I gave :)",2019-12-30 10:36:17,87545.0,no update,
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,70961,9,"I think you are looking for a **piecewise linear regression**. It does exactly what you need: break the space of *y* into segments of different lenght, and run a separate linear regression on each of those.",2019-12-30 16:57:45,65131.0,no update,
65585,<python><scikit-learn><decision-trees>,94656,87545.0,87545.0,98892,9,This method was researched in the 90s and I'm afraid that it wasn't very successful. Do a search on treed-regression.,2020-01-01 19:36:04,87663.0,no update,
25961,<python><neural-network><deep-learning><keras><jupyter>,26007,35386.0,,30342,0,does it stuck or exits the code and shows the message?,2017-12-24 13:37:46,28175.0,update,comment
25961,<python><neural-network><deep-learning><keras><jupyter>,26007,35386.0,,30344,0,"@Media It just gets stuck. The message is shown on terminal as soon as I execute `classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)`, then it will go up to around epoch 7 and get stuck and my computer fan starts whirring loudly",2017-12-24 14:21:59,35386.0,no update,
25961,<python><neural-network><deep-learning><keras><jupyter>,26007,35386.0,,30345,0,I suggest you update your anaconda. I had a similar problem on my windows version of anaconda jupyter notebook. But the keras worked fine in my ubuntu. I updated it on windows and it didn't get stuck.,2017-12-24 14:33:24,28175.0,no update,
25961,<python><neural-network><deep-learning><keras><jupyter>,26007,35386.0,,30374,0,"@Media Thanks very much, that fixed it! Merry Christmas",2017-12-26 1:21:51,35386.0,no update,
25961,<python><neural-network><deep-learning><keras><jupyter>,26007,35386.0,,30382,0,happy christmas,2017-12-26 5:17:20,28175.0,no update,
104947,<python><pandas><dataframe><python-3.x>,104955,128721.0,,108545,1,I’m voting to close this question because it does not belong to data science as it.,2021-12-26 9:54:53,26394.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16366,0,"Train 2 word2vec style language models -- one for each users.
And... that is it, more or less. You can ask the language models the probabilities of each saying the word/sequence of words. (The only trick is to keep the language models, rather than just the word embedding)",2016-10-24 13:10:43,2680.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16368,0,"@Oxinabox I don't fully understand what do you mean with ""keep the language models"", I'm kinda new to all this, could you explain?",2016-10-24 13:29:16,25526.0,update,comment
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16372,0,"Are you familiar with a language model? (Or with word2vec?) If not, perhaps go and do a bit more reading, or maybe ask another question ""What is a language model, and how does word2vec differ from a traditional language model"".",2016-10-24 14:23:45,2680.0,update,post
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16373,0,"@Oxinabox I'm not, will do my research on the term as you said. Thank you for your insights!",2016-10-24 14:29:50,25526.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16374,0,"Good luck, I think it is an interesting problem. How much data do you have BTW? (in terms of rough number of words). You won't get fair with any kinda language modelling (or I think even more general machine learning without at very least 10,000 words from each, I think. That gives you a chance to at least a solid subset of there day to day vocabulary. You can maybe do less if they are in a restricted vocab environment (Like they are playing a game and stay on topic). But still 10K words is pushing it to begin with.)",2016-10-24 14:32:28,2680.0,update,no
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16375,0,"@Oxinabox Actually I have a fair amount of it, 50k-60k words per user. I guess it should be enough.",2016-10-24 14:39:50,25526.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16405,0,"When you ask 'what structure should the data have?', you mean 'what representation' and presumably 'which features'?",2016-10-26 19:28:29,8501.0,update,post
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16406,0,"Also, as previously explained this is not straight ""text classification"", but rather ""user classification"", which is somewhat different. So it's not necessary to use e.g. word-vector or to semantically understand what the user is saying, only how they're saying it.",2016-10-26 19:32:01,8501.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16408,0,"@smci Edited the question, hopefully I made myself clearer this time.",2016-10-26 21:03:47,25526.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16411,0,"Are you asking the question in general sense or not? Because if you constrain us to handling dicts, that implies => bag-of-words and we drop counts, TFIDF, tree representations... Also why the constraint to use Naive-Bayes? It usually sucks.",2016-10-26 21:52:49,8501.0,no update,
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16412,0,"@smci well Im just using this to learn and understand how classifiers work, Naive Bayes seems the simplest, so I thought that It would be the best to start with, which one would you use for this matter?",2016-10-26 21:56:10,25526.0,update,no
14730,<machine-learning><python><classification><feature-engineering><text>,14768,25526.0,8501.0,16413,0,"@smci And it Dont have to be dicts, thats just t'he way I organized my data, and ofc can be changed, if i use some of the parameters of your answers I would be on a numeric array.",2016-10-26 21:59:26,25526.0,no update,
31134,<python><scikit-learn>,31311,49162.0,43000.0,53018,4,"I would have made a comment but I do not have enough reputation. If you set output_dict to True for classification_report() my function will take a list of dics and the label names and returns a pandas DataFrame with each cell containing a string ""mean +- std"" over all the dics you passed. def report_average(class_reports=None, label_names=None): sum_up = np.zeros((len(class_reports),len(label_names)+3,4)) for i, report in enumerate(class_reports): j = 0 for class_name, results in report.items(): h = 0 for metric, value in results.items(): sum_up[i,j,h] = value h += 1 j += 1 report_mean = np.m",2019-02-22 16:35:10,68240.0,no update,
78187,<python><r><regression><correlation>,78217,71246.0,71246.0,82859,2,"Can you give a better example of the Y variable? Is it nominal or ordinal which makes a huge difference. Also what is your end goal? Is it just identifying and explaining a dependency/relationship, do you want to predict or model something?",2020-07-23 14:20:47,79227.0,update,both
78187,<python><r><regression><correlation>,78217,71246.0,71246.0,82878,2,"You're right, that wasn't very precise and I'm not sure wether the term categorical is correct here. I added some more information to clearify.",2020-07-24 4:59:54,71246.0,no update,
78187,<python><r><regression><correlation>,78217,71246.0,71246.0,82880,2,"Ordinal means there is a clear order/hierarchy whereas nominal does not (e.g gender,etc) I assume cleanliness here is ordinal then because it is clear which value is better even if we cannot quantify better.",2020-07-24 5:37:24,79227.0,no update,
78187,<python><r><regression><correlation>,78217,71246.0,71246.0,82881,2,"Alright, thank you! Then it should be ordinal.",2020-07-24 5:42:06,71246.0,no update,
113971,<machine-learning><python><deep-learning><neural-network><keras>,113986,138833.0,138833.0,115191,0,"try and print ""self.model_.outputs"" and you will see the problem.",2022-08-31 13:38:42,139509.0,no update,
45663,<machine-learning><python><scikit-learn><pandas>,45664,66386.0,67483.0,52559,5,this question could easily be googled... one convenient way is `df2.sample(frac=1.0)`,2019-02-15 21:00:27,23305.0,no update,
45663,<machine-learning><python><scikit-learn><pandas>,45664,66386.0,67483.0,52560,5,"Thanks for the tips, I am running a ML regression experiment and shuffling the data cuts the rmse in half",2019-02-15 21:05:46,66386.0,no update,
75124,<python><text-mining><nltk><text-classification>,75130,96922.0,,79884,1,"This is extremely broad. The insights you can get depends entirely on what you are looking for in the first place. You don't ask ""what answer can I give?"" if you don't have a clear question. Why are you looking at POS in the first place? What are you trying to achieve?",2020-06-02 7:15:56,38887.0,update,post
21918,<python><markov>,21921,33171.0,,25474,7,Check http://pomegranate.readthedocs.io/en/latest/,2017-08-03 16:27:52,18465.0,no update,
67141,<machine-learning><python><dataframe><smote>,67164,89010.0,89010.0,72498,2,This is an SF topic where I found the suggested way to convert the 2nd argument of `fit_sample` to Series https://stackoverflow.com/questions/59453238/attributeerror-dataframe-object-has-no-attribute-name-when-using-smote,2020-01-28 5:41:58,89010.0,no update,
23211,<python>,23218,28175.0,28175.0,27035,0,"Assuming these are colour images, it seems likely that the 64x64 is row and column index, and the remaining three values are the RGB channels for the image.",2017-09-21 15:42:49,24895.0,no update,
23211,<python>,23218,28175.0,28175.0,27036,0,"@RHill This is exactly my point. Shouldn't it be (3, 64, 64)?",2017-09-21 15:49:21,28175.0,update,no
23211,<python>,23218,28175.0,28175.0,27043,0,@NeilSlater you are telling me this ordering is different from what we have in other containers such as list or tuples?,2017-09-21 16:24:40,28175.0,update,no
23211,<python>,23218,28175.0,28175.0,27047,0,"@NeilSlater actually I want to change my example, instead of tuples or list think about arrays in numpy or matrices in numpy. whenever we want to access the rows and columns of matrices (actually tensors) with 3-d shape we use second and third entries to access the corresponding elements.",2017-09-21 16:28:10,28175.0,no update,
18048,<python><classification><keras><convolutional-neural-network>,18146,25429.0,29169.0,20438,0,"Standard backpropagation techniques to update the weights should work, since all layers of your neural net are differentiable. What exactly are you confused about?",2017-04-02 22:03:56,30408.0,update,post
18048,<python><classification><keras><convolutional-neural-network>,18146,25429.0,29169.0,20440,0,how do i parse my input/output data to keras model.fit ?,2017-04-03 2:51:07,25429.0,update,comment
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39052,3,What about word embeddings? Is the dataset open source?,2018-06-15 17:00:14,35644.0,update,both
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39056,3,"@Aditya I have prepared the data set from twitter, blogs, forums etc. Could you give an example of how to use word-embeddings?",2018-06-15 17:54:54,47403.0,update,no
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39058,3,"Checkout fast.ai , Racheal Thomas workshop on this, Deep Learning.ai Andrew NG on YouTube there's a course on NLP Sequence Modelling",2018-06-15 18:02:03,35644.0,no update,
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39059,3,How did you scrapped so many tweets? Can the dataset be shared? It will help me in my ongoing internship,2018-06-15 18:03:59,35644.0,update,post
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39104,3,"**Text clustering is hard.** Do not expect it to ""just"" work. In particular with algorithms such as k-means that make very different assumptions on your data... Word embeddings are all the rage, but I doubt they work actually much better. It's just that people *want* the results to be better. In the end, you still have 300 dimensional vectors, with plenty of anomalous documents, and k-means neither is good for high dimensions, nor for such noisy data.",2018-06-16 18:16:02,924.0,no update,
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39105,3,@Anony-Mousse I have now considered `HDBSCAN` algorithm for clustering but am stuck at how to create vectors from my the list of posts I have.,2018-06-16 18:20:23,47403.0,no update,
33227,<python><clustering><k-means><unsupervised-learning>,33233,47403.0,47403.0,39106,3,"I don't think it will work any better, because of the distance functions not working well enough.",2018-06-16 18:22:21,924.0,no update,
99998,<machine-learning><python><clustering><feature-selection>,100068,118696.0,118696.0,104418,0,Provide an example …,2021-08-12 19:20:54,8752.0,update,no
99998,<machine-learning><python><clustering><feature-selection>,100068,118696.0,118696.0,104509,0,"I think I literally just answered this question in a different topic https://datascience.stackexchange.com/a/100079/74387. Does it help? The point is that some features could be correlated with several others, and then pairwise correlation could be low despite all the features being strongly correlated.",2021-08-14 23:03:57,74387.0,update,no
92968,<python><scikit-learn>,92972,115840.0,43000.0,97198,4,"I don't think this is a duplicate; it's a follow-up.  _Given_ that `fit_transform`=`fit`-then-`transform`, and you usually `fit_transform` training data then `transform` test data, why does `fit` exist as a separate method (or when/why should you use it)?",2021-04-12 20:13:49,55122.0,update,post
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,33314,12,Fine tune or use pre-trained weights,2018-03-02 10:18:12,35644.0,no update,
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,33315,12,Thanks @Aditya but i don't want to predict on different dataset .... i want  to enrich my train dataset and then predict my results without retrain my train dataset from the beginning,2018-03-02 10:29:29,44044.0,no update,
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,33332,12,Which algorithm? Are you using Scikit learn?,2018-03-02 13:57:55,45507.0,update,both
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,33334,12,My question was not for a specific algorithm ......it was a general question.....why?,2018-03-02 14:04:43,44044.0,update,post
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,33883,12,Possible duplicate of [Using a pre trained CNN classifier and apply it on a different image dataset](https://datascience.stackexchange.com/questions/28383/using-a-pre-trained-cnn-classifier-and-apply-it-on-a-different-image-dataset),2018-03-10 6:54:35,44456.0,no update,
28512,<machine-learning><python><deep-learning>,28517,44044.0,40853.0,107214,12,"I think it matters which algorithm from which package. For instance, it seems to me that calling `fit` on a trained model in tf.keras updates the weights. This can be seen by a very low loss in the first epoch of the second call to fit.",2021-11-10 16:36:20,103314.0,no update,
104428,<python><classification><class-imbalance><overfitting><smote>,104433,127992.0,,107658,4,"""Unbalanced"" data are not a problem unless you use inappropriate evaluation measures, and oversampling will not solve this non-problem: [Are unbalanced datasets problematic, and (how) does oversampling (purport to) help?](https://stats.stackexchange.com/q/357466/1352)",2021-11-25 14:54:10,2853.0,update,no
104428,<python><classification><class-imbalance><overfitting><smote>,104433,127992.0,,110997,4,See https://datascience.stackexchange.com/q/15630/55122,2022-03-22 20:14:04,55122.0,no update,
57371,<machine-learning><python><feature-selection><random-forest><dimensionality-reduction>,57416,59028.0,59028.0,65584,2,"Hi, I see you are working on the Kaggle House Prices. Just one question, did I get it right, that you want to get the correlation between the SalePrice and all columns? Wouldn't you just have to use `df.drop(""SalePrice"", axis=1).apply(lambda x: x.corr(df.SalePrice))` to get it (I mean after one-hot-encoding)?",2019-09-13 8:54:37,75897.0,update,no
77429,<python><confusion-matrix>,77493,98535.0,98535.0,81892,1,Please print a few rows of - labels and prediction.,2020-07-09 13:45:54,58341.0,update,post
77429,<python><confusion-matrix>,77493,98535.0,98535.0,81928,1,@10xAI I have jsut added,2020-07-09 18:33:30,98535.0,no update,
20113,<python><keras><reference-request><books>,20124,27526.0,31513.0,23414,4,"Keras encapsulates details in a way that makes MOOCs looking at the underlying theory less likely to use it (e.g. you don't implement dropout, just declare it). As a result, you can learn to use it with only a very lightweight guide to the underlying theory. If you have the endurance for a bit more maths than Andrew Ng's course, then https://www.coursera.org/learn/neural-networks might be good addition, although it covers some things that are not used in practice in modern NNs, whilst it is 5 years old so missing some recent things like VAEs, GANs, LSTM etc.",2017-07-02 12:41:05,836.0,update,post
20113,<python><keras><reference-request><books>,20124,27526.0,31513.0,23415,4,"@NeilSlater ouch! I can live without GANs, but I would need LSTMs...my applications are mainly time series. I'll add that detail in the question. Isn't there anything more recent? If no MOOC, I could accept a book/lecture slides as an answer, but nothing huge such as Murphy's book.",2017-07-02 12:46:49,27526.0,update,post
43937,<python><numpy>,43939,65055.0,,51011,2,"This question is not about data-science, it is purely about code and belongs in stackoverflow.",2019-01-14 12:06:37,50833.0,no update,
32639,<python><pandas><stata>,32648,53163.0,,38335,0,"search for masking using np.where or loc,iloc,idx...",2018-06-05 6:43:40,35644.0,no update,
68194,<python><python-3.x>,68212,90151.0,89927.0,73426,1,You should search for NaNs inyour data,2020-02-17 0:07:05,35644.0,no update,
11214,<python><scikit-learn><pandas><pca>,11224,17886.0,,11292,1,http://stats.stackexchange.com/questions/123318/why-are-there-only-n-1-principal-components-for-n-data-points-if-the-number,2016-04-14 15:39:10,676.0,no update,
104597,<python><scikit-learn>,104618,128293.0,92050.0,107752,0,What is `cat_vars`?,2021-11-28 22:23:47,42378.0,update,comment
104597,<python><scikit-learn>,104618,128293.0,92050.0,107759,0,'cat_vars'' is the name of the data set,2021-11-29 6:39:37,128293.0,no update,
104597,<python><scikit-learn>,104618,128293.0,92050.0,107763,0,"I mean what is the data type? What are the dimensions? If you don't show its assignment, how could anyone know for sure? We are left to guess.",2021-11-29 12:12:21,42378.0,update,both
38540,<python><nlp><language-model>,52901,59385.0,,45206,16,[Tensorflow 1b words LM](https://github.com/tensorflow/models/tree/master/research/lm_1b),2018-09-20 15:46:23,14006.0,no update,
38540,<python><nlp><language-model>,52901,59385.0,,45219,16,Well this is not at all readily usable but it's something. Thanks :),2018-09-20 17:57:51,59385.0,no update,
38540,<python><nlp><language-model>,52901,59385.0,,45220,16,"That's a pre-trained model that you can simply download and run, and you think that is ""not at all readily usable"" ...",2018-09-20 18:06:20,14006.0,no update,
38540,<python><nlp><language-model>,52901,59385.0,,45221,16,"I think you and me have very different definitions of what ""readily usable"" means... I would need to figure out how to get the tensorflow ops I want (input and output) and how they behave, figure out if there's any preprocessing to this and then wrap everything in some perplexity function. I'm not saying I can't do it, I'm just saying it is not at all the ""readily usable"" function I showed. But again, thanks for the pointer",2018-09-20 18:17:36,59385.0,no update,
38540,<python><nlp><language-model>,52901,59385.0,,45460,16,Have you tried google? I hear they get a fair amount of data :) Not sure if they have the exact metrics you're after. https://cloud.google.com/natural-language/docs/,2018-09-24 19:04:07,20971.0,update,no
38540,<python><nlp><language-model>,52901,59385.0,,45472,16,Yeah I actually use google nlp quite extensively... But no they don't have language modeling,2018-09-24 23:56:25,59385.0,no update,
25073,<python><scikit-learn><dataset>,25077,42275.0,,49783,5,how you create data distrbution using hostogram? could you share the code here?,2018-12-17 10:19:38,64444.0,update,post
10809,<python><classification><neural-network><scikit-learn><image-classification>,10819,1426.0,,17985,2,http://stats.stackexchange.com/q/202901/2921,2017-01-04 22:39:54,8560.0,no update,
67566,<machine-learning><python><scikit-learn><pandas>,67628,89458.0,,72839,0,"There is no way of knowing what the ""best classification algorithm"" will be, you need to try multiple algorithms out and see which one performs the best.",2020-02-05 10:01:24,34269.0,no update,
94594,<python><neural-network><keras><tensorflow><reinforcement-learning>,107813,117892.0,29575.0,98823,2,"Hello, your `build` function seems to have an error. You should use `n` instead of `N` inside the function. Besides, I think you can directly `return np.ones(N)` to have a correct shape.",2021-05-18 17:38:09,45726.0,no update,
94594,<python><neural-network><keras><tensorflow><reinforcement-learning>,107813,117892.0,29575.0,98828,2,"thank you very much for your answer,  but I think I have identified the problem although I do not know how to improve.",2021-05-18 20:18:47,117892.0,no update,
94594,<python><neural-network><keras><tensorflow><reinforcement-learning>,107813,117892.0,29575.0,98829,2,"I write a ""problem"" as an answer to my question",2021-05-18 20:19:16,117892.0,no update,
100539,<python><text-mining>,100548,123533.0,75157.0,104897,0,"Please clarify your specific problem or provide additional details to highlight exactly what you need. As it's currently written, it's hard to tell exactly what you're asking.",2021-08-26 15:04:35,-1.0,update,post
100539,<python><text-mining>,100548,123533.0,75157.0,104909,0,a method like word2vec or bert may help from the area of NLP,2021-08-26 18:23:08,100269.0,update,post
61078,<python><pandas><multiclass-classification><csv><data-table>,61129,58433.0,29169.0,66489,0,So you have dataframe with 32 columns and 5000 rows. And some of column names have more than 10 characters. You want to display them in your notebook right? Or some of column values (in rows) string objects have more than 10 characters?,2019-10-01 8:07:34,77900.0,update,no
61078,<python><pandas><multiclass-classification><csv><data-table>,61129,58433.0,29169.0,66517,0,"some of labels are more than 10 characters! I need to see them and use their label to apply some ML algorithm based on their labels! No,values are real numbers but I have problem in observation of labels!",2019-10-01 20:29:00,58433.0,update,no
61078,<python><pandas><multiclass-classification><csv><data-table>,61129,58433.0,29169.0,66519,0,Can you display some of rows as an example?,2019-10-01 20:52:55,77900.0,update,no
61078,<python><pandas><multiclass-classification><csv><data-table>,61129,58433.0,29169.0,66542,0,"ok i got the data, one last thing: label = column name right? like fastest2minwindspeed",2019-10-02 6:35:50,77900.0,update,no
51429,<python><deep-learning><keras><cnn><computer-vision>,51433,73678.0,29169.0,57551,2,"What exactly is the memory error message? If you are simply running out of RAM memory, you will need to break the list down into smaller chunks and process them individually.",2019-05-05 12:31:29,45264.0,update,both
51429,<python><deep-learning><keras><cnn><computer-vision>,51433,73678.0,29169.0,57552,2,updated the error..please check.. what do you mean by process individually ?,2019-05-05 12:39:35,73678.0,update,comment
23586,<python><classification><scikit-learn><ensemble-modeling><binary>,23622,39547.0,-1.0,27469,4,"I am using `XGBClassifier()` for xgboost. It outperforms SGD by a significant margin and unfortunately doesn't have a `warm_start`-like feature because of the nature of decision trees. I was able to use xgboost `xgb_model` parameter as in `xgboost.train(parameters, dtrain, num_boost_round=250,  xgb_model='model.pkl')` but its performance differs from the sklearn wrapper despite me setting the equivalent hyperparameters.",2017-10-06 17:37:56,39547.0,update,both
23586,<python><classification><scikit-learn><ensemble-modeling><binary>,23622,39547.0,-1.0,27486,4,"I think we need more context. Is this the same problem, split into 30 pieces, or 30 separate problems?",2017-10-07 13:37:06,30212.0,update,post
23586,<python><classification><scikit-learn><ensemble-modeling><binary>,23622,39547.0,-1.0,75279,4,Some sklearn models allow a `prefit` option; there's been some chat about adding that to ensembles like VotingClassifier (https://github.com/scikit-learn/scikit-learn/issues/12297) and StackingClassifier (https://github.com/scikit-learn/scikit-learn/pull/16748).,2020-03-26 12:55:15,55122.0,no update,
46077,<python><pandas>,46095,,,53032,2,What should the graph look like? Do you just want a scatter plot with the values on y-axis and datetime on x-axis? Or a histogram binned by 360 minute periods?,2019-02-23 7:04:30,29587.0,update,post
46077,<python><pandas>,46095,,,53036,2,@JahKnows I thought first I will try with scatter plot. Actually I want a line graph x1 with time. I upload the pic (graph) that I want as a example(This is just for understand which shape of graph line I need),2019-02-23 7:55:45,,no update,
13377,<python><visualization>,14827,23330.0,-1.0,14748,4,Maybe `plt.tight_layout()` can help before `plt.savefig()`,2016-08-12 8:54:55,23345.0,no update,
13377,<python><visualization>,14827,23330.0,-1.0,14752,4,I'm getting max() arg is an empty sequence. I think that plt.tight_layout() only works when you have subplots.,2016-08-12 12:50:06,23330.0,no update,
11347,<python><statistics><pandas><ipython>,11373,18058.0,381.0,11451,11,"Done, added some sample data.",2016-04-22 4:05:53,18058.0,no update,
26380,<python><scikit-learn><categorical-data>,26389,43870.0,,30770,1,Why not processing / encoding your categorical features ?,2018-01-07 15:25:23,18434.0,update,post
26380,<python><scikit-learn><categorical-data>,26389,43870.0,,30772,1,Are you suggesting one-hot? It seems like a too much of a workaround just because the library doesn't support it,2018-01-07 15:53:55,43870.0,update,no
26380,<python><scikit-learn><categorical-data>,26389,43870.0,,30774,1,Sklearn provides features processing tools including one Hot encoding. See http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing,2018-01-07 16:03:46,18434.0,no update,
26380,<python><scikit-learn><categorical-data>,26389,43870.0,,30776,1,"Still, seems like a workaround simply because this specific library doesn't support it. Is there nothing else?",2018-01-07 17:19:57,43870.0,update,no
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38011,4,"To understand an algorithm it's always best to choose the simplest case to remove all the nitty gritty stuff. Naive Bayes is the same regardless of what data you use. However, the majority of your description focuses on a bag-of-words approach and obfuscates the Naive Bayes question.",2018-05-30 10:03:16,29587.0,no update,
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38013,4,"Okay, i wanted to know in context of Scikit Learn in Python .",2018-05-30 10:05:50,31689.0,no update,
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38014,4,Can you give us a sample of the data and I'll write a small how-to?,2018-05-30 10:09:17,29587.0,update,post
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38016,4,Okay our team is already using NaiveBayes from Scikit Learn to do SA.  So I wanted to know how it works under the hood.,2018-05-30 10:11:16,31689.0,no update,
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38017,4,I can't really understand your description unfortunately. How many features do you have? What do they represent? Is it a bag-of-words approach? How many unique labels do you have?,2018-05-30 10:12:58,29587.0,update,comment
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38018,4,"I am scraping comments from the web. Let us say i get comments about a movie 'Star Wars'. I have 5 labels (1-poor, 2-fair, 3-ok, 4-good, 5-very good). We train it using labelled data, using Scikit Learn, Naive Bayes algorithm. Then we run it on real data.",2018-05-30 10:14:44,31689.0,no update,
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38019,4,"Ok that answers the questions about the labels. What about the features. What do you extract from the Star Wars review? How many words in your ""bag"" do you keep if you are using bag-of-words?",2018-05-30 10:16:48,29587.0,update,post
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38020,4,"I have expanded my question. I am ASSUMING Scikit Learn Naive Bayes, uses a bag of words. I have put down my imagined algorithm. I wanted to know if that is how the real algorithm works.",2018-05-30 10:22:19,31689.0,no update,
32379,<python><scikit-learn><naive-bayes-classifier><sentiment-analysis>,32384,31689.0,31689.0,38022,4,"Naive Bayes does not do any bag-of-words or any pre-processing, you need to do all these steps separately. Naive Bayes is simply a statistical model which can be used to classify data from a novel set based on the parameters it has learned from the training set.",2018-05-30 10:25:03,29587.0,no update,
33780,<python><pandas><plotting><numpy><matplotlib>,33783,50896.0,,39710,5,pass the c parameter..,2018-06-28 19:42:32,35644.0,no update,
33780,<python><pandas><plotting><numpy><matplotlib>,33783,50896.0,,39717,5,Can you please elaborate more? @Aditya,2018-06-28 20:01:04,50896.0,update,comment
33780,<python><pandas><plotting><numpy><matplotlib>,33783,50896.0,,39718,5,"Added an answer below, tweak it to suit your target variable, you will find it in the docs",2018-06-28 20:02:52,35644.0,no update,
33780,<python><pandas><plotting><numpy><matplotlib>,33783,50896.0,,49786,5,May I know what data of CSV file,2018-12-17 16:04:09,64462.0,no update,
9262,<python><clustering><scikit-learn>,9271,8206.0,98307.0,9013,42,"By KL, do you mean Kullback-Leibler divergence?",2015-12-08 10:55:07,11097.0,update,comment
9262,<python><clustering><scikit-learn>,9271,8206.0,98307.0,9014,42,"Yes, exactly that!",2015-12-08 10:57:05,8206.0,update,no
9262,<python><clustering><scikit-learn>,9271,8206.0,98307.0,9015,42,"By running `sklearn.metrics.mutual_info_score([1.346112,1.337432,1.246655], [1.033836,1.082015,1.117323])`, I get the value `1.0986122886681096`.",2015-12-08 11:05:12,11097.0,no update,
9262,<python><clustering><scikit-learn>,9271,8206.0,98307.0,9016,42,"Sorry, i was using values1 as [1, 1.346112,1.337432,1.246655] and values2 as values2 as [1,1.033836,1.082015,1.117323] and hence the difference value.",2015-12-08 11:07:36,8206.0,no update,
61391,<python><neural-network><prediction><ensemble-learning>,61393,54115.0,,66810,2,"Can you specify the overview of the problem you are trying to solve...
Ie. Binary classification, multi class classification, multi label classification Looks you are trying for Ensembling of various networks.",2019-10-07 17:52:30,65189.0,update,comment
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,61650,0,"So do you want your columns to be something like after applying oneHotEncoder? meaning: $SITE_NAME, ..., Aluminum, Beryllium, Value$ and then fill these columns for each row with 0s and 1?",2019-07-18 22:50:08,74421.0,update,post
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,61651,0,I need the 'Value' and 'Units' for each sample.,2019-07-18 22:56:52,78015.0,no update,
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,61656,0,could you provide the columns and rows names of the table you are aiming to get?,2019-07-19 0:58:47,74421.0,update,comment
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,62004,0,"Hi , I am trying to get it in the form   as below:                                                     
  SITE_NAME SAMP_DATE_TIME ALUMINIUM(ug/L) Berilium(ug/L) Chloride (ug/l) and so on …",2019-07-24 21:58:58,78015.0,update,no
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,62005,0,But there are duplicates of the same samples in your data. For instance the first and second rows are totally the same.,2019-07-24 22:21:01,74421.0,no update,
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,62006,0,"Yes, I want those values listed in different rows for different timestamp and drifferent columns for different parameter",2019-07-24 22:22:47,78015.0,no update,
55961,<python><dataset><pandas><dataframe>,56347,78015.0,74421.0,62007,0,"I mean, even the timestaps are the same for some rows. 
Could you depict the actual output of the tables in the question? just like your current table.",2019-07-24 22:25:14,74421.0,update,comment
27386,<python><clustering><visualization><recommender-system>,27387,44190.0,44190.0,31869,0,https://en.wikipedia.org/wiki/Dendrogram,2018-02-02 20:24:29,23305.0,no update,
22233,<python>,22235,37059.0,8432.0,25822,3,"I highly recommend you to use pandas in these types of work, as the answer suggested.",2017-08-14 11:23:16,8432.0,no update,
22233,<python>,22235,37059.0,8432.0,68918,3,This one should be moved to stack-overflow. There's no `science` stuff here.,2019-11-25 14:38:37,85398.0,no update,
26958,<python><scikit-learn><svm>,26987,42781.0,29169.0,83370,1,"If I got the introduction of this book http://www.charuaggarwal.net/outlierbook.pdf correctly, the hyperparameter tuning in outlier detection is not recommended.",2018-01-24 9:58:14,43469.0,no update,
32627,<machine-learning><python><time-series><anomaly-detection><ipython>,32635,53148.0,53148.0,38323,0,"You want to conduct a _hypothesis test_. The _null hypothesis_ is that it stays constant, and the _alternate hypotheses_ are for increasing and decreasing. The parameter of the test is the slope of linear regression model, unless there is seasonality, in which case you will need to estimate the _trend_ by _time series decomposition_. You can do the test in batch or sequentially (cf. _sequential hypothesis test_). Welcome to the site and good luck!",2018-06-04 20:43:35,381.0,no update,
66201,<python><nlp><text-filter>,66408,88079.0,,71585,3,"I think you'll need to find a definition of ""credibility"" that can be expressed mathematically before anyone can really help you. I am not sure if a recommendation for a package is considered ""on topic"" in this forum either.",2020-01-09 16:15:56,54953.0,no update,
77215,<python><dataset><data-science-model><dataframe>,77219,100295.0,,81726,0,I’m voting to close this question because there is no explanation on what the user already tried,2020-07-07 6:12:59,91826.0,no update,
26283,<python><scikit-learn><data-mining><random-forest>,26284,43455.0,55122.0,51125,14,"You don't need to conduct one_hot if you are using a tree model,cause it is not measuring distance like other method.",2019-01-15 7:53:18,65883.0,no update,
26283,<python><scikit-learn><data-mining><random-forest>,26284,43455.0,55122.0,77887,14,"@JunYang, scikit-learn does currently require encoding categoricals.",2020-04-27 21:29:22,55122.0,no update,
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86085,8,"My guess is that in the first case, your training dataset has been artificially balanced, so you're estimation of performance of your model is on a dataset that does not follow the original distribution of your data - you also estimate your score on data that is synthetic which seems improper - the grid search val split doesn't know that some data is sort of duplicated by SMOTE therefore the original sample might be in train and the oversampled version might get into the val set. In the second case though, I imagine that the pipeline takes care for you of only oversampling the training set.",2020-09-22 15:09:16,104626.0,no update,
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86086,8,or at least oversampling each set only after the train/val split has been done,2020-09-22 15:10:17,104626.0,no update,
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86088,8,"Does this answer your question? [Why did sampling boost the performance of my model?](https://datascience.stackexchange.com/questions/60764/why-did-sampling-boost-the-performance-of-my-model), also [Oversampling before Cross-Validation, is it a problem?](https://datascience.stackexchange.com/q/44327/55122)",2020-09-22 16:04:05,55122.0,update,no
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86154,8,"@BenReiniger to be *very* precise, and despite the similarity in the titles, none of the linked questions is about SMOTE, which is actually the case here",2020-09-23 12:08:15,43000.0,no update,
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86167,8,"The voted duplicate is about SMOTE, albeit only after a few comments.",2020-09-23 13:54:32,55122.0,no update,
82073,<python><scikit-learn><cross-validation><class-imbalance><smote>,82093,99648.0,85045.0,86173,8,"@BenReiniger Comments are not part of the question unless explicitly edited so by the OP. The voted duplicate is clearly not about SMOTE, and there is no mention of SMOTE at all in the second alleged duplicate, but suit yourself.",2020-09-23 14:37:32,43000.0,no update,
45163,<python><nlp><class-imbalance><imbalanced-learn>,45187,62616.0,62616.0,52115,3,Could you please post your approach/code here?,2019-02-06 14:37:55,62097.0,update,both
45163,<python><nlp><class-imbalance><imbalanced-learn>,45187,62616.0,62616.0,52117,3,which code??I ask a general question based on number of samples??,2019-02-06 15:18:42,62616.0,update,post
14355,<python><scikit-learn><k-means>,14366,24930.0,24930.0,15964,0,"Look at this, In[35], at the end - http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb. It might help.",2016-10-04 17:47:28,17290.0,no update,
14355,<python><scikit-learn><k-means>,14366,24930.0,24930.0,15965,0,"@HonzaB, thanks!  are you telling me to try using knearest neighbors?  :)

I added a chart for what my ""Clusters"" currently look like",2016-10-04 18:05:08,24930.0,update,comment
14355,<python><scikit-learn><k-means>,14366,24930.0,24930.0,15966,0,"Not necessarily. Both algorithms are quite similar. The thing is that iris dataset contains 4 variables in fact. So as you can see, usually authors choose two attributes (pedal width and length) which allows them to plot it on 2D scatter plot. So i'd suggest to do the same. Or you can try silhouette analysis.",2016-10-04 18:20:12,17290.0,no update,
15812,<python><classification><data-mining><time-series><pandas>,15814,27194.0,,17696,9,"What do you mean by ""similarity"" here? Can you define it mathematically?",2016-12-20 8:18:17,471.0,update,no
15812,<python><classification><data-mining><time-series><pandas>,15814,27194.0,,17937,9,"Well, I don't have a math definition in hand right now. May I ask I guidance on this?",2017-01-02 19:37:00,27194.0,update,no
30181,<machine-learning><python><nlp><cnn>,77741,49268.0,49268.0,35428,4,Are you familiar with Conv1D?,2018-04-12 14:28:12,28175.0,update,no
30181,<machine-learning><python><nlp><cnn>,77741,49268.0,49268.0,35454,4,"I have applied 1D CNN to signals, I am stuck on how to apply text to CNN.",2018-04-13 10:56:38,49268.0,no update,
2355,<python><graphs><ipython>,2371,3281.0,26596.0,2548,3,"It is a known issue in GraphLab Create 1.0.1 and earlier that the visualization output will not show in an IPython Notebook running over HTTPS. If you are using HTTPS, try over HTTP instead. This will be fixed in a future release of GraphLab Create.",2014-10-28 17:56:45,4834.0,no update,
2355,<python><graphs><ipython>,2371,3281.0,26596.0,17175,3,"I am also facing the same issue, When I have set the target as browser, it opened a new tab and display the charts and data but when using graphlab.canvas.set_target('ipynb'), nothing happened. I am using - Python 2.7 Jupyter Notebook Graphlab create 1.21",2016-11-28 2:06:03,26561.0,no update,
80207,<machine-learning><python><r><ai>,80208,,85045.0,84075,2,"Welcome! R and Python are both object orientated https://en.wikipedia.org/wiki/List_of_object-oriented_programming_languages. For a discussion on R vs Python, see this post: https://datascience.stackexchange.com/questions/76824/is-python-a-viable-language-to-do-statistical-analysis-in/76841#76841. Why don‘t you just start with looking at one language, e.g. R. There are great online sources: http://faculty.marshall.usc.edu/gareth-james/ISL/ or https://r4ds.had.co.nz/. I vote close because the question is opinion based.",2020-08-13 11:41:02,71442.0,no update,
31330,<python><jupyter><numpy>,31331,51724.0,,36722,3,See if [this SO answer](https://stackoverflow.com/questions/4318615/python-numpy-memoryerror/4319312) helps.,2018-05-07 13:22:42,18323.0,no update,
31330,<python><jupyter><numpy>,31331,51724.0,,36748,3,"That link requires registration so I'd suggest you describe the best you can the problem with code snippets included. In addition you should post the specs of your computer in case it's just a simple ""not enough RAM error"" which appears to be the case.",2018-05-07 22:39:29,13023.0,no update,
31330,<python><jupyter><numpy>,31331,51724.0,,36816,3,"@wacax Below are the code lines just below the above error giving line:
`from tqdm import tqdm
    def read_img(img_path):
        img = cv2.imread(img_path)
        img = cv2.resize(img, (128, 128))
        return img

    train_img = []
    for img_path in tqdm(train.Image_name.values):
        train_img.append(read_img(TRAIN_PATH + img_path))`

Any idea how to know how much space that variable is taking ?
And how to decide min Gb of RAM required to solve such problems?",2018-05-08 15:07:48,51724.0,update,comment
72503,<python><matplotlib><plotting>,72512,95092.0,,77304,0,"Hello @Omerlewitz, welcome to the site. Your answer seems quite vague, have a look [here](https://datascience.stackexchange.com/help/how-to-ask) to see how to ask better questions.
Nevertheless, could you describe better what your *desired outcome* would be?",2020-04-17 18:14:33,54395.0,update,post
72503,<python><matplotlib><plotting>,72512,95092.0,,77307,0,each algorithm will be filled to the percentage it should be and the bar will be blue Like the bottom ones,2020-04-17 18:23:39,95092.0,no update,
16122,<machine-learning><python><neural-network><nlp><deep-learning>,16125,27668.0,27668.0,18063,0,"Look up ""sentence embedding"", word2vec, and paragraph2vec.",2017-01-06 3:45:54,381.0,no update,
90854,<python><time-series><matplotlib><plotting>,90884,110503.0,,95051,0,Could you give us the shapes of X_test so that we can maybe synthetically reproduce the plot?,2021-03-19 3:16:24,27114.0,update,both
90854,<python><time-series><matplotlib><plotting>,90884,110503.0,,95053,0,What about plotting the differences - i.e. y_true - yhat_conv (or the other way round) ... basically they look like they are going to have a small'ish variance ..,2021-03-19 6:37:57,113414.0,update,comment
90854,<python><time-series><matplotlib><plotting>,90884,110503.0,,95054,0,And offset a few of them (i.e. add 3 to some of the differences and 6 to some more - if you can control the colours pick them too to make pairs line up easily but be visually different [so you only end up with 2/3 per line row of differences instead of 10]...,2021-03-19 6:40:04,113414.0,no update,
90854,<python><time-series><matplotlib><plotting>,90884,110503.0,,95055,0,"Or eliminate all the ones where the difference is really low and only look at the ""worst"" ??",2021-03-19 6:40:58,113414.0,update,no
90854,<python><time-series><matplotlib><plotting>,90884,110503.0,,95056,0,"@PSub The shape of X_test is (3000, 10, 30)",2021-03-19 6:47:19,110503.0,no update,
9208,<machine-learning><python><neural-network>,9239,14535.0,11097.0,10211,8,The comparisons in that link you provided were very informative,2016-02-18 16:58:31,12515.0,no update,
22062,<python><visualization>,33131,37059.0,37059.0,25623,6,Can you provide the dataset for the visualization?,2017-08-08 19:45:14,24000.0,update,no
22062,<python><visualization>,33131,37059.0,37059.0,25775,6,You should add your dataset as @tuomastik mentioned. Otherwise people can't answer...,2017-08-12 2:19:41,8432.0,no update,
22062,<python><visualization>,33131,37059.0,37059.0,25879,6,"The dataset is a bit big to add on here. I could do some aggregation, however I am now seeing a similar problem where the violin for a category  has a totally different range when created from different DataFrames, despite the data being exactly the same.",2017-08-16 10:36:40,37059.0,no update,
22062,<python><visualization>,33131,37059.0,37059.0,25883,6,"It seems that the `scale = ""count""` option doesn't actually give a representation based on the overall ""count"" and whilst the data is properly represented, the labels don't match the data.",2017-08-16 12:41:53,37059.0,no update,
22062,<python><visualization>,33131,37059.0,37059.0,76585,6,"I am getting a similar problem with the R version. I split the violin plot in positive and negative side to describe a binary with a ratio of 2:1 but the area of each side does not follow this proportion. And, as it is in your case, the count modality does not work properly only for one of the categories I have.",2020-04-02 18:16:56,93900.0,no update,
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84920,1,What do you want to compare?,2020-09-01 14:35:00,73930.0,update,comment
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84925,1,"the two results given by the matrix, that is to say, two versions of the vader.py algorithm for sentiment classification (1 being very negative and 5 very positive).",2020-09-01 15:21:42,104104.0,no update,
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84927,1,In what way do you want to compare them? You already know that they are slightly different.,2020-09-01 15:24:53,73930.0,update,no
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84928,1,I just can't read the results,2020-09-01 15:41:30,104104.0,update,no
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84929,1,What do you mean that you can't read the results?,2020-09-01 15:44:56,73930.0,update,no
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84930,1,I can't understand what it means basically: what is the difference between them,2020-09-01 15:51:11,104104.0,update,post
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84931,1,Do you not know what a confusion matrix tells you? Do the decimal numbers instead of integers throw you off?,2020-09-01 16:02:05,73930.0,update,post
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84932,1,"yes, also I make confusion between the anti-diagonal and what's outside. I know I have to read what's outside but I can't understand what are the most ""relevant"" cells I have to look at!",2020-09-01 16:07:36,104104.0,no update,
81034,<machine-learning><python><nlp><nltk>,81114,104104.0,104104.0,84942,1,"You should not compare the confusion matrices directly but the metrics that derive from it, like f1, roc_auc, precision, recall, etc https://en.wikipedia.org/wiki/Confusion_matrix",2020-09-01 20:18:58,92050.0,no update,
33766,<python><bigdata><numpy>,33771,53928.0,53928.0,39719,0,"What is `output` exactly, within `compare_neighbor`? You have not initialised it. Also, is there an upper limit on the integer contents of your main input array?",2018-06-28 20:11:07,45264.0,update,comment
33766,<python><bigdata><numpy>,33771,53928.0,53928.0,39720,0,Thanks for your replies. It's true that I just showed the principle lines. Output is a np.ndarray. Then I did a zero-padding of one layer to the surface of the matrix in order to avoiding the out of range from the check_neighbor function,2018-06-28 20:19:58,53928.0,no update,
33766,<python><bigdata><numpy>,33771,53928.0,53928.0,39733,0,"I think you are on the right lines, using an approach like @jayprich's. Your **Solution 1** code does not work though, as there are basic copy/paste errors I think. Can you fix it? Also, is there an upper limit on the integer values that can be present in your input array?",2018-06-29 10:52:36,45264.0,update,comment
33766,<python><bigdata><numpy>,33771,53928.0,53928.0,39736,0,@n1k31t4 thanks for your remark. I edited and added a helper function to remove the lowest or toppest slice. Please try again,2018-06-29 12:05:25,53928.0,no update,
114348,<python><r><learning><coursera>,114349,140335.0,,115612,0,"This is an opinion based question that will be closed. R is generally fine, esp. if you have a specific topic for which there is an history of open research. Python is considered more general nowadays. I am personnaly biased toward Python and would advise to start with the kaggle course.",2022-09-15 8:10:18,303.0,no update,
30559,<python><random-forest><decision-trees><predictor-importance>,30566,24984.0,40853.0,35856,1,How did you use diagnostics and R-squared to check overfitting in Economics?,2018-04-20 9:43:49,36977.0,update,both
73587,<python><r><statistics><monte-carlo>,73590,96454.0,,78314,0,"So is your question, how to generate `n` points generated from a normal distribution given a mean (`mu`) and standard deviation (`sigma`)?",2020-05-05 10:35:38,96159.0,update,post
73587,<python><r><statistics><monte-carlo>,73590,96454.0,,78315,0,"Yes, exactly, sorry for not being more precise",2020-05-05 11:01:50,96454.0,no update,
73587,<python><r><statistics><monte-carlo>,73590,96454.0,,78316,0,"Sure, no problem, I'll write up an answer now",2020-05-05 11:04:27,96159.0,no update,
47628,<python><clustering><unsupervised-learning>,47635,59627.0,,54645,1,Please don't cross-post duplicates! https://stats.stackexchange.com/q/398336/7828,2019-03-19 19:17:39,924.0,update,no
47628,<python><clustering><unsupervised-learning>,47635,59627.0,,54647,1,"If anyone is interested in theoretical solutions, one reference is Clark, Colbourn and Johnson's paper ""Unit Disk Graphs"" https://core.ac.uk/download/pdf/82543588.pdf",2019-03-19 19:40:38,6550.0,no update,
74949,<python><neural-network><visualization><pytorch><graphviz>,82265,97996.0,97996.0,79651,1,"Can you show the, dot / graphviz, code you used to create the image. Maybe you need some hidden nodes and edges and / or some things with `rank = same`  / `rankdir \ LR`.",2020-05-28 9:06:07,98044.0,update,comment
74949,<python><neural-network><visualization><pytorch><graphviz>,82265,97996.0,97996.0,79661,1,I have added the code in the post,2020-05-28 13:14:49,97996.0,no update,
74949,<python><neural-network><visualization><pytorch><graphviz>,82265,97996.0,97996.0,79664,1,This is python code and not graphviz / dot.,2020-05-28 13:40:12,98044.0,no update,
74949,<python><neural-network><visualization><pytorch><graphviz>,82265,97996.0,97996.0,79670,1,I've edited it to the dot source now. Sorry,2020-05-28 15:59:29,97996.0,no update,
42891,<machine-learning><python><dataset><pandas>,42892,64593.0,,49909,0,You should put inplace not implace :) Welcome ;),2018-12-19 17:01:49,28175.0,no update,
42891,<machine-learning><python><dataset><pandas>,42892,64593.0,,49910,0,Can someone flag this as a typo and off topic (because this belongs to SO).,2018-12-19 17:06:00,61418.0,update,post
44456,<python><deep-learning><keras>,44459,66387.0,,51457,3,"You may want to use pixel-wise classification, fully convolutional networks. YOLO based approaches can be considered too.",2019-01-23 17:39:18,28175.0,no update,
97926,<python><random-forest><optimization><gridsearchcv>,97929,110191.0,110191.0,102234,0,2 options: use n_jobs = -1 so that all the available cores are used. Alternative using RandomizedGridSearch instead,2021-07-15 19:19:57,92050.0,no update,
97926,<python><random-forest><optimization><gridsearchcv>,97929,110191.0,110191.0,102235,0,"Yep, I tried that, surprisingly it was faster in 9 minutes. What should be my `cv` and `n_estimators`?",2021-07-15 19:50:11,110191.0,update,comment
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118299,1,What is the accuracy of your model? It might simply be that your current model doesn't fit the data very well and therefore always predict zero.,2023-01-04 11:34:59,75157.0,update,both
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118300,1,@Oxbowerce The accuracy is 0. I tried fitting the RandomForestClassifier on test_feature_vector and tried to predict it on part of train_feature_vector and still got all zero array.,2023-01-04 11:40:31,144509.0,update,post
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118301,1,Then it simply means that your model is unable to fit the data well. This can be caused by the fact that your model is too simple or your features are simply not predictive. Try changing the hyperparameters of your model to see if it improves the performance.,2023-01-04 11:42:09,75157.0,no update,
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118305,1,"I looked at the notebook. You are using VGG models, which were built for detecting objects like car etc. Using that as a feature extraction layer for cancer detection is not going to work well. You can try other feature extraction methods or fit custom layers!",2023-01-04 13:11:59,23204.0,no update,
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118309,1,@hssay Im trying to recreate the architecture from this paper https://www.igi-global.com/gateway/article/full-text-html/269406&riu=true,2023-01-04 13:56:49,144509.0,update,comment
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118336,1,"@gray my apologies. I misunderstood the feature extraction part. So it is VGG16 without the last 3 dense layers, so can be used as a feature extractor ",2023-01-05 7:46:58,23204.0,no update,
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118337,1,"@gray, in the linked paper, they talk of 512 dimensional feature vector. Your X matrix have 25k columns (512 X 7 X 7). Is it possible that in the paper the output of maxpooling layer was aggregated? Since there are ~2k training examples, maybe the ~25k (512 X 7 X 7) columns are too much for a good classification",2023-01-05 7:57:34,23204.0,update,no
117505,<python><tensorflow><cnn><random-forest><vgg16>,117549,144509.0,,118362,1,"@hssay I used the original dataset from the paper, I was using a different one before. I got 29% accuracy on test data on default parameters. With some tuning I've got the accuracy to go up to 63% for now.",2023-01-06 16:43:34,144509.0,update,no
22952,<python><pandas>,22953,28175.0,28175.0,26751,5,"Citation, please.",2017-09-11 17:14:58,381.0,update,comment
22952,<python><pandas>,22953,28175.0,28175.0,26753,5,@Emre [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjBkd3C0Z3WAhVMsBQKHfZ1DVQQFggnMAA&url=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fpython-data-analysis%2Flecture%2FLn156%2Fpandas-idioms&usg=AFQjCNHKDkwSCZabMMM42aghkOrMrQKhsw) you are,2017-09-11 17:17:41,28175.0,update,post
758,<python><tools><version-control>,759,895.0,4647.0,1813,63,"Where is the *question* in this question? Please take a moment to review [the Help Center guidelines](http://datascience.stackexchange.com/help/dont-ask), specifically: ""If your motivation for asking the question is 'I would like to participate in a discussion about ______', then you should not be asking here.""",2014-07-18 15:05:57,322.0,update,no
758,<python><tools><version-control>,759,895.0,4647.0,1837,63,"""You should only ask practical, answerable questions based on actual problems that you face.""",2014-07-21 21:04:02,895.0,no update,
758,<python><tools><version-control>,759,895.0,4647.0,1838,63,"This is practical, answerable and based on an actual problem in much the same way that ""Tell me how to perform data science"" is practical, answerable and based on an actual problem.",2014-07-21 21:44:19,322.0,update,comment
758,<python><tools><version-control>,759,895.0,4647.0,32700,63,See also: [Why is the accuracy of my CNN not reproducible?](https://stackoverflow.com/a/47781091/562769),2018-02-20 6:02:38,8820.0,update,both
10857,<python><pandas><scraping>,10937,3314.0,,10888,10,[Create a CSV first](http://stackoverflow.com/questions/1403087/how-can-i-convert-an-html-table-to-csv).,2016-03-24 2:35:51,381.0,no update,post
10857,<python><pandas><scraping>,10937,3314.0,,10949,10,"As you can  see from the question, creating CSV programmatically is the problem. I need to first get the data by writing a script and that is where I am running into problem.",2016-03-28 14:23:20,3314.0,update,comment
32551,<python><pandas><preprocessing>,32561,2893.0,,38244,1,"Is stratum equivalent to epoch by chance? If yes then it's done to reduce the overall bias of the network by introducing the diversity among the training samples as much as possible, because let's say you are using a nn, nn are's power enough to evenearn the batch order, so to avoid that people do shuffling...",2018-06-03 1:50:08,35644.0,update,no
26206,<machine-learning><python><k-means>,26207,43954.0,29575.0,30586,7,Solved at my own,2018-01-02 16:16:48,43954.0,no update,
76258,<python><xgboost><natural-gradient-boosting>,76269,99244.0,91826.0,80764,4,"Well, not really, I know what the differences are. I am more interested in what context GBM can have a higher score and overfit at a lesser degree than XGBoost. Knowing the difference, it's a bit strange for this to happen. Which is what I am observing.",2020-06-19 12:13:07,99244.0,no update,
76258,<python><xgboost><natural-gradient-boosting>,76269,99244.0,91826.0,80782,4,It does not make sense to compare such algos without tuning.,2020-06-19 16:50:54,36977.0,no update,
76258,<python><xgboost><natural-gradient-boosting>,76269,99244.0,91826.0,80783,4,"I think it makes a lot of sense, you can see from the beginning what is worth spending time on hyper parameter tuning and what is not worth spending time on",2020-06-19 16:54:56,99244.0,no update,
76118,<machine-learning><python><pandas><distribution>,76190,86798.0,86798.0,80656,1,"To understand how a numerical variable is distributed, it's best to firstly plot it's histogram (```plt.hist(variable)```). From there, from the ""shape"" of the distribution, you can get an idea of how the variable could be distributed. If you need any suggestions on this, then if you could edit this question and include a histogram over the data, that would be amazing.",2020-06-17 16:30:43,98614.0,no update,
76118,<machine-learning><python><pandas><distribution>,76190,86798.0,86798.0,80665,1,I did plot a histogram but the shape isn't what I expected it to be. My gut is telling something is wrong. Could you plot a histogram to confirm it matches with what kaggle provides?,2020-06-17 18:22:01,86798.0,update,both
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37162,0,How many annotated images do you have? Why do you need the training to be very fast?,2018-05-15 10:23:44,29587.0,update,post
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37165,0,@JahKnows I want the training to be fast because I will have to annotate images myself.,2018-05-15 11:27:24,52172.0,update,comment
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37244,0,Did I answer it?,2018-05-16 10:19:23,50727.0,update,comment
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37247,0,@DavidMasip could you elaborate on how to find the pixels with Lasso?,2018-05-16 10:47:36,52172.0,update,comment
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37248,0,"Features in image classification are values of pixels. You can select the important features by using lasso (as always, lasso kills most of the features, so the ones that survive are the important ones), and those features are therefore the most important pixels. As Lasso has problems with correlated features, an elastic net very close to Lasso might be a better option.",2018-05-16 12:10:18,50727.0,no update,
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37269,0,"@DavidMasip Thanks, but then the feature-finding wouldn't be really fast because I would have to run that on all pixels.",2018-05-16 16:04:22,52172.0,no update,
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37287,0,"Nope, a Lasso is performed on the whole data, not once for every pixel.",2018-05-17 7:52:42,50727.0,no update,
31678,<machine-learning><python><classification><image-classification>,31679,52172.0,,37288,0,"And unless your computer is from the 1930s, performing a Lasso is lightning fast.",2018-05-17 7:53:23,50727.0,no update,
12999,<python><scikit-learn><predictive-modeling><categorical-data><dataframe>,13001,17772.0,85045.0,14324,1,"how did you get the dataFeatures? Did you combine all the features(Feat1, Feat2..) into a list or so? and what is Y?",2016-07-26 8:46:53,21024.0,update,both
12999,<python><scikit-learn><predictive-modeling><categorical-data><dataframe>,13001,17772.0,85045.0,14325,1,@HimaVarsha I've edited the question to include that :),2016-07-26 8:48:26,17772.0,no update,
12999,<python><scikit-learn><predictive-modeling><categorical-data><dataframe>,13001,17772.0,85045.0,14328,1,What does the Y mean?,2016-07-26 8:55:01,21024.0,update,both
12999,<python><scikit-learn><predictive-modeling><categorical-data><dataframe>,13001,17772.0,85045.0,14329,1,@HimaVarsha Y is dataLabel - I've also added that to the question,2016-07-26 9:03:34,17772.0,update,comment
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66200,1,Can you include the necessary information and tags on your used framework/language and your code tried so far? (e.g. R & ggplot vs. Python & seaborn or matplotlib or...),2019-09-26 11:10:19,79227.0,update,post
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66211,1,"any library or framework is okay for me.
I tried using matplotlib but was not able to do it.",2019-09-26 12:44:36,75702.0,update,post
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66213,1,"so I can assume you use Python? (I'd add that to the question and tags because an R answer is unlikely to get you to your goal then). Can you show the matplotlibe code you tried so far, it may be easier to fix that instead of starting from 0.",2019-09-26 12:50:41,79227.0,update,comment
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66230,1,yeah... I have no idea on how to plot this kind of graph. I have this dataset and wanted to plot a similar graph and found one online.,2019-09-26 15:49:59,75702.0,update,no
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66231,1,I have edited my question with the code,2019-09-26 15:51:15,75702.0,no update,
60798,<python><visualization><data-analysis>,60867,75702.0,75702.0,66459,1,You can simply use the pretty example shown by @bombadilhom [here](https://stackoverflow.com/a/53899544/7306659),2019-09-30 18:46:31,59279.0,no update,
57226,<python><visualization><matplotlib>,57270,78416.0,,62907,0,Sorry but... Why??,2019-08-08 16:46:11,64377.0,update,comment
57226,<python><visualization><matplotlib>,57270,78416.0,,62929,0,@Erwan I want to get the point where the curve starts changing so as to find threshold.,2019-08-09 5:28:21,78416.0,no update,
57226,<python><visualization><matplotlib>,57270,78416.0,,63016,0,"@Sajjadmanal based on the code from my answer in the original post, I created a python package called [kneebow](https://pypi.org/project/kneebow/) which provides all the functionality plus some convenience methods and axis scaling.",2019-08-09 22:59:12,67412.0,update,comment
57226,<python><visualization><matplotlib>,57270,78416.0,,63037,0,@georg-un Wow. You made a whole library out of this question. Amazing..!,2019-08-10 18:51:10,78416.0,no update,
73485,<machine-learning><python><data-mining><pandas><pattern-recognition>,73511,96361.0,32492.0,78237,0,It doesn't seems like a data science problem but some basic data manipulation.,2020-05-04 7:32:11,303.0,update,post
73485,<machine-learning><python><data-mining><pandas><pattern-recognition>,73511,96361.0,32492.0,78242,0,"@lcrmorin if not on this site, where do you suggest they post this question? I would argue that ""basic data manipulation"" is the core on which data science is built",2020-05-04 8:20:21,38887.0,update,comment
73485,<machine-learning><python><data-mining><pandas><pattern-recognition>,73511,96361.0,32492.0,78243,0,"Yes « basic data manipulation » is the core on which data science is built. But as is, it is unclear to me that this question is about data science instead of basic data manipulation. Stack overflow might be more appropriate for exemple.",2020-05-04 8:35:38,303.0,no update,
9652,<python><bigdata>,9654,14487.0,14487.0,9456,2,Welcome to the site :),2016-01-06 16:31:11,11097.0,no update,
27359,<python><keras><training><convolution>,27361,39830.0,45264.0,39856,9,"Can you show me how to save those train features and their corresponding train lables in disk, so that the next time I would not need to compute the same?",2018-07-01 6:19:57,54414.0,update,no
18687,<python><pandas><visualization><dataframe><data-analysis>,18732,15412.0,85045.0,21475,2,"`df=df.groupby('timestamp').sum()` followed by `df.plot(x='timestamp', y='vote_count')`",2017-04-30 9:12:36,381.0,no update,
18687,<python><pandas><visualization><dataframe><data-analysis>,18732,15412.0,85045.0,21503,2,The provided documentation of both libraries should be more than enough for you to figure this out.,2017-04-30 11:07:53,31736.0,no update,
18687,<python><pandas><visualization><dataframe><data-analysis>,18732,15412.0,85045.0,21534,2,@Emre why don't you upgrade your comment into an answer?,2017-05-03 10:47:48,27768.0,update,no
30689,<python><apache-spark><pyspark>,38139,28423.0,98307.0,36018,6,Check out [dask](https://dask.pydata.org/en/latest) documentation and see if it can be of use. It is built on python native stack.,2018-04-23 19:00:34,26394.0,no update,
16805,<python><algorithms>,16868,10024.0,10024.0,19011,1,Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/53400/discussion-on-question-by-andrea-ianni--integer-programming-formulation-which).,2017-02-10 14:50:10,21.0,no update,
33247,<python><missing-data>,33251,51399.0,,39089,2,Pandas and SFrame.,2018-06-16 12:07:08,28175.0,no update,
39484,<python><dataset><pandas><data-cleaning>,39493,59919.0,59919.0,46462,0,You need to use integers to index a df,2018-10-10 19:34:27,35644.0,no update,
20587,<python><pandas><dataframe>,22105,35420.0,35420.0,25005,12,what about `col2`?,2017-07-20 22:36:33,23305.0,update,both
20587,<python><pandas><dataframe>,22105,35420.0,35420.0,25006,12,"`df.loc[(df['col1']+df['col1'].shift(1)+df['col1'].shift(2)>0)&(df['col1']+df['col1'].shift(1)+df['col1'].shift(-1)>0)&(df['col1']+df['col1'].shift(-1)+df['col1'].shift(-2)>0)]=1`

however, this leaves the first and last two rows untouched",2017-07-20 22:49:55,23305.0,no update,
17970,<python><visualization><scikit-learn><svm><parameter>,17982,21198.0,,20323,1,"I think the only hint is that is says, the coefficients belong to the primal problem which is briefly stated in the user guide under 1.4.7.1. It then follows probably more or less from the definition of the separating hyperplane. If your question is whether there are some more details about the coef_ attributes, then the answer is probably no. The documentation, in general, is not very detailed regarding implementation details.",2017-03-29 23:57:09,23305.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31731,1,would you say the error is for which line?,2018-01-31 14:43:27,28175.0,update,comment
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31733,1,"It is in the line `(tt_val, p_ttest) = ttest_ind(case, ctrl)`",2018-01-31 14:54:54,45433.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31737,1,"When you drop rows based on na's and assign it two variables, they might not be of same length. What you need to do `inplace` the `dropna`'s and then take `ctrl` from there.",2018-01-31 15:06:35,26394.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31746,1,@KiriteeGak eveen with doing that im still having the error. The columns for case and control are actually 92 and 95.,2018-01-31 16:25:47,45433.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31747,1,@Media  any help on how to numpy broadcast will solve this I guess,2018-01-31 16:27:35,45433.0,update,comment
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31748,1,@KiriteeGak any help on how to numpy broadcast will solve this I guess,2018-01-31 16:27:50,45433.0,update,no
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31750,1,At least point us to the line in which you are encountering the error. Debugging will be easier. And I am still quite sure what I had mentioned in my previous comment is the issue.,2018-01-31 16:35:48,26394.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31752,1,"@KiriteeGak  Its in the line `(tt_val, p_ttest) = ttest_ind(case, ctrl, equal_var=False)`. I have calculated the frequency of case, control in my progarm as you can see and I have the number 92 as case i.e. label is 0, 95 as control i.e. label is 1",2018-01-31 16:55:05,45433.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31753,1,"Try to do this `df_no_na=df.dropna(); (case, ctrl) = (df_no_na['a'], df_no_na['b'])` and do the `t_val, p_val = ...` step and tell me what error  comes up, if any.",2018-01-31 17:48:40,26394.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31789,1,"@KiriteeGak Hi I got what the issue is. its actually the objects created by the `xs` method of the Pandas DataFrame look like two-dimensional arrays. These must be flattened to look like one-dimensional arrays when passed to `ttest_ind`. The values attribute of the Pandas objects gives a numpy array, and the `ravel()` method flattens the array to one-dimension. it should be `(tt_val,p_ttest ) = ttest_ind(case.values.ravel(), ctrl.values.ravel(), equal_var=False)` . With this its working fine.",2018-02-01 14:21:18,45433.0,no update,
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31790,1,"Cool. :) Please provide the solution to your own question and mark the answer right, so that question wont pop again onto the homepage later.",2018-02-01 14:26:40,26394.0,update,comment
27295,<python><statistics><scipy>,27342,45433.0,29575.0,31792,1,@KiriteeGak Sure.. posted the answer,2018-02-01 14:32:04,45433.0,no update,
26555,<machine-learning><python><regression><linear-regression>,27182,44519.0,,30956,2,What's `X` and `y`?,2018-01-12 9:08:19,18375.0,update,no
26555,<machine-learning><python><regression><linear-regression>,27182,44519.0,,38877,2,"Why you are adding 50 ones in the 1st column?
X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)",2018-06-12 8:34:43,53532.0,update,both
26555,<machine-learning><python><regression><linear-regression>,27182,44519.0,,44689,2,Its impossible to calculate independent value using dependent value.,2018-09-10 9:35:56,58850.0,no update,
6727,<python><csv>,6747,9035.0,,14396,3,If the answers solve your problem please mark the one you like.,2016-07-28 22:57:32,381.0,no update,
10459,<python><statistics><visualization><pandas>,10461,16096.0,11097.0,10401,38,https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html,2016-03-01 8:27:03,381.0,no update,
10459,<python><statistics><visualization><pandas>,10461,16096.0,11097.0,106023,38,Pandas Profiling does this for you with one line https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/,2021-10-07 2:02:23,484.0,no update,
61470,<machine-learning><python><pandas><numpy>,61481,83459.0,56354.0,66888,1,"Use Anaconda:
https://www.anaconda.com/distribution/",2019-10-09 11:20:43,56354.0,no update,
106626,<machine-learning><python><classification><cross-validation><pipelines>,106638,111822.0,111822.0,108776,0,Would this not be solved by changing the method called from `fit_transform` to just `transform`?,2022-01-02 13:12:55,75157.0,update,post
106626,<machine-learning><python><classification><cross-validation><pipelines>,106638,111822.0,111822.0,108777,0,"@Oxbowerce no, because then `CountVectorizer` or `TfidfVectorizer` would never be fitted. It would lead to a `sklearn NotFittedError: Vocabulary not fitted or provided`.",2022-01-02 13:21:53,111822.0,no update,
106626,<machine-learning><python><classification><cross-validation><pipelines>,106638,111822.0,111822.0,108779,0,Why not fit them during fit then?,2022-01-02 14:48:56,55122.0,update,comment
106626,<machine-learning><python><classification><cross-validation><pipelines>,106638,111822.0,111822.0,108783,0,"As @BenReiniger mentioned, why then not simply fit the vectorizer within the `fit` method?",2022-01-02 15:27:50,75157.0,update,no
106626,<machine-learning><python><classification><cross-validation><pipelines>,106638,111822.0,111822.0,108784,0,"You are right guys, I indeed left that out. Such a stupid little mistake. Thanks a lot to both of you, it works now! :)",2022-01-02 15:30:25,111822.0,no update,
82644,<python><statistics><simulation><monte-carlo>,82646,104161.0,,86709,0,Monte-carlo is used for arbitrary distribution !,2020-10-06 21:46:01,86325.0,no update,
96945,<machine-learning><python><scikit-learn><text-classification>,96946,109113.0,,101158,1,probably or possibly?!,2021-06-22 8:13:00,8878.0,update,no
96945,<machine-learning><python><scikit-learn><text-classification>,96946,109113.0,,101159,1,@KasraManshaei probably,2021-06-22 8:38:03,109113.0,no update,
32368,<machine-learning><python><scikit-learn><clustering><feature-selection>,32377,52664.0,50727.0,38003,4,"Why (`a`, `c`, `d`) if `a` and `d` have 3 elements in common but `a` and `c` have 4 elements in common?",2018-05-30 8:55:59,52109.0,update,no
32368,<machine-learning><python><scikit-learn><clustering><feature-selection>,32377,52664.0,50727.0,38005,4,"Because c and d also have 2 elements in common, also, it's for clustering purposes, not couple creation, I just want a method to create a bucket where all similar elements would end up",2018-05-30 9:12:25,52664.0,no update,
19405,<python><clustering><image-classification><image-recognition>,19426,32936.0,,22532,1,Please don't cross-post.,2017-06-02 21:46:54,924.0,no update,
19405,<python><clustering><image-classification><image-recognition>,19426,32936.0,,26867,1,Try [this](https://github.com/BatyaGG/Gaussian-Mixture-Models) implementation based on EM.,2017-09-14 15:15:38,39252.0,no update,
45797,<python><nlp>,45877,67966.0,,52712,1,"""find articles-duplicates of a given reference article"" is a bit unclear my friend. What are you looking for exactly?",2019-02-19 10:43:49,8878.0,update,comment
72820,<python><sampling>,72844,95550.0,26019.0,86488,1,This is interesting to me. I have a similar problem. Did you ever find an answer on your own?,2020-10-01 2:05:40,45799.0,update,comment
72820,<python><sampling>,72844,95550.0,26019.0,86538,1,@oaxacamatt it turned out I had errors in my code slowing the whole process down. My bad I should have marked Fnguyen s answer below as the correct answer. Sorry I have nothing more to offer.,2020-10-02 2:12:54,95550.0,update,both
5038,<classification><python>,5071,985.0,,5340,0,What type of data is `y`?  Is it an int or a float?  Do you use `from __future__ import division`?,2015-02-05 9:36:42,8110.0,update,post
5038,<classification><python>,5071,985.0,,5348,0,y is an int... changed it to a float and solved the issue! thanks a ton!,2015-02-06 7:14:07,985.0,no update,
5038,<classification><python>,5071,985.0,,5352,0,"Great!  I have added the answer as an answer below, please mark it as correct so others can find it more easily.",2015-02-06 9:42:57,8110.0,no update,
48035,<python><matplotlib><seaborn>,48044,70243.0,67328.0,55032,5,https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies,2019-03-26 15:00:22,60221.0,no update,
48035,<python><matplotlib><seaborn>,48044,70243.0,67328.0,55041,5,"@Alexis, please I have made edits to the question.",2019-03-26 15:55:06,70243.0,no update,
48035,<python><matplotlib><seaborn>,48044,70243.0,67328.0,55042,5,@Esmailian I have made some edits but I still have some issues. Please take a look.,2019-03-26 15:55:53,70243.0,update,both
48035,<python><matplotlib><seaborn>,48044,70243.0,67328.0,55043,5,"It is better to say ""I want the percentages to the right of horizontal bars""",2019-03-26 15:58:46,67328.0,update,post
48035,<python><matplotlib><seaborn>,48044,70243.0,67328.0,55044,5,@Esmailian I have edited it Sir. So whats the solution?,2019-03-26 16:06:03,70243.0,update,post
67067,<python><scikit-learn><regression><linear-regression><weight-initialization>,67073,82702.0,,72434,4,Why do you want to do this? By definition different models work in different ways: assigning coefficients is not the only way and even for those which do the coefficients might not be comparable.,2020-01-26 14:13:55,64377.0,update,no
67067,<python><scikit-learn><regression><linear-regression><weight-initialization>,67073,82702.0,,72441,4,"@Erwan I am interested to see the weights each model will assign each feature I have in my data, and see areas where the models assign different weights to some feature Xi for example, while they might all agree on assigning low weight for example to some feature Xj",2020-01-26 15:56:15,82702.0,update,comment
116242,<python><classification><machine-learning-model><xgboost>,116413,142876.0,29169.0,117349,0,"precision, recall, f1 score depend on the cut-off value. Change your cut-off value you will get a different answer. I suggest tuning the cut-off value for your business problem if still want to use f1 score. Better is to use a proper scoring rule - https://www.fharrell.com/post/class-damage/. Like Darkness says below, get more data. Does your business problem really only have 2 features? The file says simulateddata. Simulate more data.  Personally I do not like smote and over/under sampling may not be needed. You did not mention the imbalance ratio. Every model does not need 50-50 split.",2022-11-17 10:08:15,28780.0,no update,
116242,<python><classification><machine-learning-model><xgboost>,116413,142876.0,29169.0,117383,0,"Please let me know how I can change the cut off value .The actual business problem has 100,000 rows and 40 columns. This data has been simulated with 2 columns to  better understand why the model does not work.  The imbalance ratio is 93:7 .Thanks in Advance",2022-11-18 3:01:18,142876.0,update,both
116242,<python><classification><machine-learning-model><xgboost>,116413,142876.0,29169.0,117389,0,"predict_proba() gets the probability. Then the cutoff value is optimized for the problem. Usually take in benefits/revenue of TP & TN vs costs of FP & FN. Or some other optimization or business problem metric.  93-7 is not heavily imbalanced. I typically work with models in the 99.99 to 0.01 and less. If I rebalance, I do not go above 90-10. But when there is a 99.99 to 0.01 I might go to 99-1. Instead of focusing on imbalance and hyperparameters, I recommend to focus on the data and cutoff with data priority. Maybe the data is not separable or you need more.",2022-11-18 11:39:16,28780.0,no update,
116242,<python><classification><machine-learning-model><xgboost>,116413,142876.0,29169.0,117409,0,Using predict_proba() and a proper scoring rule as described by the link you shared has solved my issue . Thanks for your help. Please add it as an  answer instead of a comment so that I can close this question,2022-11-19 5:18:56,142876.0,no update,
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10001,27,Did you try map-reduc'ing in a framework like Spark?,2016-02-06 14:23:05,11097.0,update,comment
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10002,27,Nope.. how does it work and can you please direct me..,2016-02-06 14:46:32,16024.0,update,comment
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10003,27,Pl go through [Spark's documentation](http://spark.apache.org/) for understanding it :),2016-02-06 14:48:24,11097.0,no update,
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10090,27,See if [this](https://github.com/saurfang/spark-tsne) Spark implementation works.,2016-02-11 17:55:32,381.0,no update,
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10092,27,@Emre: Which language is used in that implementation ? It seems there is bit of R and scala.. I haven't worked in any of these.. I was looking for python implementation.,2016-02-11 19:03:00,16024.0,update,comment
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10093,27,It's Scala for Spark. If you want a python implementation you might be able to translate it; Spark runs on python too.,2016-02-11 19:07:20,381.0,no update,
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10094,27,@Emre So does it mean I should install `Spark` and compile this `spark-tsne` and then import as a module in python.,2016-02-11 19:20:54,16024.0,update,comment
10103,<python><bigdata><nlp><scikit-learn><dimensionality-reduction>,28772,16024.0,11097.0,10098,27,"No, it means you should read the Scala package and write one in python based on it. Personally I would advise trying to use the Scala package as is. Although I have no personal experience with, [Beaker](http://beakernotebook.com/features) might help you use Scala and python concurrently. As an alternative to t-SNE, you could one of many python neural network libraries to find a autoencoded 2D embedding.",2016-02-11 20:45:02,381.0,no update,
31545,<python><classification><nlp><text-mining>,31654,50625.0,,37063,3,"It would help to do some analysis of the scripts to identify aspects that distinguish the various categories. Once you do this manually for some examples, you could consider writing some rules based on the observations. The rest of the examples can be labeled using the rules. For a model-based approach, if you label a small set of examples (~50), then a simple model (Naive Bayes, etc.) can potentially be trained on these.",2018-05-12 13:28:28,33759.0,no update,
31545,<python><classification><nlp><text-mining>,31654,50625.0,,37077,3,what kind of rules do you mean?,2018-05-12 20:15:20,50625.0,update,both
31545,<python><classification><nlp><text-mining>,31654,50625.0,,37080,3,"The rules can target specific patterns that occur in the scripts. These can be individual words, phrases, or multiple words (not necessarily consecutive) that indicate a specific category. This can be used if the analysis shows common patterns across scripts in a category.",2018-05-13 4:36:38,33759.0,no update,
31545,<python><classification><nlp><text-mining>,31654,50625.0,,37114,3,"@raghu can you please write the first comment as an answer, so I can mark it as such? :) And thanks for your help :)",2018-05-14 11:02:11,50625.0,update,no
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62407,1,I do not understand why chi2 would require non negative values in the first place- maybe you can ellaborate?,2019-08-01 11:08:03,29217.0,update,comment
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62411,1,There is an answer for: https://www.quora.com/Why-cant-chi-squared-be-calculated-on-negative-values,2019-08-01 11:59:58,78808.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62419,1,"Since you're using pandas, perhaps the built-in handling of such operations would be faster than custom iteration? Have you tried something along the lines of: `df - df.min()`? The value of `df.min()` can be stored for later inverse transformation like: `df + df_min`.",2019-08-01 13:02:14,78674.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62421,1,Im sorry I didnt get it. Can you give me an example If you dont mind?,2019-08-01 13:15:56,78808.0,update,no
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62422,1,But the problem is Im not sure if its best way to handle this problem to summing all values with 8,2019-08-01 13:16:49,78808.0,update,both
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62429,1,"No worries. My previous comment is unrelated to the actual topic of the question (I hastily jumped in with suggestions) and relates only to the performance issues you were experiencing with custom iteration in your code. You are right that ""shifting"" the values is not the solution, since the chi-squared test works with a contingency table that is based on your data, not the actual data itself.",2019-08-01 13:50:50,78674.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62431,1,"You can use [pd.crosstab](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) to calculate a contingency table from your data, then use that for the chi-squared test. Negative values in your data are not a problem, but the data should be discrete (integers, categories, etc)",2019-08-01 13:57:32,78674.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62438,1,"Sadly my dataset is not discrete (It consists of very small spaced float type numbers like 1.157, 1.268, 2.56 etc.) and even I use this function I dont know how to integrate to my code.",2019-08-01 15:05:47,78808.0,update,post
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62446,1,"You can always bin your continuous data into discrete intervals by using [pd.cut](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html). Perhaps [this tutorial](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/) will help you understand how the chi-squared test can be applied to your data. If it doesn't help, please update the question with your latest code and I'll attempt to answer it.",2019-08-01 16:07:10,78674.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62518,1,I didnt get it how to implement pd.cut. Updated my all code with comments. If you dont mind can you take a look?,2019-08-02 9:30:19,78808.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62571,1,"Of course, I'll try to be as thorough as I can in my answer.",2019-08-03 10:05:45,78674.0,no update,
56747,<machine-learning><python><optimization>,57092,78808.0,78808.0,62632,1,"Thanks a lot, I really appreciate it",2019-08-05 9:00:04,78808.0,no update,
117254,<python><nlp><machine-translation>,117257,140946.0,,118124,0,"I have also tried to use the following code from [here](https://www.tutorialspoint.com/python_text_processing/python_text_translation.htm) with no success  ```from translate import Translator
translator= Translator(to_lang=""German"")
translation = translator.translate(""Good Morning!"")
print translation ```",2022-12-22 19:12:35,140946.0,update,both
109823,<python><r><conda>,110554,134383.0,,111655,0,"I don't think it is possible (currently) to load multiple Python instances in the same R session. Maybe you can try running multiple R sessions with some shell script rather than handling the logic in a package?

For #1, you can try to force reticulate to use the right env by setting the `RETICULATE_PYTHON` environment variable.",2022-04-16 5:54:44,87594.0,no update,
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47891,1,The dataset is too much biased towards negative class. positive:negative is around 1:6. Try to run the model with equal amount of positive and negative examples and see how it works.,2018-11-06 12:19:25,16024.0,no update,
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47911,1,"Everyone here is talking about imbalanced data but based on the reported confusion matrix I don't think this is a severe problem in this scenario. The problem I see is that you are claiming that 95% accuracy is unrealistic. And the question is: Why? What are your assumptions to make such claim. In my opinion, RandomForest algorithm is able to achieve those or even better results if the data is informative enough. Maybe you should try and replicate your experiments with other algorithms (XGB, SVM, kNN, MLP...) and see if the results are consistent. Perhaps your data is better than you thought!",2018-11-06 16:10:59,45166.0,no update,
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47918,1,"Thank you both for your answers.
For yazhi: am I really suppose to balance it? Going theoretical (sorry for it), shouldn't I try to come as close as possible to a i.i.d. of the real-world distribution D? I don't exactly know D, but for sure the Italian business environment has at least a 100:1 ratio of legal firms every mafia one.
For TitoOrt: I'd say it's unrealistic because otherwise I'd have found an almost perfect detection model for mafia firms, which seems quite ambitous.",2018-11-06 17:00:06,62107.0,no update,
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47920,1,"I have to concur with @TitoOrt.  The imbalance does not look terrible and it seems that the RandomForrest is performing well.  Accuracy can be a bit misleading at times.  For example, if the data was 100:1 you could easily get an accuracy of 99% simply classifying everything as non-mafia.  So, I would also look at the `sensitivity` and `specificity` measures to understand the model's performance. Since you are using cross validation, the performance measures should be relatively stable on unseen data.  That is why we perform CV after all, to understand the performance on unseen data.",2018-11-06 19:15:31,23159.0,update,both
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47940,1,"@FrancescoAmbrosini you do well by no trusting exceptionally good results :) However, I am not implying that your system is necessarily good though. Maybe somewhere when the data is being collected/generated and/or preprocessed you did something that makes the samples different between the two classes and this is what the algorithm is detecting. But it is hard to say. I'd say that the RF algorithm is doing a good job but if you feel the results are to good, take a good dip in the data, make plots and statistical tests to double check there are no issues like the ones I mentioned.",2018-11-07 7:33:28,45166.0,no update,
40815,<machine-learning><python><classification><random-forest>,40854,62107.0,62107.0,47957,1,"Skiddles, as you can see from my edit, sensitivty was not exceptional, thank you!
TitoOrt, I'm afraid you actually nailed it. I was given the dataset from my professor, but it's data collection was probably biased towards a certain kind of companies :(",2018-11-07 12:44:06,62107.0,no update,
9793,<machine-learning><python><scikit-learn><prediction>,9796,15412.0,85045.0,10127,1,Can you add the code you're trying to run ?,2016-02-14 7:52:20,16260.0,update,comment
9860,<python><pandas><scraping>,9878,3314.0,13413.0,9688,0,"Which version of pandas are you using? With 0.17.1 I was able to read a csv consisting only of ""©1990-2016 AAR,All rights reserved"" with and without the `encoding` flag.",2016-01-19 21:44:34,381.0,update,both
44795,<machine-learning><python><nlp><clustering><similarity>,44799,66738.0,,51917,1,Please do not cross-post duplicate questions: https://stackoverflow.com/q/54429050/1060350,2019-02-02 9:07:31,924.0,no update,
44795,<machine-learning><python><nlp><clustering><similarity>,44799,66738.0,,51936,1,I'm sorry. What is the right course of action in this situation? Should I delete one of them?,2019-02-02 19:04:42,66738.0,update,no
44795,<machine-learning><python><nlp><clustering><similarity>,44799,66738.0,,51957,1,"As the answer here is trivial, you can try to delete this copy (it's not likely to help anybody else). But it's mostly for the future.",2019-02-03 8:31:18,924.0,no update,
53513,<machine-learning><python><scikit-learn><pandas><kaggle>,53515,73912.0,,59323,3,Quick note: you have loaded the same dataframe for both `train` and `test`,2019-06-10 9:41:50,65131.0,no update,
53513,<machine-learning><python><scikit-learn><pandas><kaggle>,53515,73912.0,,59346,3,"I've been displeased with how OneHotEncoder works based on hot LabelEncoder works. like the accepted answer, `pd.get_dummies` does OneHotEncoding without the (unnecessary) hassle of setting up the class.",2019-06-10 20:25:12,70898.0,no update,
111456,<machine-learning><python><classification>,111487,136403.0,,112953,0,"It is my opinion that it is a problem caused by severely imbalanced data. Oversampling is not only approach, down sampling also may work as well as hyper parameters tuning (eg class weights). In the final analysis if nothing else works, then that is the way it is with some data. Cannot do better.",2022-05-31 19:31:21,100269.0,update,post
28025,<machine-learning><python><deep-learning><tensorflow><decision-trees>,28029,46433.0,29575.0,74704,10,"Even if Decision Trees were offered for TensorFlow(which they don't), I wouldn't suggest using it. There is so much technical overhead with getting a tensorflow model going compared to sklearn...
Pick the right tool for the job! <https://scikit-learn.org/stable/modules/tree.html>",2020-03-12 19:41:44,56302.0,no update,
40477,<python><keras><cnn><numpy><reshape>,40543,61156.0,61156.0,47601,0,Videos generally have 4 dimensions,2018-10-31 6:03:32,35644.0,no update,
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60217,1,"Someone with appropriate reputation points, please add document-classifier tag.",2019-06-26 9:05:19,76674.0,no update,
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60218,1,It is just a classification problem based on whatever text you have available. But how will you extract keywords from documents?,2019-06-26 9:19:29,71006.0,update,both
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60229,1,I have the keywords already in a `csv` file as shown above. I want to know how to classify the `.doc` files now as according to the words found in them using a classifier.,2019-06-26 10:20:44,76674.0,no update,
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60234,1,"Would really be helpful, if you upvote the question so it can be answered.",2019-06-26 12:01:54,76674.0,update,no
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60239,1,you can start with a simple bag-of-words representation of your documents (which are just vectors). use the python collections.Counter or the appropriate [nltk](https://www.nltk.org/) tool to perform it.,2019-06-26 12:36:37,21825.0,no update,
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60243,1,ok great will try that and let you know @manu-h but how to make a dataset out of `.doc` files ?,2019-06-26 12:50:59,76674.0,no update,
54515,<machine-learning><python><classification><model-selection><classifier>,54555,76674.0,76674.0,60248,1,@Rex5 you may use any tool extracting plain text (maybe [tika](https://www.nltk.org/)?) Don't hesitate to precise your question when you have played a little with all those tools and are stuck again.,2019-06-26 13:13:50,21825.0,no update,
104311,<python><pandas>,104352,127960.0,,107507,0,Could you add your required output?,2021-11-20 6:37:34,83275.0,update,comment
104311,<python><pandas>,104352,127960.0,,107511,0,"Aren't pandas/python programming questions best asked on stackoverflow? You'll get a million times more exposure. From the FAQ: ""If you think a question is equally appropriate on multiple sites, ask on the site with the most users """,2021-11-20 10:47:03,471.0,update,post
52956,<python>,53086,75147.0,,58804,1,Update your post with what you have tried and why it is not working.,2019-05-31 23:38:22,10814.0,no update,
52956,<python>,53086,75147.0,,58876,1,I don't know what kind of barchart here so don't know how to handle it? could you give any idea about what kind of chart?,2019-06-02 10:07:38,75147.0,update,both
31352,<python><scipy>,31356,49914.0,,44957,11,"Beside @Federico answer, one can find more description and some good examples from [this link](https://cmdlinetips.com/2018/03/sparse-matrices-in-python-with-scipy/).",2018-09-13 7:16:01,59027.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20233,1,soemthing similar happens to me while trying another classifier. I restarted the kermel and all was good (strange but true).,2017-03-27 19:25:19,30437.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20320,1,"switched from notebook to desktop PC with another CPU, GPU and tensorflow-gpu 1.1.0rc0, still getting same results.",2017-03-29 16:32:26,30396.0,update,comment
17885,<python><tensorflow><keras>,18863,30396.0,,20340,1,I  tried the script myself and get the same results as you (acc: 0.81 and val_acc: 0.72) and again (acc: 0.81 and val_acc: 0.7087). apparently others can replicate it https://github.com/kimardenmiller/NLP_CNN/blob/master/Embeddings/0.0.4%20word%20index%20save.py and here https://groups.google.com/forum/#!msg/keras-users/s_veHQbyQmc/h419lLudBQAJ;context-place=topic/keras-users/4wUnPDutY5o  I will take a closer look,2017-03-30 16:07:10,30437.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20345,1,Both links correspond to Keras version before 2.0. Maybe we should try to roll back to earlier versions?,2017-03-30 17:59:05,30396.0,update,comment
17885,<python><tensorflow><keras>,18863,30396.0,,20346,1,"I found a small difference but had not time to check it . embedding_matrix = np.zeros((num_words + 1 , EMBEDDING_DIM)). Take a look and let me know",2017-03-30 18:07:57,30437.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20347,1,"Here is the commit, that replaces nb_words+1 by nb_words https://github.com/fchollet/keras/commit/f1a95869ebad98db11aba463e7dab031de6dcba0. Are you sure that this is the issue?",2017-03-30 18:15:02,30396.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20350,1,u r right. It is not the issue. however I can replicate the 95 % accuracy (I get 92 %) using the code in first link. I am using keras 2.0.,2017-03-30 18:56:34,30437.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20359,1,"Yep, I'm also got even better than 95% accuracy (0.11 logloss  on validation) after 7 epochs with the link that you have provided. So, the question is - what is the main difference in these examples?",2017-03-30 21:29:26,30396.0,no update,
17885,<python><tensorflow><keras>,18863,30396.0,,20623,1,have you made any progress on this ?,2017-04-08 19:46:44,30437.0,update,comment
58825,<python><scikit-learn>,58840,76137.0,76137.0,64350,1,"1) How are you vectorizing your sentences? 2) Which clustering algorithm are you using, k-means?",2019-09-07 12:37:56,54395.0,update,no
58825,<python><scikit-learn>,58840,76137.0,76137.0,64353,1,"Im using k-means  and i have used `TfidfVectorizer`  and with it, i got around 3 for max `silhouette_score `, and then I have tried `CountVectorizer` and with it, i got around 7 for max `silhouette_score `.",2019-09-07 17:16:06,76137.0,no update,
71751,<python><keras><tensorflow>,71775,93685.0,23143.0,76710,4,"Cross-posted: https://stackoverflow.com/q/61039480/781723, https://datascience.stackexchange.com/q/71751/8560.  Please [do not post the same question on multiple sites](https://meta.stackexchange.com/q/64068).",2020-04-05 17:43:48,8560.0,no update,
71751,<python><keras><tensorflow>,71775,93685.0,23143.0,76711,4,"Sorry about that! I am OK to delete either post, but I received interesting answers on both SO and the Data Science Stack Exchange and I would like to keep track of them all. What should I do?",2020-04-05 19:02:43,93685.0,update,comment
71751,<python><keras><tensorflow>,71775,93685.0,23143.0,76714,4,"The accuracy that you report, is it the training or testing accuracy? In general, have you checked whether your model can reach a very high training accuracy on CIFAR-10 after training it long enough? If it is not the case, then your model is too simple and you should add more parameters. I have no source for this but from experience I would say CIFAR-10 is much more complex to learn than MNIST.",2020-04-05 9:30:30,74728.0,update,no
71751,<python><keras><tensorflow>,71775,93685.0,23143.0,76715,4,"I am reporting the training accuracy, since I am looking at the output of `history = model.fit(train_images, train_labels, epochs=10)`.",2020-04-05 10:19:22,93685.0,no update,
71751,<python><keras><tensorflow>,71775,93685.0,23143.0,76716,4,"You are not doing anything wrong, model that works with one dataset does not mean it will work in another dataset. MNIST datasets are much easier to classify than CIFAR10.",2020-04-05 11:23:34,17.0,update,both
77590,<machine-learning><python><churn>,77667,100637.0,,82188,1,Can you write SQL?,2020-07-13 20:00:36,99684.0,update,comment
77590,<machine-learning><python><churn>,77667,100637.0,,82190,1,"Yes, I can write SQL.",2020-07-13 20:06:35,100637.0,no update,
31251,<python><deep-learning><reference-request><pytorch><books>,31254,27526.0,,36648,2,Take a look at coursera's datascience in python 5 specialization. They introduce good free books. They are also available in their reference page.,2018-05-05 12:37:35,28175.0,no update,
31100,<machine-learning><python><neural-network><tensorflow><autoencoder>,31106,47380.0,,36472,1,"It would be helpful to also see the validation/test accuracy of your code, in order to compare it to that in the notebook. The training accuracy on its own can sometimes be a little misleading or _nonrepresentative_.",2018-05-02 9:08:41,45264.0,update,no
20275,<python><clustering><outlier>,24389,4668.0,4668.0,24672,2,maybe providing data and your code would make it easier for people to willing to help?,2017-07-09 20:15:42,2968.0,no update,
20275,<python><clustering><outlier>,24389,4668.0,4668.0,24675,2,I can include a small case example of 18 points but my questions is more generic. If it can be generalized it would be: should the output of GLOSH algorith be the same as the noise labeled points?,2017-07-09 21:00:46,4668.0,update,no
10060,<python><text-mining><apache-hadoop><k-means><distance>,10067,15927.0,-1.0,9947,6,"Hello @Dawny33 (Pikachu), Lapras here. :) Thank you!",2016-02-02 1:59:35,15927.0,no update,
16348,<python><language-model><sentiment-analysis><encoding>,16356,1426.0,1426.0,18324,4,"It's not clear why do you need to look up in dictionary for ""1-hot encoding""... Could you provide a small sample data set and desired data set based on that sample one?",2017-01-16 17:12:23,20492.0,no update,
16348,<python><language-model><sentiment-analysis><encoding>,16356,1426.0,1426.0,18325,4,"OK, I added an edit, hop it helps.",2017-01-16 17:35:28,1426.0,no update,
16348,<python><language-model><sentiment-analysis><encoding>,16356,1426.0,1426.0,18326,4,Querying dictionaries should be fast; are you not using a `dict`? See also [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing).,2017-01-16 18:47:29,381.0,update,n0
16348,<python><language-model><sentiment-analysis><encoding>,16356,1426.0,1426.0,18333,4,"well there are 100K+ words in the dict (yes, I use it), 1 sentence takes ~1 sec, hence...",2017-01-16 20:16:49,1426.0,update,comment
98096,<python><scikit-learn><xgboost>,98111,120937.0,120060.0,102410,2,If you are trying to using the classifier to classify multiple things that can fail at the same time it's probably better to use a separate model for each of the classes since the model assumes that only one class can be true at a time. Because of this the values are normalized to get the percentage for each class.,2021-07-20 8:31:35,75157.0,no update,
14883,<python><nlp>,14933,20974.0,20974.0,16531,13,"[This](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/) explains how modern chat bots are built, but I wouldn't call it simple. You can learn more by searching for ""question answering"".",2016-11-02 8:36:04,381.0,no update,
14883,<python><nlp>,14933,20974.0,20974.0,16599,13,I so like how people asking for questions and receiving answers mark an answer as acceptable one :P,2016-11-04 11:51:00,2573.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52237,1,"which deep learning library are you using? tensor-flow or pytorch? Also for this type of question, please include your code in the question.",2019-02-09 9:56:27,41525.0,update,post
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52238,1,@LouisT I’m using a numpy implementation of an LSTM should I just put the code for forward propagation?,2019-02-09 9:57:57,63540.0,update,comment
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52239,1,"The NaN can occur at backdrop as well. If your code is too long, put it in a gist on GitHub and include a link to it.",2019-02-09 10:02:28,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52241,1,"@LouisT Just added the link to the gist. Just checking, can you view it? (haven't made a gist before)",2019-02-09 10:41:46,63540.0,update,comment
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52242,1,"congrats on your first gist :)  and yes, I can see the gist. Can you try to modify this code such that someone can simply copy the code and run it on their machine to reproduce the NaN you are seeing? You might need to save the output your `read_mini` in another gist.",2019-02-09 11:12:16,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52243,1,@LouisT I updated the code so it can be run but how do I add the music files to the gist?,2019-02-09 13:01:50,63540.0,update,comment
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52244,1,"on your line 45 `data,durations,offsets = read_midi('./Downloads/*.mid')`. Instead of doing this, you can manually construct the `data`, `duration`, `offset`? This is called mocking or stubbing in testing. This might help with your debugging process in general as well. As this will allow you to see what kind of kinds will cause the error to raise.",2019-02-09 13:21:56,41525.0,update,both
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52263,1,@LouisT I added mock results to the gist and it looks like all the calculated probabilities in the forward step recur to 0 (1.2e-159) so the loss becomes extreme highly but this doesn't happen when I only have a single one-hot feature.,2019-02-09 22:23:22,63540.0,update,post
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52265,1,"@treutum, great, you code failed at L51 `unique_offsets = sorted(list(set(offsets)))`
offsets is a float, I believe it should be an ndarray?",2019-02-09 22:47:48,41525.0,update,post
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52266,1,@LouisT My mistake I have made it into an array and should now run.,2019-02-09 22:57:56,63540.0,update,comment
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52269,1,"Glanced over the code, the `forward_backward` doesn't look right to me. It looks like you are accumulating all of the loss occurred in all steps before doing the backprops. So, you are doing 1 backprops every iteration (epoch). Is that correct?  If so, then that's wrong. You need to do the backprops at every step. So 32427 - 1  backprops per epoch.",2019-02-09 23:59:07,41525.0,update,no
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52270,1,"@LouisT The backprop should run on every step, it runs through the sequence in forward propagation then backwards through the sequence in the second part of forward backward. Does it not?",2019-02-10 0:15:08,63540.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52271,1,"t0 => forward => t1_pred => loss(t1_pred, t1) => backprops, t1 => forward => t2_pred => loss(t2_pred, t2) => backprops, .....",2019-02-10 0:36:22,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52272,1,"It's be clear with the terminolgy here.A **batch** of data contains a number of **series** of data, each **series** contains multiple **steps**. In you example, you are not doing batch at all, so you won't have that dimension. You have `data = np.random.randint(0,251,(32427))` . This is one series with 32427 steps",2019-02-10 0:42:26,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52273,1,I assume you have a reason for doing this by hand in numpy. Using a deep learning library could save you a lot of trouble otherwise.,2019-02-10 0:43:32,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52274,1,@LouisT The data is split into batches during the training step,2019-02-10 0:54:12,63540.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52275,1,"okay I see what you did there regarding the batches. But the problem is still the same. Don't forward through the entire series, then backprops. Do it one time steps at a time",2019-02-10 0:58:42,41525.0,no update,
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52276,1,"Recurrent neural net in general, take the input at t to predict input at t+1. If you want to do this in a single backprops for the entire series (as are you doing it now), you need to do what is called `unrolling` the RNN.",2019-02-10 1:03:46,41525.0,update,comment
45300,<python><neural-network><rnn><feature-construction>,45304,63540.0,63540.0,52277,1,"https://machinelearningmastery.com/rnn-unrolling/ have a read of this, see if it helps",2019-02-10 1:05:30,41525.0,no update,
27288,<python><neural-network><tensorflow><convolution>,30632,42681.0,,31721,2,you have to call it through the object of your model I guess.,2018-01-31 12:15:36,28175.0,no update,
27288,<python><neural-network><tensorflow><convolution>,30632,42681.0,,31725,2,@Media thanks for your help again.. Could you elaborate?,2018-01-31 12:43:55,42681.0,update,comment
27288,<python><neural-network><tensorflow><convolution>,30632,42681.0,,31732,2,"sure, would you first say what is your model variable which you try to optimize? Its better to explain your code first.",2018-01-31 14:45:25,28175.0,update,comment
37220,<python><classification><scikit-learn><class-imbalance><metric>,103921,26019.0,26019.0,43586,3,Is it possible to put your data?,2018-08-21 13:21:22,28175.0,update,comment
37220,<python><classification><scikit-learn><class-imbalance><metric>,103921,26019.0,26019.0,43592,3,"@Media sorry, I can not put my data because it is not too small. However, this issue is also reported here: https://github.com/scikit-learn/scikit-learn/issues/8101 but I couldn't understand how to fix it. Thanks",2018-08-21 13:29:33,26019.0,update,comment
37220,<python><classification><scikit-learn><class-imbalance><metric>,103921,26019.0,26019.0,43599,3,Have you tried to update it?,2018-08-21 14:13:22,28175.0,update,no
37220,<python><classification><scikit-learn><class-imbalance><metric>,103921,26019.0,26019.0,43640,3,"@Media sorry I installed the latest version of scikit using anaconda. However, unfortunately the problem is not solved.",2018-08-22 5:38:47,26019.0,update,post
37220,<python><classification><scikit-learn><class-imbalance><metric>,103921,26019.0,26019.0,43791,3,I guess the issue may still be remained.,2018-08-24 13:29:44,28175.0,no update,
96645,<python><sentiment-analysis>,96729,119351.0,119351.0,100907,1,"As long as the dictionary-based approaches give the same kind of result as the machine learning approach (e.g. positive/negative/neutral), the same accuracy evaluation should be applicable to both. Why do you think the same accuracy evaluation is not applicable for dictionary-based approaches?",2021-06-16 18:59:46,14675.0,no update,
96645,<python><sentiment-analysis>,96729,119351.0,119351.0,100909,1,Maybe I was wrong. So how do I measure accuracy on dictionary-based approaches?,2021-06-16 19:39:34,119351.0,update,no
27888,<machine-learning><python><classification><scikit-learn><multiclass-classification>,27891,46356.0,69822.0,32529,9,"I just saw that you tried under-sampling. Just fyi, Startified K-fold cross validation in Sci-Kit Learn also takes class distribution into account.",2018-02-16 12:20:46,8878.0,update,no
10880,<python><scikit-learn><nltk><gensim>,10901,17309.0,,10905,3,"sounds reasonable to start with NB using a bag-of-words approach... use this as your baseline, then start improving it via tf-idf, feature engineering while exploring alternative ML algorithms.",2016-03-24 20:33:18,6478.0,no update,
10880,<python><scikit-learn><nltk><gensim>,10901,17309.0,,10906,3,"So, just to be clear, would you train a BoW model for each classification?  Then create a BoW model for an incoming doc and run NB against the trained models to test?",2016-03-24 21:09:03,17309.0,update,post
10880,<python><scikit-learn><nltk><gensim>,10901,17309.0,,10907,3,BoW is just the features you use for training and test (a vector of binary values indicating the presence/absence of a word). You can do multiclass with NB (http://stats.stackexchange.com/questions/142505/how-to-use-naive-bayes-for-multi-class-problems). Here is another useful intro link - http://sebastianraschka.com/Articles/2014_naive_bayes_1.html#multinomial-naive-bayes,2016-03-24 21:17:31,6478.0,update,comment
85330,<machine-learning><python><r><clustering><visualization>,85332,69518.0,69518.0,89418,1,Please mention the langauge of your choice in the future so it'll be easier for people to provide help.,2020-11-13 5:56:56,103857.0,update,post
84372,<python><r><random-forest><categorical-data><encoding>,84373,106461.0,,88502,1,"Welcome to Data Science SE. What do you mean ""*I'm curious about the difference in the results I receive using the two implementations*""?",2020-10-22 14:33:16,43000.0,no update,
84372,<python><r><random-forest><categorical-data><encoding>,84373,106461.0,,88503,1,"So its a multilayer question. I'm using RF both as a classifier and also for feature selection with Boruta. My colleague used the R implementation to do feature selection without encoding her categorical data first. She gets somewhat different results than I do with encoded features in python. I figured this has something to do with RFs bias towards categorical features with many options. I'm more comfortable with python, so I was hoping to do some more experimenting with a python implementation, hence the question",2020-10-22 14:40:38,106461.0,no update,
84372,<python><r><random-forest><categorical-data><encoding>,84373,106461.0,,88505,1,"To be more specific, certain high-cardinality categorical features are selected as important in R, but are not selected in python after one-hot encoding",2020-10-22 14:53:42,106461.0,no update,
58631,<python><prediction><class-imbalance>,58899,58488.0,,82125,8,This reference is exactly what you're looking for: https://gking.harvard.edu/files/0s.pdf *Logistic Regression in Rare Events Data* (Gary King),2020-07-12 18:13:51,100655.0,no update,
58631,<python><prediction><class-imbalance>,58899,58488.0,,94653,8,Link to `ssci2015_calibrating.pdf` is dead today.  Can you find online? I cannot because you did not give an author's name or a title of the article.,2021-03-09 23:57:26,74292.0,update,both
58631,<python><prediction><class-imbalance>,58899,58488.0,,94657,8,"@pauljohn32, I believe it was ""Calibrating Probability with Undersampling for Unbalanced Classification"" by Dal Pozzolo et al.",2021-03-10 1:51:12,55122.0,no update,
42465,<machine-learning><python><pandas>,42471,64138.0,,49470,2,KNN requires a distance metric. What is the distance between TRUE and FALSE?,2018-12-11 16:59:26,37184.0,update,no
42465,<machine-learning><python><pandas>,42471,64138.0,,49471,2,How can I find this out? I'm a complete noob and this is my first project on data science,2018-12-11 17:06:10,64138.0,update,comment
27541,<python><pandas>,27696,45534.0,,77696,3,"See my answer here: https://stackoverflow.com/a/61396448/4549682

`conda install -c conda-forge pandas-profiling=2.6.0`

where 2.6.0 should be replaced with the latest version as shown here: https://anaconda.org/conda-forge/pandas-profiling",2020-04-23 20:32:27,18481.0,update,comment
103419,<python><scikit-learn><xgboost><pipelines><imbalanced-learn>,103430,79602.0,,106616,1,Are you sure you have nan values in only the 3 columns mentioned?,2021-10-23 11:05:48,119921.0,update,post
103419,<python><scikit-learn><xgboost><pipelines><imbalanced-learn>,103430,79602.0,,106618,1,I removed them before the pipeline and even without nan values I get this weird performance difference. But I also get this difference with nan values.,2021-10-23 12:08:55,79602.0,update,both
103419,<python><scikit-learn><xgboost><pipelines><imbalanced-learn>,103430,79602.0,,106624,1,Using Columntransformer does not degrade the performance of your model. There must be something you are overlooking in your code. Maybe you are using different parametrs or different number of cv if you are using one. One would have to look at the whole code to figure out the problem,2021-10-23 14:35:56,119921.0,update,comment
13350,<machine-learning><python><deep-learning><regression><keras>,13351,15412.0,85045.0,40388,7,where to download your housing.csv file to be implemted in keras REgressor????,2018-07-11 10:07:43,54912.0,update,comment
86339,<machine-learning><python><time-series><dataset><data>,86401,108459.0,,90446,0,"What do you mean by generating ""reasonable"" dummy variables?",2020-12-07 16:18:08,29169.0,update,comment
86339,<machine-learning><python><time-series><dataset><data>,86401,108459.0,,90454,0,"@Ethan I would like to generate data from the potential underlying distribution of each column without discarding their correlation to each other. Another question would be let's say I found future data for column 2, can I generate reasonable data (meaning previous correlations are not discarded) for the future values of the rest of the columns.",2020-12-07 22:20:24,108459.0,no update,
86339,<machine-learning><python><time-series><dataset><data>,86401,108459.0,,90456,0,"@Ethan 1)Another way to put it would be, how to forecast data without losing correlation of variables, 2) if you have values for one of the variables (dependent one) can you forecast the rest of the variables.",2020-12-07 22:48:40,108459.0,no update,
49197,<machine-learning><python><scikit-learn><decision-trees><logistic-regression>,49213,71218.0,,56282,1,"This is presumably implementation-specific.  You used the tag `scikit-learn`, but that (currently) requires one-hot encoding even for decision trees, so I'm not sure exactly what you want answered.
Similar questions have been asked, but mostly on StackOverflow it seems:
https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data
https://stackoverflow.com/questions/39804733/dummy-creation-in-pipeline-with-different-levels-in-train-and-test-set
https://stackoverflow.com/questions/50630447/how-to-use-pandas-get-dummies-on-predict-data",2019-04-12 19:57:23,55122.0,update,both
49197,<machine-learning><python><scikit-learn><decision-trees><logistic-regression>,49213,71218.0,,56286,1,"So, if I understand, according to the links you refered me to, the path an individual would follow would be the one indicated if all the dummy variables are zero (derived of that categorical).",2019-04-12 20:14:36,71218.0,no update,
49197,<machine-learning><python><scikit-learn><decision-trees><logistic-regression>,49213,71218.0,,56288,1,"Only if you have preprocessed your data to do that.  If you one-hot encode the validation set separately, then the columns will be different than the training ones, and sklearn will give an error in either model.  If you do not one-hot encode, the models won't train on the categorical 'A','B','C' in the first place.  If you one-hot encode the train and validation together, then the training set sees var_D as useless and the model will never care about that dummified feature.  If you modify the columns as suggested in one of those links, then your last sentence is correct.",2019-04-12 20:20:55,55122.0,no update,
49197,<machine-learning><python><scikit-learn><decision-trees><logistic-regression>,49213,71218.0,,56299,1,Maybe split out your question about Logistic Regression to a separate question?,2019-04-13 0:44:09,54096.0,update,comment
88959,<python><nlp><python-3.x><stemming>,88962,111407.0,,93172,6,I see you found help on this portal :),2021-02-05 11:04:13,83937.0,no update,
60990,<python><pandas><data-cleaning>,61318,82577.0,29575.0,66409,1,Simple: you should tell the English teacher that they made a mistake ;),2019-09-30 0:53:34,64377.0,no update,
60990,<python><pandas><data-cleaning>,61318,82577.0,29575.0,66410,1,"More seriously, it depends what you want to do with this data: it might not matter for your application, in which case you leave them as they are. Or it might matter, in which case you standardize them. You need to give more detail about the context if you want people to help you.",2019-09-30 0:56:41,64377.0,no update,
17057,<python><neural-network><classification><keras>,17059,27962.0,,19252,2,Can you add the links of these 3 papers please? Thanks!,2017-02-19 12:34:12,28628.0,update,comment
26565,<python>,26576,41557.0,15412.0,30971,0,This is more of a StackOverflow question. Nothing related to Data Sciene.,2018-01-12 16:50:14,15412.0,no update,
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,35035,17,"Try to deepen the architecture, try adding more filters and dense layers",2018-04-04 9:51:17,35644.0,no update,
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,35038,17,"I added 3 dense layers (128,64,32) and it still produces similar results @Aditya",2018-04-04 9:55:49,49944.0,update,no
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,35040,17,"Keras does not shuffle the data before doing the training/validation split. This means that if the data appearing at the beginning (i.e. the one you train on) is very different from the data appearing by the end (i.e. the one you use for validation), the validation accuracy will be low. Try shuffling the data before feeding it to `fit`.",2018-04-04 10:56:01,14675.0,no update,
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,35085,17,@ncasas Thank you a lot ! it helped ! validation accuracy jumped to 78%.. great improvement.. but my question is there a way to increase accuracy?,2018-04-05 8:17:26,49944.0,no update,
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,54759,17,"I have a similar issue with my model. I'm trying to use the most basic Conv1D model to analyze review data and output a rating of 1-5 class, therefore the loss is categorical_crossentropy.
Model structure is as below # define model model = Sequential() model.add(Embedding(vocab_size, 100, input_length=max_length)) model.add(Conv1D(filters=32, kernel_size=8, activation='relu')) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(10, activation='relu')) model.add(Dense(5, activation='softmax')) Total params: 15,865,417 Trainable params: 15,865,417 Non-trainable params: 0 Tr",2019-03-21 6:10:16,69958.0,update,comment
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,71648,17,What is your data set?,2020-01-10 17:16:30,33245.0,update,both
29893,<python><deep-learning><classification><keras><overfitting>,29896,49944.0,49944.0,91969,17,I think you should use regularizition L1 or L2 and Dropout because your resualt is overfit.,2021-01-11 20:20:46,110180.0,no update,
19476,<machine-learning><python><pandas><feature-extraction>,19491,33020.0,29575.0,22589,3,"Not sure I understand the question. If you want say to access all values in Bcol, and you loaded the dataframe into a pandas df, then simply do df['Bcol']. Was that all?",2017-06-05 12:15:08,27768.0,update,post
24679,<python>,24705,16416.0,,28732,1,"As your link suggests, perhaps try something like this `exe_location=C:/Program Files/RGF/bin/rgf.exe`",2017-11-13 21:33:21,38887.0,no update,
98223,<python><scikit-learn><random-forest><decision-trees>,98246,76801.0,76801.0,102593,1,"There is to my knowledge no separate method that returns both the probabilities and final classification, so you'd likely have to run both `predict` and `predict_proba` yourself.",2021-07-23 13:04:25,75157.0,no update,
98223,<python><scikit-learn><random-forest><decision-trees>,98246,76801.0,76801.0,102594,1,I think I cannot run both and merge the results because the Random Forest will run two random independent processes,2021-07-23 13:06:08,76801.0,no update,
98223,<python><scikit-learn><random-forest><decision-trees>,98246,76801.0,76801.0,102596,1,"I would expect the same inputs to give the same outputs as long as the model is not refit on the data in between the two calls, but to make sure you could try using the `random_state` parameters to set the seed. Another option would be to fork the source code and simply add an extra return argument to the `predict` method since [it already calls `predict_proba` under the hood](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/ensemble/_forest.py#L609).",2021-07-23 13:55:38,75157.0,no update,
98223,<python><scikit-learn><random-forest><decision-trees>,98246,76801.0,76801.0,102600,1,"does it make sense to run `predict_proba` once and have a threshold, say 0.7, that if the probability is greater than the threshold then the result is X, else is Y?  Isn't the 0.7 what's under the hood in `predict`?",2021-07-23 14:51:00,76801.0,update,no
98223,<python><scikit-learn><random-forest><decision-trees>,98246,76801.0,76801.0,102605,1,"In `predict` they are not using a threshold but are simply selecting the class with the highest score, whether that be 0.7 or 0.2.",2021-07-23 15:24:34,75157.0,no update,
47885,<python>,47887,70117.0,,54907,1,Is this from a test / assignment ?,2019-03-24 9:12:58,41771.0,update,comment
9483,<python><linear-regression><xgboost>,9485,14925.0,816.0,9249,12,Please do not [cross-post](http://stackoverflow.com/q/34414255/4993513),2015-12-22 12:03:21,11097.0,no update,
9483,<python><linear-regression><xgboost>,9485,14925.0,816.0,9273,12,@Dawny33 deleted from SO.,2015-12-23 18:14:35,14925.0,no update,
31080,<python><perceptron>,31084,51462.0,,36441,1,"The weights specify a line, 1 and -1 represents whether the point is on each side of the line.",2018-05-01 15:37:22,28175.0,no update,
31080,<python><perceptron>,31084,51462.0,,36443,1,"Okay, I revisited now the entire code. But aren't the weights like 0.something values?How do they represent a line?",2018-05-01 16:22:34,51462.0,update,post
31080,<python><perceptron>,31084,51462.0,,36446,1,"Take a look at [here](https://datascience.stackexchange.com/a/26642/28175). About the weights, do you know about decision boundary? I guess the link may help you.",2018-05-01 18:31:45,28175.0,update,comment
76962,<machine-learning><python>,77006,67502.0,,81454,3,"*In the current process, I never found a motivation to use OOPs concepts.* So don't be persuaded by others, probably less knowledgable in your domain, to use alien concepts.  Personally I make zero use of OOP in most of my data processing.",2020-07-01 14:41:05,97446.0,update,both
76962,<machine-learning><python>,77006,67502.0,,81505,3,"There are exceptions, but in general an OOP style is more widely used for production code written by software engineers. A Functional style, etc is used more by Non-Software Engineers, such as Data Scientists, working on a POC or in a notebook. OOP will help to avoid re-inventing the wheel or writing the same code many times. Functions accomplish this to some degree, but Classes are much more flexible, through class inheritance, polymorphism, etc.",2020-07-02 12:15:35,91826.0,no update,
76962,<machine-learning><python>,77006,67502.0,,81539,3,Are you doing ML regularly using sklearn?,2020-07-03 8:28:38,50727.0,update,comment
42884,<python><plotting><confusion-matrix>,43627,62052.0,,49899,1,You can probably start from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py and add precision and recall as additional row and columns.,2018-12-19 15:42:58,61418.0,no update,
93440,<python><logistic-regression>,93441,116522.0,116522.0,114480,1,Why did you multply X with (y - y_hat) while calculating new weight. Thanks,2022-08-01 20:57:45,138765.0,update,both
67500,<machine-learning><python><scikit-learn><clustering><k-means>,67513,89394.0,89394.0,79008,0,"Hi Devin, I am working on a similar kind of problem. Did you find a way to achieve this with K-Means Clustering? If so, can you please share it here?",2020-05-16 7:00:37,97213.0,update,comment
39077,<machine-learning><python><dataset><k-nn><csv>,39079,60030.0,,45978,2,I don't think you have to shrink the dimension of your data for using KNN! You have to keep that information,2018-10-03 12:57:46,59171.0,no update,
27482,<python><data-mining>,27485,45779.0,45779.0,31973,3,what is the DB that is storing the data ?,2018-02-05 15:48:24,45762.0,update,post
27482,<python><data-mining>,27485,45779.0,45779.0,31974,3,@MaxouMask : good point. edited my question above.,2018-02-05 15:50:05,45779.0,no update,
10490,<machine-learning><python><classification>,10491,16680.0,,10446,5,http://sacred.readthedocs.org/ See also https://www.reddit.com/r/MachineLearning/comments/3npg0d,2016-03-02 22:35:05,381.0,no update,
10490,<machine-learning><python><classification>,10491,16680.0,,10447,5,"@Emre exactly what I was looking for. Please convert it to an answer, and I will happily accept it.",2016-03-02 22:47:41,16680.0,no update,
10490,<machine-learning><python><classification>,10491,16680.0,,10448,5,"Sacred is looking great, though even more valuable can be the featureforge app (see https://github.com/machinalis/featureforge), mentioned in reddit link, especially if you use scikit-learn.",2016-03-02 22:50:16,16680.0,no update,
34357,<python><pandas><sql>,34366,13672.0,103697.0,42079,139,Comments are not for extended discussion; this conversation has been [moved to chat](https://chat.stackexchange.com/rooms/80497/discussion-on-question-by-vy32-why-do-people-prefer-pandas-to-sql).,2018-07-22 3:43:34,21.0,no update,
34357,<python><pandas><sql>,34366,13672.0,103697.0,83496,139,Pandas docs on this are [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html).,2019-09-03 9:33:06,20448.0,no update,
53609,<python><deep-learning><keras><numpy>,53610,75857.0,29169.0,108606,7,"I checked this piece of code and I am wondering on the layer definition in the Sequential model. Running similar structure on my end, ValueError is thrown as in the first command, ""Flatten()"", the input shape is missing / not defined...",2021-12-28 18:11:40,100453.0,no update,
27563,<python><keras><rnn><lstm>,27572,45892.0,,63097,16,Here's a nice tutorial on this topic: https://www.tensorflow.org/beta/tutorials/text/time_series,2019-08-12 14:17:07,54143.0,no update,
27563,<python><keras><rnn><lstm>,27572,45892.0,,109123,16,@CubeBot88 that link now 404's. Perhaps this is the tutorial you indicated? https://www.tensorflow.org/tutorials/structured_data/time_series,2022-01-13 14:11:32,35483.0,update,post
16658,<python><scikit-learn><cross-validation>,16675,26019.0,26019.0,18722,4,what's the result for X_train.shape and y_train.shape?,2017-01-31 20:47:58,23305.0,update,both
16658,<python><scikit-learn><cross-validation>,16675,26019.0,26019.0,18724,4,"@oW_ Thanks. They respectively are (1052, 60) and (1052, ).",2017-01-31 21:11:59,26019.0,no update,
16658,<python><scikit-learn><cross-validation>,16675,26019.0,26019.0,18725,4,"Try y_train.reshape((1052,1))",2017-01-31 21:16:56,23305.0,no update,
16658,<python><scikit-learn><cross-validation>,16675,26019.0,26019.0,18730,4,"@oW_ Thanks. I tried y_train.reshape(len(y_train),1) but unfortunately the same error is reported.",2017-02-01 5:53:14,26019.0,no update,
58861,<python><nlp><named-entity-recognition>,61188,80850.0,,64385,1,"Hello Farzin, 

It would be better if you could explain the problem at hand in an elaborate fashion. With the problem, you could also outline the results you have achieved with Spacy and stanford NER. 

This could help contributors answer your questions better.",2019-09-08 22:10:51,1029.0,no update,
58861,<python><nlp><named-entity-recognition>,61188,80850.0,,64408,1,"thank you @NischalHp for your help, I will edit my question",2019-09-09 11:57:59,80850.0,no update,
58861,<python><nlp><named-entity-recognition>,61188,80850.0,,65541,1,Related [10814](https://datascience.stackexchange.com/a/54290/10814).,2019-09-12 2:14:45,10814.0,no update,
88441,<python><time-series><data-science-model><boosting>,88836,44188.0,,92642,2,"Tree-based models don't follow any parameter-based relation of Features e.g. LinearRegression Or NeuralNetwork. It simply divides the training data into finer spaces, to have a minimum possible impurity. So, it will definitely miss the Trend.",2021-01-25 16:11:53,58341.0,no update,
98008,<python><pandas><one-hot-encoding>,98026,119921.0,119921.0,102317,0,Does is it still occur if you reset the Index of both dataframes before concatenation?,2021-07-17 17:28:48,84891.0,update,comment
98008,<python><pandas><one-hot-encoding>,98026,119921.0,119921.0,102331,0,@Sammy It still gives me nan values in one of the columns which is weird because I performed imputation before encoding!,2021-07-18 6:41:52,119921.0,no update,
98008,<python><pandas><one-hot-encoding>,98026,119921.0,119921.0,102341,0,@Sammy It finally worked after 3 hours of debugging!! Just a silly mistake I made during preprocessing! XD. Man I am dumb! You want to post your answer so that I can mark it as best answer?,2021-07-18 12:46:47,119921.0,update,no
98008,<python><pandas><one-hot-encoding>,98026,119921.0,119921.0,102344,0,Glad you managed to solve it! I've posted my comment as an answer.,2021-07-18 13:12:48,84891.0,no update,
39320,<machine-learning><python><correlation>,39326,60105.0,,46263,3,Do you know what correlation is and how it's defined? Have you at read the Wikipedia article on correlation? What is the objective of your analysis?,2018-10-07 21:01:59,1156.0,update,comment
39320,<machine-learning><python><correlation>,39326,60105.0,,46266,3,Yes I have read as far as I know there's both positive and negative correlation and if the relation is at 0 means no correlation. I want to know if other features will be useful for building my classification model for prediction my target variable. `Loan_Status`,2018-10-07 21:04:54,60105.0,no update,
39320,<machine-learning><python><correlation>,39326,60105.0,,46267,3,There is more to correlation than that.,2018-10-07 21:06:32,1156.0,no update,
39320,<machine-learning><python><correlation>,39326,60105.0,,46269,3,I think you should refer to an introductory statistics textbook.,2018-10-07 21:13:05,1156.0,no update,
69958,<python><classification><machine-learning-model><confusion-matrix><auc>,70009,92040.0,92040.0,75056,1,The FPR and TPR must be swapped somewhere but I can't see where in the code you give.,2020-03-20 0:57:25,64377.0,no update,
69958,<python><classification><machine-learning-model><confusion-matrix><auc>,70009,92040.0,92040.0,75077,1,"How do you generate `y_pred`?  Is it the same in the first and third code blocks?  If so, that's probably the problem...",2020-03-20 14:18:29,55122.0,update,comment
69958,<python><classification><machine-learning-model><confusion-matrix><auc>,70009,92040.0,92040.0,75084,1,"@Erwan, I tried it while fitting roc_curve, but the function roc_curve() returns fpr first. I added my code to the question again, I appreciate your inputs.",2020-03-20 18:19:10,92040.0,no update,
69958,<python><classification><machine-learning-model><confusion-matrix><auc>,70009,92040.0,92040.0,75085,1,"@BenReiniger, I edited the question to make it more clear Ben, yes, the y_pred is same.",2020-03-20 18:19:49,92040.0,no update,
19711,<machine-learning><python><classification><scikit-learn><churn>,46692,33393.0,,22983,5,"Another theory suggests that it is not correct to only allow only a single sample for each contract and one should create a sample for each past time-period (e.g. month) of each contract. 

E.g., if a contract was not cancelled and it is 12 months old, we should have 12 samples, each representing the features as at each of those 12 months respectively and each with label = 0. 
On the other hand if a contract was cancelled now and is 12 months old, we should have 11 samples where the contract was not cancelled (label = 0) and 1 (the most recent) where it was cancelled (label = 1).",2017-06-15 9:58:24,33393.0,no update,
19711,<machine-learning><python><classification><scikit-learn><churn>,46692,33393.0,,29392,5,"I am having exactly the same problem and I have found no solution yet. One suspicion I have is that we are dealing with some type of unbalanced class problem. I my case, I am training the model where churners are 60% of total sample because built from 6 years worth of data, but the ""live dataset"" only has 6% churners because it collects data from only 2 months into the future. So, the model learns from a population which is not realistic, and when applied to the true population, it fails. You might be in the same situation.",2017-11-24 15:48:32,25548.0,no update,
19711,<machine-learning><python><classification><scikit-learn><churn>,46692,33393.0,,44299,5,I´m building a similar model to that. Do you have any updates on the matter? Could we maybe share some insights?,2018-09-03 19:04:39,58555.0,update,post
19711,<machine-learning><python><classification><scikit-learn><churn>,46692,33393.0,,53539,5,"@LucianoAlmeidaFilho Do you still need insights? I missed the question, sorry.",2019-03-05 9:05:33,33393.0,update,both
54052,<python><pandas><dummy-variables>,54077,18247.0,29169.0,63688,5,See also https://stackoverflow.com/q/54786266/10495893,2019-08-24 1:46:31,55122.0,no update,
20168,<machine-learning><python><data-mining>,20171,33838.0,,23642,0,Please don't cross-post duplicates: https://stackoverflow.com/q/44895903/1060350,2017-07-08 7:27:49,924.0,no update,
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37825,1,"How many output classes are you expecting? Regardless, 80% is not necessarily bad. You should try your code using some toy data to see if there are any errors in the code. But, 80% is not necessarily a bad result for a multiclass perceptron depending on the complexity of the classification task.",2018-05-28 2:32:41,29587.0,update,post
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37826,1,"For this project, accuracy needs to be at least 90%. The classes could be from 0 to 8, so 9 classes.",2018-05-28 3:25:19,10799.0,no update,
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37827,1,is the list `train` the entirety of your training set? What are their respective labels?,2018-05-28 3:32:38,29587.0,update,comment
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37828,1,Can you post the entirety of your code please? What is the `data_test` variable?,2018-05-28 3:33:47,29587.0,update,no
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37829,1,"Sorry, that should've been `test`. I corrected it. The full training set has 1,000+ instances. I know the data isn't perfectly linearly separable, and I'm also pretty sure I need to add a bias, but am unsure how.",2018-05-28 3:37:34,10799.0,no update,
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37830,1,"Ok, I'll take a look. Is your real data 3D as well?",2018-05-28 3:38:19,29587.0,update,post
32233,<machine-learning><python><perceptron>,32253,10799.0,10799.0,37831,1,"Thanks! No, the data is the same dimensions as above, 2. The 3rd value is the class.",2018-05-28 3:42:47,10799.0,no update,
42771,<machine-learning><python><classification>,44362,64469.0,64469.0,50803,0,I think your question needs some clarification in order to get the most help - where is the data science need in this? Isn't this something that you can solve analytically by just grouping the SIMs together according to account-holder? Please clarify your posting to highlight where exactly you need an algorithm,2019-01-09 19:31:32,23240.0,update,comment
42771,<machine-learning><python><classification>,44362,64469.0,64469.0,50900,0,"The point is, that i don't know account-holder, obviously. Hence, i need to infer wether two arbitrary phone numbers belong to the same client.",2019-01-11 4:54:05,64469.0,no update,
61602,<python><pandas><feature-selection>,62212,83591.0,,66991,1,Feature engineering is a whole thing by itself. You could use a linear regression or a classifier to investigate which columns are the most related to your target variable.,2019-10-11 14:04:05,14560.0,no update,
61602,<python><pandas><feature-selection>,62212,83591.0,,67000,1,"Depending on the type of your variables, you could also just do something as simple as computing the correlation of your target variable with each feature using that to filter. If you have time series data, there is *Granger causality*, to measure influence of other variables over time.",2019-10-11 16:57:39,45264.0,no update,
60706,<machine-learning><python><cnn><image-classification>,60737,80406.0,29169.0,66110,2,Let it run longer,2019-09-24 17:45:34,33202.0,no update,
60706,<machine-learning><python><cnn><image-classification>,60737,80406.0,29169.0,66111,2,This is a big network when you only have ~1500 images,2019-09-24 17:46:18,33202.0,no update,
60706,<machine-learning><python><cnn><image-classification>,60737,80406.0,29169.0,66115,2,Use dropout in your dense layers. The other comment is also true and has to be considered.,2019-09-24 18:39:57,28175.0,no update,
60706,<machine-learning><python><cnn><image-classification>,60737,80406.0,29169.0,66118,2,I have only one dense layer and there's a dropout in there. How much I have to reduce my layers? 3 convolutionals? 4?,2019-09-24 19:23:45,80406.0,update,both
38666,<machine-learning><python><scikit-learn><decision-trees><supervised-learning>,38671,47403.0,,49431,11,"I have succeeded in implementing Cost complexity pruning on Sklearn's model, and here is the link: https://github.com/appleyuchi/Decision_Tree_Prune
you may like it.",2018-12-10 4:14:10,63952.0,no update,
38666,<machine-learning><python><scikit-learn><decision-trees><supervised-learning>,38671,47403.0,,49432,11,"While this link may answer the question, it is better to include the essential parts of the answer here and provide the link for reference.  Link-only answers can become invalid if the linked page changes. - [From Review](/review/low-quality-posts/29256)",2018-12-10 19:47:30,23305.0,no update,
10320,<python><visualization>,10321,16096.0,11097.0,10231,1,"[This](http://stackoverflow.com/a/28232401/4993513) might also help you if you want to plot in matplotlib. Else, @Emre's answer should be good to go :)",2016-02-19 16:39:16,11097.0,no update,
88851,<python><visualization><matplotlib>,88950,108233.0,29169.0,93111,0,What exactly are you after? Changing e.g. '2014' to '2014-01-01'?,2021-02-04 5:40:52,32492.0,update,comment
88851,<python><visualization><matplotlib>,88950,108233.0,29169.0,93112,0,yes. and more frequent dates rather than just one date from every year.,2021-02-04 5:45:18,108233.0,no update,
88851,<python><visualization><matplotlib>,88950,108233.0,29169.0,93113,0,How frequent? If there are too many labels then you will have a problem of them significantly overlapping.,2021-02-04 6:16:12,32492.0,update,post
88851,<python><visualization><matplotlib>,88950,108233.0,29169.0,93118,0,yes.maybe one date per month,2021-02-04 7:19:51,108233.0,no update,
17713,<python><deep-learning><tensorflow><convolution>,17969,30173.0,30173.0,20069,2,"The conv2d_transpose is nothing but the gradient of the conv2d. So if u want to get original data, calculate gradient of the embedded_chars w.r.t. original input.",2017-03-22 3:22:36,14310.0,no update,
17713,<python><deep-learning><tensorflow><convolution>,17969,30173.0,30173.0,20082,2,"@BhagyeshVikani , thank you for you comment.  I'm confused, how gradient of the embedded_chars w.r.t. original input ((tf.gradients(embedded_chars, input_x)) can help me get original data?",2017-03-22 16:36:28,30173.0,update,comment
17713,<python><deep-learning><tensorflow><convolution>,17969,30173.0,30173.0,20083,2,"Yes, `tf.gradients(embedded_chars, input_x)` can help get original data. You can also do this: `tf.gradients(layer_output_tensor, input_x)`. This will give you direct original data from `any given layer output tensor`.",2017-03-22 16:46:53,14310.0,no update,
17713,<python><deep-learning><tensorflow><convolution>,17969,30173.0,30173.0,33997,2,"is P in unpooling step , the output of pooled layer ? can you explain how unpooling is happening ?",2018-03-12 11:16:11,47643.0,update,comment
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,14985,33,"Could you clarify why you don't think that you can normalise those features? Presumably they are numerical the same as other features, so you can take mean/sd? Is your concern about having natural measure of distance between locations? If so, does the data cover a small area (with similar values) or is it global?",2016-08-20 7:13:11,836.0,update,no
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,14986,33,@NeilSlater It's just that intuitively it does not make sense to me to normalize these features. Will the information not be lost if normalized? I have the dataset covering counties of America.,2016-08-20 8:15:45,23600.0,update,post
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,14987,33,"What information do you think will be lost? It probably will not be actually lost, but if you explain in your question what your concern is, someone will be able to answer. Not knowing any more, I would just normalise regardless - for fully global values and some problems (where distance between points is important) I might create a 3d cartesian co-ordinates feature from the long/lat.",2016-08-20 8:20:41,836.0,no update,
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,14995,33,What's your question here? What are you trying to find out from the data? Correlation? Clustering? Classification? Prediction? Interpolation? How is location important to your model?,2016-08-20 12:46:19,471.0,update,both
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,15011,33,@Spacedman Please see edit.,2016-08-20 18:58:30,23600.0,no update,
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,15032,33,"So maybe you want a regression model with a spatial surface defined by a Gaussian field, or a 2-dimensional parametric spline surface or something like that? Are you just hoping to feed the numbers into an ML algorithm or Random Forests or something?",2016-08-21 14:33:13,471.0,update,post
13567,<machine-learning><python><feature-engineering><feature-scaling><normalization>,13575,23600.0,23600.0,15078,33,@Spacedman I am still working on the feature engineering part but yes I am trying to build a regression model.,2016-08-23 15:44:22,23600.0,no update,
28676,<python><neural-network><keras><software-recommendation>,30084,47309.0,47309.0,33578,9,Why do you want to use Complex Nos?Any significance?Please Explain?,2018-03-06 8:05:44,35644.0,update,post
28676,<python><neural-network><keras><software-recommendation>,30084,47309.0,47309.0,33658,9,Because many real world problems are formulated in terms of complex numbers.,2018-03-07 3:18:42,47309.0,no update,
28676,<python><neural-network><keras><software-recommendation>,30084,47309.0,47309.0,33663,9,"See literature on MPS, MERA, PEPS in condensed matter physics. There are libraries for these attached to some of the papers.",2018-03-07 5:59:30,21969.0,no update,
35807,<python><neural-network><pytorch>,35810,56499.0,29575.0,42037,0,"Cross-posted: https://stackoverflow.com/q/51433774/781723, https://datascience.stackexchange.com/q/35807/8560.  Please [do not post the same question on multiple sites](https://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted.  Also, re-posting your question when you already got an answer there and some suggestions on how to proceed, without acknowledging that, discussing whether it meets your need, or trying those suggestions doesn't seem like what we're looking for.  You should try those things first.",2018-07-20 19:21:49,8560.0,no update,
35807,<python><neural-network><pytorch>,35810,56499.0,29575.0,42080,0,I'm voting to close this question as off-topic because it was cross-posted,2018-07-22 3:44:14,21.0,no update,
35807,<python><neural-network><pytorch>,35810,56499.0,29575.0,42229,0,"@SeanOwen my bad for cross-posting -- is there a way to keep this question and mark the other one as off-topic instead? I will avoid doing so in the future, but I believe that my question is more relevant to data science than programming.",2018-07-24 20:52:25,56499.0,update,comment
1225,<python><correlation>,1235,3580.0,,83532,2,"Use one of Pandas' built in functions:
http://pandas.pydata.org/pandas-docs/stable/computation.html#correlation",2014-10-08 22:12:32,3586.0,no update,
51935,<machine-learning><python><feature-selection><numpy><kaggle>,51942,73912.0,71218.0,57944,1,why do you think the values should be higher?,2019-05-14 9:56:18,73332.0,update,no
51935,<machine-learning><python><feature-selection><numpy><kaggle>,51942,73912.0,71218.0,57945,1,"Because there is a strong correlation between sex, class and survival.  Women and rich passengers were most likely to survive.",2019-05-14 9:59:14,73912.0,no update,
84867,<python><scikit-learn><pandas><numpy>,84888,76801.0,76801.0,89051,1,https://stackoverflow.com/questions/23872942/sklearn-and-large-datasets,2020-11-02 22:51:42,45166.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77621,1,How big is your dataset?,2020-04-22 22:10:59,94454.0,update,comment
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77622,1,@nwaldo 10k observations,2020-04-22 22:12:00,94339.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77630,1,Have you normalized the data before performing regression.,2020-04-23 2:53:58,68023.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77638,1,"@BenReiniger I’ve just used cross validation dividing to 5 parts and it shows -0.1, seems very bad.",2020-04-23 5:40:23,94339.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77639,1,@ShubhamPanchal yes,2020-04-23 5:41:03,94339.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77644,1,"Please add you full code, I can't see Normalization part. What is the extra point that you have mentioned for featire_4. Is your train, test score ok? Any Outliers, then do Standardization not Normalization.",2020-04-23 6:25:17,58341.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77652,1,"@RoshanJha my normalization is just dividing each value by maximum in that feature values. Should I user some other normalization? My extra about 4th feature is that I guess 30km as the maximum, because I have some trash in user location.",2020-04-23 9:33:22,94339.0,update,both
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77657,1,"What is your aim? Prediction? When you do nothing to the data (no normalisation etc), linear regression would read like: ""If gym rating goes up by one unit, user's visit change by ""Feature: 0, coef"" units. Not knowing how your data looks like, it is impossible to say if you might have a problem here. You can do any linear transformation to the data without corrupting results. However, the interpretation of coefs changes in this case. If you have super small unit changes, super large coefs would be a ""normal"" result.",2020-04-23 10:42:21,71442.0,update,no
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77667,1,"@Peter thanks for your response, if I share some data bunch of data, can u give me some advices?",2020-04-23 12:10:05,94339.0,update,comment
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77691,1,"A phenomenon called ""(multi-)collinearity"".",2020-04-23 19:36:20,36977.0,no update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,77732,1,"@MichaelM you were right, I've plot my data and they are almost the same, when I drop some of them, it works okay! Thanks!",2020-04-24 11:46:52,94339.0,update,
72815,<python><scikit-learn><feature-selection><linear-regression><features>,72872,94339.0,71442.0,79785,1,"Two of your features has a consistently small value. 
These features might have some outliers. You should standardize instead of Normalize.",2020-05-31 8:00:32,58341.0,no update,
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89424,3,You should ask this on Data Science Stack Exchange. I suggested the question to be migrated.,2020-11-09 19:54:47,60503.0,no update,
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89425,3,"@RomainReboulleau I agree, the problem is not caused by the code, I guess using a shallower model will resolve the problem.",2020-11-11 0:59:19,39501.0,no update,
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89426,3,"I am agree to migrate the question, how to do it preserving the bounty?",2020-11-11 10:43:56,,update,comment
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89427,3,"I have run your model and found no such problems in losses. The losses decrease perfectly. As @meTchaikovsky said in the answer, probably the problem is due to model initialization. Also I have used `tensorflow.keras` instead of `keras`",2020-11-11 12:52:43,80430.0,no update,
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89428,3,"@hafiz031 I have just update my github with 'univariate_test.ipynb', I use 'seed' for replicability and 'tensorflow. keras' as you, you can not that the third try doesn't learn anything.",2020-11-11 20:14:03,,no update,
85340,<python><keras><deep-learning><time-series><loss-function>,85342,107395.0,,89429,3,"Use `LeaklyReLU` instead of `ReLU` and the problem will be fixed. Simply remove `activation=""relu""` from `Dense()` and add another layer of `LeaklyReLU` after each of the Dense layers like: `model.add(LeakyReLU(alpha=0.05))`. I ran your code with this change for 100 times (n_repeat=100) and this problem didn't occur for a single time.",2020-11-12 12:33:19,80430.0,no update,
46885,<python><keras>,46895,69553.0,29587.0,53685,1,"Hi Adnane, welcome to AI stackexchange, could you please give more informations about your problem?",2019-03-07 4:08:59,21378.0,update,both
28001,<python><scipy>,28051,44477.0,,32696,1,"It wasn't throwing any error when i executed the same. I'm using `python                     3.6.4                h6538335_1  defaults`, using the latest anaconda.",2018-02-20 4:13:13,40853.0,no update,
28001,<python><scipy>,28051,44477.0,,33449,1,Duplicate on Stack Exchange: https://stackoverflow.com/questions/48877227/how-to-calculate-vif-in-python-3-6-with-scipy-1-0-0/49089230#49089230,2018-03-03 21:08:16,30472.0,no update,
94067,<machine-learning><python><classification><k-nn><ai>,94069,117221.0,,98353,2,2nd parameter is standard deviation. The output of the classifier is a long vector (1000 itens) each item represents the class of this input vector (the ith one),2021-05-06 15:52:01,100269.0,no update,
57472,<python>,57475,58771.0,,63118,1,"What should be the dimension, the shape, and the type of the output?",2019-08-13 7:25:26,64596.0,update,comment
33725,<python><neural-network><keras>,33760,47204.0,45264.0,39658,3,What do you mean by half the size of your input? which input?,2018-06-27 14:37:10,45264.0,update,both
33725,<python><neural-network><keras>,33760,47204.0,45264.0,39660,3,"Input image is of the form (448,448,3) so in order to have (224,224,64) we use 64 kernels of size (7,7) and padding = 3 and strides = 2. 224 is half the size of 448. Also, from (14,14,1024) to (7,7,1024) we use padding = 3 and strides = 2. Seven is half of 14.",2018-06-27 15:15:20,47204.0,no update,
71846,<python><keras><tensorflow><cnn>,72220,70662.0,,76756,0,Hi and welcome to the site! Can you please paste and properly format your code?,2020-04-06 15:25:44,84891.0,update,post
71846,<python><keras><tensorflow><cnn>,72220,70662.0,,76775,0,I am sorry i dont know how to do that,2020-04-06 19:20:29,70662.0,no update,
53009,<python><neural-network>,53566,75232.0,38892.0,58800,5,what have you tried so far?,2019-05-31 21:22:38,23305.0,update,comment
53009,<python><neural-network>,53566,75232.0,38892.0,58802,5,"I change max epochs, learning rate, and size data to learn from.
I'm wondering if I have some error in my attached code, that prevents me from finding solution.
All my approximations equal zero...",2019-05-31 23:09:51,75232.0,no update,
31740,<machine-learning><python><deep-learning><keras><theano>,31743,51852.0,,37267,1,I think I might have solved this - see my answer.,2018-05-16 15:46:11,51852.0,no update,
6099,<python><distributed><sampling>,6130,7966.0,7966.0,6491,0,"I'm voting to close this question as off-topic because this is a stats question, migrate to stats.stackexchange.com",2015-06-16 16:44:50,471.0,no update,
6099,<python><distributed><sampling>,6130,7966.0,7966.0,6492,0,"@Spacedman Thank you for your opinion. Initially, stats.stackexchange.com wanted to do the same for being off-topic because this was considered a stat-software question.",2015-06-16 18:43:59,7966.0,no update,
6099,<python><distributed><sampling>,6130,7966.0,7966.0,6496,0,"I think it's close enough for this site since it's more about doing this is in software, and DS is more of the overlap between stats and engineering.",2015-06-17 7:33:48,21.0,no update,
13465,<python><scikit-learn><decision-trees><error-handling>,13466,23452.0,23452.0,14812,3,"It's also unnecessary to assign the result from fit back onto clf, it is an inplace method which changes your clf object",2016-08-16 16:55:27,14904.0,no update,
13465,<python><scikit-learn><decision-trees><error-handling>,13466,23452.0,23452.0,14825,3,what version of python are you using?,2016-08-16 20:56:42,2549.0,update,both
13465,<python><scikit-learn><decision-trees><error-handling>,13466,23452.0,23452.0,19704,3,"Very useful, it solved the problem.",2017-03-05 23:59:36,29657.0,no update,
87262,<python><time-series>,87346,107456.0,85045.0,91375,1,"Assuming you have many values per second, you could just group the points by second (from hh:mm:ss:000ms to hh:mm:ss:999ms) and for every group calculate the mean of every coordinate. Of course some information will be lost.",2020-12-29 15:53:40,64377.0,no update,
58484,<python><pandas>,58504,80463.0,80463.0,64044,0,"I think the problem is that you are giving the year as an element of `.loc` which should be index, whereas it is the actual value of the year.
please provide the columns of each of these datasets. and what you actually want as an outcome",2019-08-31 23:49:16,74421.0,no update,
58484,<python><pandas>,58504,80463.0,80463.0,64046,0,I mean if it's possible to add 2 rows of each dataset to your question.,2019-09-01 0:03:30,74421.0,no update,
58484,<python><pandas>,58504,80463.0,80463.0,64048,0,I used .loc and not .iloc in order to find the values by the labels and not by the position.,2019-09-01 0:16:57,80463.0,no update,
69246,<python><python-3.x>,69296,90151.0,,74356,0,"it's not possible to correlate your error traceback with your code, so it's difficult to understand where the error comes from. you'll get more useful answers if you can create a minimal reproducible example: https://stackoverflow.com/help/minimal-reproducible-example",2020-03-06 3:39:10,39007.0,no update,
19768,<python><neural-network><scikit-learn><cross-validation><hyperparameter>,19771,33476.0,23305.0,33476,6,"I am very new to Python and was going through this post. My query is similar and response on setting up of Hidden layers helped a lot. However, I am **unable to set up alpha** in same way as mentioned above. Any help on this? I mean when I am setting up alpha as *alpha': [10.0 ** -np.arange(1, 7)]*, it is giving me error.",2018-03-04 13:14:56,47225.0,update,post
19768,<python><neural-network><scikit-learn><cross-validation><hyperparameter>,19771,33476.0,23305.0,33477,6,"the alpha parameter of the MLPClassifier is a scalar. [10.0 ** -np.arange(1, 7)], is a vector. Which works because it is passed to gridSearchCV which then passes each element of the vector to a new classifier. Have you set it up in the same way?",2018-03-04 14:03:36,27432.0,no update,
19768,<python><neural-network><scikit-learn><cross-validation><hyperparameter>,19771,33476.0,23305.0,86414,6,"`'hidden_layer_sizes': [x for x in product(range(1,100), range(1,3))]`",2020-09-28 16:30:14,105377.0,no update,
5601,<classification><python><svm>,5603,9281.0,836.0,5977,1,"Please, change the topic of your post.",2015-04-24 15:15:26,173.0,no update,
11655,<python><random-forest><pandas><class-imbalance><ensemble-modeling>,12411,18098.0,-1.0,11812,2,Where did you learn abt the algorithm? The paper?,2016-05-09 14:36:11,11097.0,update,comment
11655,<python><random-forest><pandas><class-imbalance><ensemble-modeling>,12411,18098.0,-1.0,11813,2,No. Someone else suggested it for my problem. I tried SMOTE but it didn't seem to work. I'll be really grateful if someone could help. As I said it's generating 9 matrices of my attributes and 9 corresponding arrays of output. Thanks,2016-05-09 15:50:28,18098.0,no update,
55808,<python><pandas><dataframe>,55813,38091.0,38091.0,61482,0,Can you elaborate on what you mean by dynamically?,2019-07-17 6:05:56,24409.0,update,no
55808,<python><pandas><dataframe>,55813,38091.0,38091.0,61483,0,"@grldsndrs Dynamically means, like iterating through columns, and change type of column.",2019-07-17 6:31:18,38091.0,no update,
55808,<python><pandas><dataframe>,55813,38091.0,38091.0,61484,0,So you just want to iterate through the columns of it dataframe?,2019-07-17 6:32:32,24409.0,update,post
55808,<python><pandas><dataframe>,55813,38091.0,38091.0,61485,0,"Yes, iterate through the columns of dataframe. Find the columns with numeric values, but stored as string. And convert those column type to int/float.",2019-07-17 6:33:37,38091.0,no update,
58713,<python><optimization><genetic-algorithms>,75497,34281.0,,64300,1,"In the vast majority of optimization problems, the real bottleneck is the evaluation of the objective function, not the internals of the algorithm. What are 100 ms spent inside an optimization algorithm iteration compared to real life problems which require minutes (or hours) to evaluate the objective function? Your question makes sense only if you're playing with toy objective functions or you're lucky enough that your problem can be super-quickly solved. There are many implementations of genetic/metaheuristic algorithms in Python.",2019-09-05 20:05:44,80744.0,no update,
69799,<python><deep-learning><tensorflow><cnn><speech-to-text>,69814,91869.0,91869.0,74900,1,Hi. Could you add a reference and maybe polish your text to make it more readable? Thanks.,2020-03-17 2:56:11,23305.0,update,comment
69799,<python><deep-learning><tensorflow><cnn><speech-to-text>,69814,91869.0,91869.0,74901,1,Hi @oW_ I added a reference,2020-03-17 6:12:36,91869.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105588,1,"Try this `X_train = train1.drop(['Sales', 'Date', 'Customers'], axis = 1)`. Dont put the `.values` at the end.",2021-09-23 5:56:30,119921.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105593,1,"@spectre I tried your suggestion, but am still getting the same error msg.",2021-09-23 9:04:17,125624.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105595,1,"Why are you splitting the data manually? You can use sklearn's module `train_test_split` which might solve the problem. If that doesn't solve your problem, link a collab notebook and i'll look at it.\",2021-09-23 9:16:46,119921.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105599,1,"@spectre The train_test_split doesn't seems to work, it throws ValueError `ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.`

I have shared a collab link : https://colab.research.google.com/drive/1KsnUecPnOAwKmU0sif7hu3XlqSRFhcF-?usp=sharing",2021-09-23 10:15:31,125624.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105622,1,@ArislanMakhmudov I have mounted my drive have attached the uploaded the csv files. The collab will work now.,2021-09-23 17:25:01,125624.0,no update,
102370,<machine-learning><python><numpy><jupyter>,102430,125624.0,119921.0,105651,1,"@ArislanMakhmudov The problem with files was it was getting removed after runtime got restarted, you can look up the files on this link https://drive.google.com/drive/folders/14HxbGRXINA_yWU4L9IWq-BLsLYgfX3zA?usp=sharing
Thanks very much for your time & help.",2021-09-24 7:52:34,125624.0,no update,
9850,<machine-learning><python><neural-network><statistics><tensorflow>,9870,13809.0,,9676,59,Good question.  Welcome to the site :),2016-01-19 11:49:57,11097.0,no update,
9850,<machine-learning><python><neural-network><statistics><tensorflow>,9870,13809.0,,9707,59,"Usually, MSE is taken for regression and Cross-Entropy for classification. Classification Figure of Merit (CFM) was introduced in ""A novel objective function for improved phoneme recognition using time delay neural networks"" by Hampshire and Waibel. If I remember it correctly, they also explain why they designed CFM like they did.",2016-01-20 13:26:34,8820.0,no update,
9850,<machine-learning><python><neural-network><statistics><tensorflow>,9870,13809.0,,9731,59,"I think reduce_sum(y_train*tf.log(y_output)) is used a lot because its a fairly common ""simple case"" example. It'll run sum each batch's error, which means your error's double the cost (and the magnitude of the gradient) if your batch_sizes double. Making the simple change to reduce_mean will at the very least make debugging and playing with settings more understandable in my opinion.",2016-01-21 13:24:45,15599.0,no update,
11935,<python><pandas><performance>,11952,3024.0,3024.0,12080,0,Could you post a sample CSV?,2016-05-26 17:21:16,381.0,update,comment
58894,<machine-learning><python><machine-learning-model><nlp>,58895,49268.0,49268.0,64404,0,This is naive but have you tried restricting 'BE' to be capital letters?,2019-09-09 10:07:09,49590.0,update,both
58894,<machine-learning><python><machine-learning-model><nlp>,58895,49268.0,49268.0,64405,0,"I cannot restrict capital letters, because there is some other process before my algorithm which gives text in small letters. so as my vocab.",2019-09-09 10:14:00,49268.0,update,no
57406,<python>,57485,79348.0,79348.0,63066,1,"There are many examples of how to do `groupby` operations in Python. If you are not already, I would suggest using the Pandas library. Here is a walkthrough/tutorial: https://www.marsja.se/python-pandas-groupby-tutorial-examples/. Otherwise, It'd be helpful if you showed the code you have so far.",2019-08-11 23:43:38,45264.0,no update,
57406,<python>,57485,79348.0,79348.0,63067,1,I just added an example using the data from this link.,2019-08-12 0:04:47,79348.0,no update,
64107,<machine-learning><python><deep-learning><tensorflow><java>,64225,60379.0,60503.0,69404,1,I you have to deploy in a Java environment and are unable to use something like docker to simply provide a API with a service.,2019-12-04 20:11:12,45264.0,no update,
57608,<python><pandas><data-cleaning>,57950,45573.0,,63249,2,"This is kind of a stack overflow question really.  Yet, I believe they wanted `df.groupby('day').apply(lambda x: set(x['meal']))`",2019-08-15 18:23:54,74735.0,no update,
57608,<python><pandas><data-cleaning>,57950,45573.0,,63252,2,"@grochmal Won’t this give the set of meals eaten by anyone on that day, rather than only those eaten by both? So day 1 = {breakfast, lunch, dessert, beer} instead of {breakfast, lunch}?",2019-08-15 19:40:19,45573.0,update,comment
57608,<python><pandas><data-cleaning>,57950,45573.0,,63343,2,"@myseun I think the answer by grochmal is what the question wanted, the question wants the whole meals consumed by all the people in each day, whereas in the **goal** dataset this is not achieved",2019-08-17 20:29:20,74421.0,no update,
57608,<python><pandas><data-cleaning>,57950,45573.0,,63345,2,@FatemehAsgarinejad The goal dataframe was provided and so is definitely what the question wanted.,2019-08-17 22:08:25,45573.0,no update,
15110,<python><image-classification><computer-vision><preprocessing>,17010,24696.0,94792.0,108273,8,Were you able to reproduce Goodfellow's figure 12.1? I can't seem to do it.,2021-12-15 11:28:52,105406.0,update,post
15110,<python><image-classification><computer-vision><preprocessing>,17010,24696.0,94792.0,108305,8,I have not try it. I will try it and let u know the result,2021-12-16 14:57:09,24696.0,no update,
398,<python><visualization>,405,872.0,,1340,8,All these questions have been beaten to death on StackOverflow.  What value is there in rehashing it here?,2014-06-16 14:21:06,515.0,update,post
398,<python><visualization>,405,872.0,,1346,8,"@DirkEddelbuettel Because they are off-topic on SO, hence often closed?",2014-06-16 18:56:19,843.0,update,no
86253,<machine-learning><python><recommender-system>,86262,108325.0,43000.0,90332,0,do you mean making user and item embeddings dependent from features or ranking in general being feature dependent?,2020-12-03 21:16:23,107507.0,update,comment
86253,<machine-learning><python><recommender-system>,86262,108325.0,43000.0,90344,0,[Same question](https://datascience.stackexchange.com/questions/834/recommending-movies-with-additional-features-using-collaborative-filtering),2020-12-04 8:58:47,6550.0,no update,
47363,<python><pandas>,47369,3314.0,3314.0,54311,2,Just a heads up: you should only ask one question per question.,2019-03-15 18:03:22,54252.0,no update,
17513,<python><neural-network><feature-extraction>,17530,10810.0,,39408,8,if i want to make my own model as a feature extractor then what should i do ??,2018-06-19 18:59:55,53896.0,update,post
17513,<python><neural-network><feature-extraction>,17530,10810.0,,46427,8,"I checked the above tensorflow resnet model and generated features.h5 file but unable to view the features that are extracted, and also unable to proceed further step",2018-10-09 18:33:58,60453.0,no update,
13358,<machine-learning><python><pca>,13388,20926.0,23283.0,14734,3,Where did you get this formula for implementation?,2016-08-11 7:41:25,2750.0,update,comment
13358,<machine-learning><python><pca>,13388,20926.0,23283.0,14738,3,I'm taking Andrew Ng's Machine Learning [Course](https://www.coursera.org/learn/machine-learning) .Week 8 and Lecture  - Dimensionality Reduction .Why did you ask that?Is it wrong?This formula.,2016-08-11 9:15:07,20926.0,no update,
87597,<python><nlp><data-cleaning>,87705,103309.0,,91804,1,Commenting on how you ask the questions. You need to make your questions more clear by grouping them up or specifically putting them in bullet points. I find it confusing.,2021-01-09 5:22:29,63995.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,25494,3,"It depends on what you want to do, and what your performance requirements are. ETL can be done in a database like postgres (relational), MapD (analytics on GPU), or Druid (analytics). Try using one If that's not fast enough then you can spin up a Spark cluster in the cloud. If you want to build a model, you might not need all that data for a prototype.",2017-08-04 6:04:21,381.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,25497,3,"the problem with a relational database is that there is a 1024 column limit. With biological data, it's real common to have > 1000 columns.  I was using a RDBMS and realized that I'm going to routinely exceed 1000 columns.  I was hoping that doing everything in python would be possible.",2017-08-04 6:11:41,32637.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,25503,3,"Adding to @Emre's suggestion, you can store the data in a DB which is suitable to your use case (columnar databases like HBase), wherein you can store the related columns in the table within the same column family. 
Chances are that you would not use a majority of the total columns at the same time. If so, a columnar DB can be quite useful.
Additionally, pull the data from HBase into Spark (pySpark / Scala) and proceed.
^may not be the exact solution you sought, but one alternative",2017-08-04 11:58:31,25742.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,26144,3,Did you try any databases in the meantime?,2017-08-22 2:15:16,381.0,update,comment
21952,<python><pandas>,22739,32637.0,32637.0,26145,3,"yes, I tried SQL Server. Problem is that I can not exceed 1024 columns for a table.  So that's going to be a big issue when there can easily be 10,000 features to feed into a neural network.",2017-08-22 2:48:59,32637.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,26146,3,"Although SQL Server is not ideal, it does support [wide tables](https://technet.microsoft.com/en-us/library/cc645884.aspx) -- up to 30,000 columns. Did you try using them?",2017-08-22 2:56:33,381.0,update,no
21952,<python><pandas>,22739,32637.0,32637.0,26147,3,"Not really.  I looked up the concept though.  My result set would be say even 2000 columns.  To ""save"" that results as a wide table, I've got to first predefine the wide table schema, and my 2000 columns make that kind of clunky to define that many columns.  Then I have to insert them.  If your columns change, you have to redefine the wide table schema.  It just seems like the wrong tool for the job, but more importantly I don't have any examples of how to create a wide table for 2000 columns without typing it all painstakingly.",2017-08-22 3:17:24,32637.0,no update,
21952,<python><pandas>,22739,32637.0,32637.0,26291,3,"As @Emre said, it depends on what you want to do. Using databases is convenient because they are massively index the data. If what you want to do is simple, you might be able to do your own indexation (isolating/sorting columns, creating shards for faster access etc.). There are plenty of python tools to do such things.",2017-08-25 12:57:19,17208.0,no update,
39072,<python><nlp>,39194,44456.0,,45939,3,"Unfortunately I can't replicate your ""local"" results. I get the same as the ""remote"" version. Windows 7 Enterprise, Anaconda Python 3.6.6, Spacy 2.0.11.",2018-10-02 16:44:06,1156.0,no update,
39072,<python><nlp>,39194,44456.0,,45949,3,"Me neither. I recommend creating a new virtual environment with the required packages only and if this unexpected behaviour still occurs, file an issue on their GitHub page!",2018-10-02 20:47:56,45264.0,no update,
81479,<python><scikit-learn><data-imputation><pipelines>,81559,89795.0,89795.0,85338,1,"You want a sequential pipeline, so use Pipeline:  https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html",2020-09-10 1:18:05,55122.0,no update,
81479,<python><scikit-learn><data-imputation><pipelines>,81559,89795.0,89795.0,85348,1,"This will not resolve the issue. A Pipeline like this Pipeline(steps=[('PreProc', ColumnTransformer(....)), ('model', SVC())]) will still have the same issue. Can you please provide an example.",2020-09-10 4:53:01,89795.0,no update,
81479,<python><scikit-learn><data-imputation><pipelines>,81559,89795.0,89795.0,85372,1,"Can you list for each feature what operation you want to appy? eg: feat_1: SimpleImputer, StandardScaler; feats_2: SimpleImputer; etc. ?",2020-09-10 10:05:05,50534.0,update,comment
81479,<python><scikit-learn><data-imputation><pipelines>,81559,89795.0,89795.0,85397,1,feat_1: SimpleImputer and StandardScaler; feat_2: SimpleImputer and StandardScaler; feat_3: StandardScaler;,2020-09-10 13:48:43,89795.0,no update,
69210,<python><nlp>,69213,83066.0,,74315,0,Is there no other input to the generation of the description but these 3 binary features?,2020-03-05 13:45:28,14675.0,update,no
69210,<python><nlp>,69213,83066.0,,74316,0,"There may be more binary features, and a little manual introduction (i.e. ""Call of duty is"") or a categorical feature with the name",2020-03-05 14:22:26,83066.0,no update,
43229,<machine-learning><python><recommender-system><machine-learning-model>,43231,54229.0,,50300,3,Have a look on GitHub Projects!,2018-12-28 4:27:37,35644.0,no update,
45856,<machine-learning><python><nlp><nltk><regex>,45990,63693.0,,52766,5,See : https://datascience.stackexchange.com/questions/45854/date-extraction-in-python/45855#45855,2019-02-20 6:34:10,41771.0,no update,
45856,<machine-learning><python><nlp><nltk><regex>,45990,63693.0,,95004,5,"If any one need library to accomplish this then you can use datefinder,
It will detect all types of natural styles of dates.

Github Link: https://github.com/akoumjian/datefinder",2021-03-18 3:56:21,63693.0,no update,
89778,<python><keras><tensorflow><nlp>,90160,44083.0,,94305,0,"Can you share more of your code, including your test dataset code?",2021-03-02 14:55:25,1330.0,update,post
93501,<python><dataset><pandas><data-table>,93506,116594.0,43000.0,97791,1,It seems you're not applying any function to aggregate each group.,2021-04-24 19:42:05,61522.0,no update,
41421,<python><ensemble-modeling>,41430,57019.0,57019.0,48581,3,@SeanOwen may you advise to free the question it is an intersiting question and i gave all details,2018-11-21 11:39:19,57019.0,no update,
41421,<python><ensemble-modeling>,41430,57019.0,57019.0,48599,3,"I don't mind reopening it; it's borderline. Really it's better to boil this down to the essential piece of code you're asking about, and give more detail about what you have observed, narrowing it down further.",2018-11-21 19:16:27,21.0,no update,
41421,<python><ensemble-modeling>,41430,57019.0,57019.0,48605,3,Thanks for your comment i found it an interesting question about a stacking principle and it is not about code So i put all the details and explanation as required with an example with iris dataset no further  details to add thanks for your kind support i see people here evaluate the effort of others without even asking i asked to demand explanation,2018-11-22 6:55:14,57019.0,no update,
55902,<machine-learning><python><classification><xgboost>,56011,77900.0,69822.0,61800,6,Related https://datascience.stackexchange.com/a/28801/29781,2019-07-21 8:07:12,29781.0,no update,
30249,<machine-learning><python><data-cleaning><data>,30252,50449.0,,35439,8,You really have to tell us what data structure it is in. E.g. is it SQL?,2018-04-13 1:32:28,49195.0,update,comment
86244,<machine-learning><python><preprocessing>,86268,99648.0,99648.0,90322,0,"I do not understand why there is an issue with multiple samples per data? As long as each sample is taken at a different time it should be ok, since some measurements will still be different e.g. gestationWeek, abdomcirc. A decent model should learn that there is a relationship between gestationWeek and your other features (so include week as a feature). In this case, the only real risk is to not use the same patient in different datasets: train, validation and test as this would bias the results",2020-12-03 18:08:39,106358.0,update,post
86244,<machine-learning><python><preprocessing>,86268,99648.0,99648.0,90345,0,"@Burger the issue is with certain features having numerous values for the same pregnancy, leading to multiple rows in the dataset being associated with the same patient. My understanding is that every row should represent a patient, and for every patient the model will predict the target variable. Is this not correct?",2020-12-04 10:13:05,99648.0,update,both
865,<python><parsing>,875,988.0,,1935,1,This question is very difficult to understand. Can you please consider rewriting it with a better description of what you are looking for and what you have already tried.,2014-07-30 12:22:03,802.0,no update,
865,<python><parsing>,875,988.0,,1964,1,"Not a data science question, its a programming question. Go ask on StackOverflow",2014-08-01 9:08:50,471.0,no update,
57953,<machine-learning><python><classification><normalization>,57957,79879.0,,63524,0,@sAguinaga Thanks  that answers my last point. Im quite confused by the need to do it in the first place.,2019-08-21 11:55:10,79879.0,no update,
57953,<machine-learning><python><classification><normalization>,57957,79879.0,,63527,0,https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn See this stack exchange question that shows the point quite well,2019-08-21 12:26:03,77162.0,no update,
57953,<machine-learning><python><classification><normalization>,57957,79879.0,,63530,0,"A better answer is offered [here](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/) and [here](https://towardsdatascience.com/when-to-standardize-your-data-in-4-minutes-f9282190707e) z score lets you do standardization: z = x –  x̄  / s, where x̄ (the sample mean)  and s (the sample standard deviation).",2019-08-21 13:07:48,27558.0,no update,
21883,<python><neural-network>,21884,37395.0,,25405,1,"eta is a greek letter $\eta$, it often is used for the learning rate. This is the distance with which you will reach to get to the next ""more optimal"" point when taking the gradient.",2017-08-01 15:08:46,29587.0,no update,
45540,<machine-learning><python><scikit-learn><regression><supervised-learning>,79652,66386.0,66386.0,52469,4,"Before getting into details,  I assume you have fluctuations, on daily basis, then training on the actual data points is quite difficult as it is very noisy! Have you tried to do a rolling mean of ""kW"" or any other averaging method to reduce those noise a bit? I do not know instead of 15-min data point, maybe average on every 2-hr, 4-hr or so...you gotta try various windows, and see how it improves.",2019-02-13 21:54:34,44456.0,update,both
45540,<machine-learning><python><scikit-learn><regression><supervised-learning>,79652,66386.0,66386.0,52471,4,I’ll try resembling the data to average it per hour.. thanks! Is noise fluctuations in the data that don’t affect the “big picture”?,2019-02-13 22:53:49,66386.0,update,comment
45540,<machine-learning><python><scikit-learn><regression><supervised-learning>,79652,66386.0,66386.0,52477,4,"Yes it wont. Your problem is a a time-series in nature. Basically you have soem of sort of seasonality, trend etc, you have to make your series non-stationary, rolling averaging is one way or up-sampling or.., just google you will find lots of materials. It is only when you have a stable changes in your target, simple models like those you used would give a reasonable result. Good luck.",2019-02-14 6:48:48,44456.0,no update,
45540,<machine-learning><python><scikit-learn><regression><supervised-learning>,79652,66386.0,66386.0,52485,4,"I may need to take that ""Yes it wont."" back, it depends!! Surely some information will be lost, but it helps to generalize.",2019-02-14 9:09:44,44456.0,no update,
45540,<machine-learning><python><scikit-learn><regression><supervised-learning>,79652,66386.0,66386.0,52504,4,"@MajidMortazavi thanks for the tips, I updated everything including the Gist for a rolling average... And it didn't improve ML mean squared error much... The distribution of the data plot is a bit less ""smooth"" looking... The curve almost looks (I think) exponential. Does that have an affect on ML algorithms??",2019-02-14 15:10:27,66386.0,update,comment
57487,<python><pandas>,57498,79448.0,,63128,1,"Hi! It would have helped me if you would provide the declaration of your data structure. I think your series is actually a column in a dataframe, but you'll manage.",2019-08-13 11:16:14,27432.0,no update,
48273,<machine-learning><python><keras><tensorflow>,48275,17752.0,,55281,2,"Try `model.evaluate(np.random.random((100, 3)))`",2019-03-30 19:41:53,28175.0,no update,
6144,<python><statistics><feature-scaling>,6524,10187.0,,6759,7,"Why would you want to work with data that hasn't been standardized?  I'm not familiar enough with hidden markov, but there are so many pitfalls to not scaling, it is so easy to do, and it is reversible, so why would you not just scale the data and move on?",2015-07-07 21:30:21,9420.0,update,post
6144,<python><statistics><feature-scaling>,6524,10187.0,,6775,7,"Upon further reflection of my question, scaling the data seemed like the right thing to do.  Thank you for confirming my thought process.",2015-07-08 15:42:53,10187.0,no update,
73072,<machine-learning><python><scikit-learn><pandas>,73259,88522.0,,77917,0,"When limiting my ""name"" column to only 50 characters the limit raises from 80k to roughly 250k in the standardized list before it starts having miss aligned results.  It seems like there is a limit somewhere I can't nail down.  Either server memory or the python variables being used.  Still searching.",2020-04-28 10:47:27,88522.0,no update,
73072,<machine-learning><python><scikit-learn><pandas>,73259,88522.0,,78005,0,"`unique_org` is a set. In Python, sets are not guaranteed to be ordered. Which means the `indices` could be different for different runs.",2020-04-29 20:13:05,1330.0,no update,
73072,<machine-learning><python><scikit-learn><pandas>,73259,88522.0,,78021,0,Have  you tried calculating the levenshtein distance between names and then selecting the right threshold. https://pypi.org/project/python-Levenshtein/,2020-04-30 8:07:57,86339.0,update,comment
97268,<machine-learning><python>,97270,120035.0,29169.0,101500,0,See page 187 of ISLR here --> https://www.statlearning.com/ (click to download first edition).,2021-06-29 19:14:54,119797.0,no update,
97268,<machine-learning><python>,97270,120035.0,29169.0,101535,0,Thank You :-)  I think it will help a lot.,2021-06-30 12:23:29,120035.0,no update,
52968,<python><pandas><dataframe>,52970,38091.0,38091.0,58747,2,"Are you trying to change, for example, all `'cheese' ` entries to `3`? Please post code of what you have tried.",2019-05-31 11:38:22,10814.0,update,post
52968,<python><pandas><dataframe>,52970,38091.0,38091.0,58748,2,"@Edmund I have edited the post. Yes, i am trying to change 'cheese' to 3,  'potato' to 2 etc.",2019-05-31 11:42:50,38091.0,no update,
81914,<machine-learning><python><scikit-learn><svm>,82196,104104.0,104104.0,86028,0,"Are the scores you're reporting the grid search's `best_score_` (and so the averaged k-fold cross-val score)?  You're using potentially a different cv-split each time, so some variation is to be expected there.  Try passing an explicit splitter (even set a `random_state`) so you're sure to be comparing the same things.  Also, `max_iter=10` seems awfully low; do you get convergence warnings?",2020-09-21 15:42:00,55122.0,update,both
81914,<machine-learning><python><scikit-learn><svm>,82196,104104.0,104104.0,86031,0,"the optimal parameter according to GridSearchCV was max iter 10... also the scores are not the best score, but accuracy score calculated by me. What do you mean for explicit splitter? Do you think the parameters I set are right?",2020-09-21 16:09:11,104104.0,update,comment
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19911,3,"Could you clarify how the model is being trained, cross-validated and tested? Especially how you split the data. Do the images fall into natural groups separate from the classes - e.g. if this is facial expression classes, are there several images the same person with different expressions? That is likely to be relevant. Your model code looks OK, except perhaps you should not really care about accuracy metric, just use the same metric as your course is using - that won't affect the results directly, but should make it quicker to pick the best performing model.",2017-03-13 12:48:58,836.0,update,comment
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19917,3,"@NeilSlater Thank you for answering, I've updated the question adding information about the images and the lines of code in charge of training, with 0.1 validation split.",2017-03-13 16:30:52,29914.0,no update,
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19918,3,"Which scores are bad? Training loss, validation loss, or test loss? If test loss, how is testing being done - do the course instructors have a test set separate to the data you have been provided?",2017-03-13 16:53:21,836.0,update,no
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19919,3,@Dominus Pls elaborate on this penalty on misclassified instances. Have you thought of incorporating this penalty into your loss?,2017-03-13 17:16:23,27250.0,update,comment
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19921,3,"@NeilSlater Training and validation both seem to be reliably good - above 60%. Then, the testing is done on a dataset (not classified obviously) but publicly available, and the pictures look pretty much the same as the ones provided for training. The results of the testing are then saved in a csv, uploaded to a server which then returns the final score.",2017-03-13 18:38:09,29914.0,no update,
17550,<python><neural-network><keras><theano><convolution>,17563,29914.0,29914.0,19922,3,"@horaceT The exact formula is not provided, but the course instructor said that it penalises baddly classified data when the certainty is high (i.e. missclassifying by a small majority, say 40% for wrong class, 30% for good one, will give a MUCH better result than missclassifying by giving a wrong class a 90% certainty).

How would one go about implementating a custom penalty? And this penalty would be set as the loss function, right?",2017-03-13 18:40:42,29914.0,update,post
97077,<machine-learning><python><deep-learning><scikit-learn>,97081,119761.0,119761.0,101370,4,3 is the number of xes,2021-06-25 20:34:48,45670.0,no update,
22957,<python><statistics><pandas>,22978,33510.0,,26760,8,Try calling `hist(logx=True)` instead.,2017-09-11 18:11:13,381.0,no update,
106388,<python><multiclass-classification>,106410,119921.0,119921.0,108529,1,"Why are you predicting the bin instead of the value? This looks like a regression problem, not classification and maybe not even ordinal regression.",2021-12-25 4:10:13,73930.0,update,no
106388,<python><multiclass-classification>,106410,119921.0,119921.0,108531,1,Can you expand on your statement? I am failing to understand how can it be a regression problem?,2021-12-25 4:57:30,119921.0,update,comment
106388,<python><multiclass-classification>,106410,119921.0,119921.0,108532,1,Predict “7” instead of “bin 1-10”.,2021-12-25 6:09:04,73930.0,no update,
106388,<python><multiclass-classification>,106410,119921.0,119921.0,108533,1,Ok so assign an integer to each bins. Which is nothing but Label Encoding the target which we often do in Classification problems.,2021-12-25 7:44:16,119921.0,no update,
106388,<python><multiclass-classification>,106410,119921.0,119921.0,108537,1,"Predict the value within the bin. If the correct value is “7”, try to predict “7”. If the correct value is “2”, try to predict “2”.",2021-12-25 13:10:49,73930.0,no update,
22944,<machine-learning><python><data-mining><text-mining><distance>,22955,38395.0,,26749,1,Which implementation of Hellinger distance? Did you implement it yourself or are you using a package?,2017-09-11 13:51:42,13748.0,update,comment
87600,<python><scikit-learn><pandas><random-forest><numpy>,87604,103189.0,83275.0,91680,2,Hi and welcome to Data Science Stack Exchange :),2021-01-06 20:17:23,109934.0,no update,
87600,<python><scikit-learn><pandas><random-forest><numpy>,87604,103189.0,83275.0,91681,2,Thanks so much:),2021-01-06 22:03:40,103189.0,no update,
45767,<python><neural-network><cross-validation><sampling>,45792,67931.0,29169.0,52667,2,Have you considered using [scikit-learn's builtin functions](https://scikit-learn.org/stable/modules/cross_validation.html) for cross-validation?,2019-02-18 16:16:16,67483.0,update,both
19583,<machine-learning><python><xgboost>,19703,33020.0,,22770,0,can you post your code i.e you are using for gridsearch ???,2017-06-09 11:11:13,32686.0,update,comment
19583,<machine-learning><python><xgboost>,19703,33020.0,,22771,0,"Sure.
I am using a slightly modified version of the code given here:
_http://www.codiply.com/blog/hyperparameter-grid-search-across-multiple-models-in-scikit-learn/_ I just commented other models and inserted XGBClassifier there and in parameters I wrote 
`{'n_estimators':[10,20,10,500], `
`'learning_rate':[0.3,1,10 **etc**`",2017-06-09 11:21:13,33020.0,no update,
19583,<machine-learning><python><xgboost>,19703,33020.0,,22821,0,"look at these tutorials they explained it very well . [Tune the Number and Size](http://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/) ,  [Tune Learning Rate](http://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/) , you can also try [Multithreading Support](http://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/) for Xgboost.",2017-06-10 18:28:10,32686.0,no update,
15598,<python><keras>,15621,14360.0,14360.0,17390,2,I guess these are image files loaded by some utility? Could you clarify how you are loading the data to be tested? Edit the question and show just the code you have written to load and test the images.,2016-12-07 18:12:41,836.0,update,comment
15598,<python><keras>,15621,14360.0,14360.0,17422,2,Is the prediction in the same order as the the output of glob?,2016-12-08 17:27:09,14360.0,update,post
15598,<python><keras>,15621,14360.0,14360.0,17424,2,@Raghuram Yes it is.,2016-12-08 18:57:43,14779.0,no update,
